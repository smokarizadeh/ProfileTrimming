2015-02-28 17:44:19,867 [main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=5G
spark.logConf=true
spark.master=spark://ip-10-121-25-177:7077
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-02-28 17:44:20,280 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-02-28 17:44:20,282 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-02-28 17:44:20,283 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-02-28 17:44:20,791 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-02-28 17:44:20,882 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-02-28 17:44:21,223 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:50652]
2015-02-28 17:44:21,235 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50652.
2015-02-28 17:44:21,279 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-02-28 17:44:21,305 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-02-28 17:44:21,333 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150228174421-1cf9
2015-02-28 17:44:21,341 [main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-02-28 17:44:22,144 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-28 17:44:22,330 [main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-7e4f38f3-5f31-4b76-8e26-0abcdbee6cb8
2015-02-28 17:44:22,347 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-02-28 17:44:22,538 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 17:44:22,561 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:50653
2015-02-28 17:44:22,562 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 50653.
2015-02-28 17:44:22,767 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 17:44:22,783 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-02-28 17:44:22,784 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-02-28 17:44:22,787 [main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-02-28 17:44:22,935 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Connecting to master spark://ip-10-121-25-177:7077...
2015-02-28 17:44:23,579 [sparkDriver-akka.actor.default-dispatcher-3] WARN  org.apache.spark.deploy.client.AppClient$ClientActor - Could not connect to akka.tcp://sparkMaster@ip-10-121-25-177:7077: akka.remote.InvalidAssociation: Invalid address: akka.tcp://sparkMaster@ip-10-121-25-177:7077
2015-02-28 17:44:23,588 [sparkDriver-akka.actor.default-dispatcher-3] WARN  Remoting - Tried to associate with unreachable remote address [akka.tcp://sparkMaster@ip-10-121-25-177:7077]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: ip-10-121-25-177: nodename nor servname provided, or not known
2015-02-28 17:44:51,286 [main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-02-28 17:44:51,333 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-02-28 17:44:51,334 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-02-28 17:44:51,335 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-02-28 17:44:51,700 [sparkDriver-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-02-28 17:44:51,761 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Starting remoting
2015-02-28 17:44:51,996 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:50655]
2015-02-28 17:44:52,003 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50655.
2015-02-28 17:44:52,027 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-02-28 17:44:52,043 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-02-28 17:44:52,060 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150228174452-c988
2015-02-28 17:44:52,067 [main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-02-28 17:44:52,427 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-28 17:44:52,563 [main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-96a0f4a6-fcee-4900-bfbc-754a17e75b94
2015-02-28 17:44:52,572 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-02-28 17:44:52,729 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 17:44:52,751 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:50656
2015-02-28 17:44:52,752 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 50656.
2015-02-28 17:44:52,885 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 17:44:52,914 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-02-28 17:44:52,914 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-02-28 17:44:52,916 [main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-02-28 17:44:53,056 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:50655/user/HeartbeatReceiver
2015-02-28 17:44:53,293 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50657
2015-02-28 17:44:53,295 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-02-28 17:44:53,297 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:50657 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 50657)
2015-02-28 17:44:53,299 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-02-28 18:02:24,778 [main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-02-28 18:02:25,140 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-02-28 18:02:25,141 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-02-28 18:02:25,141 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-02-28 18:02:25,539 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-02-28 18:02:25,608 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-02-28 18:02:25,889 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:50816]
2015-02-28 18:02:25,897 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50816.
2015-02-28 18:02:25,932 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-02-28 18:02:25,947 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-02-28 18:02:25,963 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150228180225-d6cd
2015-02-28 18:02:25,968 [main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-02-28 18:02:26,360 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-28 18:02:26,508 [main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-3a03c204-3f0e-458a-8435-85a3642b8623
2015-02-28 18:02:26,523 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-02-28 18:02:26,760 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 18:02:26,790 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:50820
2015-02-28 18:02:26,791 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 50820.
2015-02-28 18:02:26,996 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 18:02:27,022 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-02-28 18:02:27,022 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-02-28 18:02:27,028 [main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-02-28 18:02:27,210 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:50816/user/HeartbeatReceiver
2015-02-28 18:02:27,455 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50821
2015-02-28 18:02:27,458 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-02-28 18:02:27,459 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:50821 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 50821)
2015-02-28 18:02:27,461 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-02-28 18:02:33,274 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-02-28 18:02:33,285 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-02-28 18:02:34,227 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-02-28 18:02:34,230 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-02-28 18:02:34,233 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50821 (size: 13.6 KB, free: 983.1 MB)
2015-02-28 18:02:34,235 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-02-28 18:02:34,254 [main] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:53
2015-02-28 18:02:48,958 [main] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-02-28 18:02:49,253 [main] INFO  org.apache.spark.SparkContext - Starting job: take at ProfileTrimmer.scala:55
2015-02-28 18:02:49,289 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at ProfileTrimmer.scala:55) with 1 output partitions (allowLocal=true)
2015-02-28 18:02:49,289 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(take at ProfileTrimmer.scala:55)
2015-02-28 18:02:49,290 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-02-28 18:02:49,299 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-02-28 18:02:49,310 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[2] at map at ProfileTrimmer.scala:54), which has no missing parents
2015-02-28 18:02:49,315 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2752) called with curMem=152562, maxMem=1030823608
2015-02-28 18:02:49,316 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 2.7 KB, free 982.9 MB)
2015-02-28 18:02:49,325 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1726) called with curMem=155314, maxMem=1030823608
2015-02-28 18:02:49,326 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1726.0 B, free 982.9 MB)
2015-02-28 18:02:49,327 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50821 (size: 1726.0 B, free: 983.1 MB)
2015-02-28 18:02:49,327 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-02-28 18:02:49,329 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-02-28 18:02:49,342 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[2] at map at ProfileTrimmer.scala:54)
2015-02-28 18:02:49,344 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-02-28 18:02:49,379 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-02-28 18:02:49,389 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-02-28 18:02:49,437 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-02-28 18:02:49,452 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-02-28 18:02:49,452 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-02-28 18:02:49,452 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-02-28 18:02:49,453 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-02-28 18:02:49,453 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-02-28 18:02:49,503 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 52759 bytes result sent to driver
2015-02-28 18:02:49,526 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 141 ms on localhost (1/1)
2015-02-28 18:02:49,528 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (take at ProfileTrimmer.scala:55) finished in 0.167 s
2015-02-28 18:02:49,529 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-02-28 18:02:49,541 [main] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at ProfileTrimmer.scala:55, took 0.287021 s
2015-02-28 18:04:30,790 [main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-02-28 18:04:31,130 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-02-28 18:04:31,131 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-02-28 18:04:31,132 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-02-28 18:04:31,541 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-02-28 18:04:31,625 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-02-28 18:04:31,908 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:50831]
2015-02-28 18:04:31,916 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50831.
2015-02-28 18:04:31,950 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-02-28 18:04:31,968 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-02-28 18:04:31,987 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150228180431-038d
2015-02-28 18:04:31,992 [main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-02-28 18:04:32,336 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-28 18:04:32,478 [main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-265bad64-0f11-4ce5-8c33-7e27c3ee48d2
2015-02-28 18:04:32,493 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-02-28 18:04:32,680 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 18:04:32,721 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:50832
2015-02-28 18:04:32,721 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 50832.
2015-02-28 18:04:32,861 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 18:04:32,878 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-02-28 18:04:32,879 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-02-28 18:04:32,882 [main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-02-28 18:04:33,065 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:50831/user/HeartbeatReceiver
2015-02-28 18:04:33,287 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50833
2015-02-28 18:04:33,290 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-02-28 18:04:33,292 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:50833 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 50833)
2015-02-28 18:04:33,295 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-02-28 18:04:38,758 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-02-28 18:04:38,769 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-02-28 18:04:39,573 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-02-28 18:04:39,575 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-02-28 18:04:39,579 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50833 (size: 13.6 KB, free: 983.1 MB)
2015-02-28 18:04:39,583 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-02-28 18:04:39,608 [main] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:53
2015-02-28 18:04:42,887 [main] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-02-28 18:04:43,161 [main] INFO  org.apache.spark.SparkContext - Starting job: take at ProfileTrimmer.scala:55
2015-02-28 18:04:43,182 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at ProfileTrimmer.scala:55) with 1 output partitions (allowLocal=true)
2015-02-28 18:04:43,182 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(take at ProfileTrimmer.scala:55)
2015-02-28 18:04:43,183 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-02-28 18:04:43,189 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-02-28 18:04:43,196 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[2] at map at ProfileTrimmer.scala:54), which has no missing parents
2015-02-28 18:04:43,201 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2752) called with curMem=152562, maxMem=1030823608
2015-02-28 18:04:43,202 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 2.7 KB, free 982.9 MB)
2015-02-28 18:04:43,209 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1726) called with curMem=155314, maxMem=1030823608
2015-02-28 18:04:43,210 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1726.0 B, free 982.9 MB)
2015-02-28 18:04:43,211 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50833 (size: 1726.0 B, free: 983.1 MB)
2015-02-28 18:04:43,211 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-02-28 18:04:43,213 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-02-28 18:04:43,226 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[2] at map at ProfileTrimmer.scala:54)
2015-02-28 18:04:43,227 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-02-28 18:04:43,252 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-02-28 18:04:43,261 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-02-28 18:04:43,295 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-02-28 18:04:43,303 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-02-28 18:04:43,303 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-02-28 18:04:43,303 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-02-28 18:04:43,303 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-02-28 18:04:43,303 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-02-28 18:04:43,366 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 52759 bytes result sent to driver
2015-02-28 18:04:43,392 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 131 ms on localhost (1/1)
2015-02-28 18:04:43,393 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (take at ProfileTrimmer.scala:55) finished in 0.157 s
2015-02-28 18:04:43,394 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-02-28 18:04:43,404 [main] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: take at ProfileTrimmer.scala:55, took 0.241614 s
2015-02-28 18:24:44,140 [main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-02-28 18:24:44,501 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-02-28 18:24:44,502 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-02-28 18:24:44,502 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-02-28 18:24:44,861 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-02-28 18:24:44,925 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-02-28 18:24:45,144 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:51045]
2015-02-28 18:24:45,151 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 51045.
2015-02-28 18:24:45,174 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-02-28 18:24:45,187 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-02-28 18:24:45,206 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150228182445-8604
2015-02-28 18:24:45,211 [main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-02-28 18:24:45,535 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-28 18:24:45,666 [main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-bbf0d277-2493-4077-b043-511a3a19e44b
2015-02-28 18:24:45,676 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-02-28 18:24:45,839 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 18:24:45,859 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:51046
2015-02-28 18:24:45,860 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 51046.
2015-02-28 18:24:45,990 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 18:24:46,001 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-02-28 18:24:46,001 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-02-28 18:24:46,004 [main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-02-28 18:24:46,165 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:51045/user/HeartbeatReceiver
2015-02-28 18:24:46,343 [main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 51047
2015-02-28 18:24:46,346 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-02-28 18:24:46,348 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:51047 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 51047)
2015-02-28 18:24:46,350 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-02-28 18:24:46,700 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-02-28 18:24:46,703 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-02-28 18:24:46,932 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-02-28 18:24:46,933 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-02-28 18:24:46,935 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:51047 (size: 13.6 KB, free: 983.1 MB)
2015-02-28 18:24:46,936 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-02-28 18:24:46,940 [main] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:53
2015-02-28 18:24:47,138 [main] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-02-28 18:24:47,153 [main] INFO  org.apache.spark.SparkContext - Starting job: count at ProfileTrimmer.scala:70
2015-02-28 18:24:47,167 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ProfileTrimmer.scala:70) with 1 output partitions (allowLocal=false)
2015-02-28 18:24:47,168 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at ProfileTrimmer.scala:70)
2015-02-28 18:24:47,168 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-02-28 18:24:47,173 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-02-28 18:24:47,179 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MapPartitionsRDD[3] at mapPartitionsWithIndex at ProfileTrimmer.scala:56), which has no missing parents
2015-02-28 18:24:47,219 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3032) called with curMem=152562, maxMem=1030823608
2015-02-28 18:24:47,219 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 982.9 MB)
2015-02-28 18:24:47,226 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1825) called with curMem=155594, maxMem=1030823608
2015-02-28 18:24:47,227 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1825.0 B, free 982.9 MB)
2015-02-28 18:24:47,228 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:51047 (size: 1825.0 B, free: 983.1 MB)
2015-02-28 18:24:47,228 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-02-28 18:24:47,229 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-02-28 18:24:47,237 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[3] at mapPartitionsWithIndex at ProfileTrimmer.scala:56)
2015-02-28 18:24:47,239 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-02-28 18:24:47,264 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-02-28 18:24:47,271 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-02-28 18:24:47,291 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-02-28 18:24:47,301 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-02-28 18:24:47,301 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-02-28 18:24:47,302 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-02-28 18:24:47,302 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-02-28 18:24:47,302 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-02-28 18:24:47,370 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-02-28 18:24:47,387 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 118 ms on localhost (1/1)
2015-02-28 18:24:47,387 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at ProfileTrimmer.scala:70) finished in 0.135 s
2015-02-28 18:24:47,388 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-02-28 18:24:47,393 [main] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ProfileTrimmer.scala:70, took 0.239176 s
2015-02-28 19:46:27,134 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-02-28 19:46:27,580 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-02-28 19:46:27,581 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-02-28 19:46:27,582 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-02-28 19:46:28,125 [sparkDriver-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-02-28 19:46:28,229 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Starting remoting
2015-02-28 19:46:28,569 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:54002]
2015-02-28 19:46:28,580 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 54002.
2015-02-28 19:46:28,620 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-02-28 19:46:28,646 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-02-28 19:46:28,682 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150228194628-6b26
2015-02-28 19:46:28,693 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-02-28 19:46:29,243 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-28 19:46:29,376 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-ff26c261-51ad-463f-9e9d-27a9d2966d2a
2015-02-28 19:46:29,391 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-02-28 19:46:29,573 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 19:46:29,594 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:54003
2015-02-28 19:46:29,594 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 54003.
2015-02-28 19:46:29,755 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-02-28 19:46:29,769 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-02-28 19:46:29,770 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-02-28 19:46:29,772 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-02-28 19:46:29,934 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:54002/user/HeartbeatReceiver
2015-02-28 19:46:30,131 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 54004
2015-02-28 19:46:30,134 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-02-28 19:46:30,135 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:54004 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 54004)
2015-02-28 19:46:30,138 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-02-28 19:46:30,568 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-02-28 19:46:30,570 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-02-28 19:46:30,763 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-02-28 19:46:30,764 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-02-28 19:46:30,766 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:54004 (size: 13.6 KB, free: 983.1 MB)
2015-02-28 19:46:30,767 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-02-28 19:46:30,772 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:53
2015-02-28 19:46:31,042 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-02-28 19:46:31,055 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-02-28 19:46:31,069 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-02-28 19:46:31,070 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-02-28 19:46:31,071 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-02-28 19:46:31,079 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-02-28 19:46:31,085 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MapPartitionsRDD[3] at mapPartitionsWithIndex at ProfileTrimmer.scala:56), which has no missing parents
2015-02-28 19:46:31,135 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3032) called with curMem=152562, maxMem=1030823608
2015-02-28 19:46:31,135 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 982.9 MB)
2015-02-28 19:46:31,144 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1825) called with curMem=155594, maxMem=1030823608
2015-02-28 19:46:31,145 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1825.0 B, free 982.9 MB)
2015-02-28 19:46:31,146 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:54004 (size: 1825.0 B, free: 983.1 MB)
2015-02-28 19:46:31,146 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-02-28 19:46:31,148 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-02-28 19:46:31,158 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[3] at mapPartitionsWithIndex at ProfileTrimmer.scala:56)
2015-02-28 19:46:31,159 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-02-28 19:46:31,183 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-02-28 19:46:31,190 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-02-28 19:46:31,216 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-02-28 19:46:31,223 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-02-28 19:46:31,223 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-02-28 19:46:31,223 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-02-28 19:46:31,223 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-02-28 19:46:31,223 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-02-28 19:46:31,285 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-02-28 19:46:31,307 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 120 ms on localhost (1/1)
2015-02-28 19:46:31,309 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-02-28 19:46:31,309 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.141 s
2015-02-28 19:46:31,315 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.259427 s
2015-03-01 12:11:01,827 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 12:11:02,194 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 12:11:02,195 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 12:11:02,196 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 12:11:02,813 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 12:11:02,937 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 12:11:03,345 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56362]
2015-03-01 12:11:03,360 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56362.
2015-03-01 12:11:03,413 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 12:11:03,443 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 12:11:03,476 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301121103-14f9
2015-03-01 12:11:03,484 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 12:11:04,582 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 12:11:04,783 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-aa52ac27-6494-4021-a9bc-83df24c4fa62
2015-03-01 12:11:04,810 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 12:11:05,125 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 12:11:05,159 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56363
2015-03-01 12:11:05,160 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56363.
2015-03-01 12:11:05,378 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 12:11:05,398 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 12:11:05,398 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 12:11:05,402 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 12:11:05,580 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56362/user/HeartbeatReceiver
2015-03-01 12:11:05,875 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56364
2015-03-01 12:11:05,878 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 12:11:05,881 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56364 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56364)
2015-03-01 12:11:05,885 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 12:11:06,493 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 12:11:06,496 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 12:11:06,784 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 12:11:06,785 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 12:11:06,787 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56364 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 12:11:06,787 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 12:11:06,796 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 12:11:07,171 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 12:11:07,193 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 12:11:07,222 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 12:11:07,223 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 12:11:07,224 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 12:11:07,233 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 12:11:07,247 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 12:11:07,298 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 12:11:07,299 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 12:11:07,309 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 12:11:07,309 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 12:11:07,310 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56364 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 12:11:07,311 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 12:11:07,313 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 12:11:07,326 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 12:11:07,328 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 12:11:07,368 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 12:11:07,379 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 12:11:07,419 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 12:11:07,436 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 12:11:07,436 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 12:11:07,436 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 12:11:07,436 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 12:11:07,436 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 12:11:07,461 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""1";0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;2;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$2.apply(ProfileTrimmer.scala:68)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 12:11:07,519 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NumberFormatException: For input string: ""1";0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;2;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$2.apply(ProfileTrimmer.scala:68)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 12:11:07,525 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 12:11:07,528 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 12:11:07,534 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 12:11:07,541 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.347104 s
2015-03-01 12:56:48,167 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 12:56:48,495 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 12:56:48,496 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 12:56:48,496 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 12:56:48,856 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 12:56:48,936 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 12:56:49,188 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56602]
2015-03-01 12:56:49,198 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56602.
2015-03-01 12:56:49,228 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 12:56:49,247 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 12:56:49,271 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301125649-713f
2015-03-01 12:56:49,276 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 12:56:49,819 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 12:56:49,957 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-3f7cc070-cd37-438f-b1f3-e144e8085e51
2015-03-01 12:56:49,967 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 12:56:50,120 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 12:56:50,140 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56603
2015-03-01 12:56:50,140 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56603.
2015-03-01 12:56:50,270 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 12:56:50,284 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 12:56:50,284 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 12:56:50,287 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 12:56:50,413 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56602/user/HeartbeatReceiver
2015-03-01 12:56:50,598 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56604
2015-03-01 12:56:50,600 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 12:56:50,602 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56604 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56604)
2015-03-01 12:56:50,604 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 12:56:50,990 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 12:56:50,992 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 12:56:51,175 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 12:56:51,176 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 12:56:51,178 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56604 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 12:56:51,178 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 12:56:51,184 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 12:56:51,444 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 12:56:51,460 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 12:56:51,475 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 12:56:51,476 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 12:56:51,476 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 12:56:51,482 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 12:56:51,488 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 12:56:51,527 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 12:56:51,527 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 12:56:51,535 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 12:56:51,535 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 12:56:51,536 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56604 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 12:56:51,537 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 12:56:51,538 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 12:56:51,545 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 12:56:51,547 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 12:56:51,575 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 12:56:51,580 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 12:56:51,601 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 12:56:51,608 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 12:56:51,608 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 12:56:51,608 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 12:56:51,608 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 12:56:51,609 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 12:56:51,624 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""1";0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;2;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:69)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 12:56:51,669 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NumberFormatException: For input string: ""1";0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;2;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;4;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;3;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:69)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 12:56:51,672 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 12:56:51,675 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 12:56:51,678 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 12:56:51,682 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.221603 s
2015-03-01 13:00:12,861 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:00:13,197 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:00:13,198 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:00:13,199 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:00:13,567 [sparkDriver-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:00:13,632 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Starting remoting
2015-03-01 13:00:13,865 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56619]
2015-03-01 13:00:13,874 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56619.
2015-03-01 13:00:13,904 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:00:13,921 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:00:13,950 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301130013-f6d2
2015-03-01 13:00:13,955 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:00:14,295 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:00:14,430 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-24fc1321-a2dd-4710-89d2-5a4325fb0fe0
2015-03-01 13:00:14,441 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:00:14,601 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:00:14,622 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56620
2015-03-01 13:00:14,622 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56620.
2015-03-01 13:00:14,773 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:00:14,786 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:00:14,787 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:00:14,789 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:00:14,918 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56619/user/HeartbeatReceiver
2015-03-01 13:00:15,118 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56621
2015-03-01 13:00:15,120 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:00:15,122 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56621 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56621)
2015-03-01 13:00:15,125 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:00:15,500 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:00:15,503 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:00:15,682 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:00:15,683 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:00:15,685 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56621 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:00:15,685 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:00:15,690 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:00:15,950 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:00:15,964 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:00:15,977 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:00:15,978 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:00:15,978 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:00:15,984 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:00:15,990 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:00:16,033 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:00:16,034 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:00:16,040 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:00:16,041 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:00:16,042 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56621 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:00:16,043 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:00:16,044 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:00:16,052 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:00:16,055 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:00:16,082 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:00:16,089 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:00:16,113 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:00:16,124 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:00:16,124 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:00:16,124 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:00:16,124 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:00:16,124 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:00:16,524 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-03-01 13:00:16,540 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 454 ms on localhost (1/1)
2015-03-01 13:00:16,541 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:00:16,542 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.474 s
2015-03-01 13:00:16,551 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.586889 s
2015-03-01 13:02:03,498 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:02:03,537 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:02:03,537 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:02:03,538 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:02:03,916 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:02:03,982 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 13:02:04,219 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56626]
2015-03-01 13:02:04,228 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56626.
2015-03-01 13:02:04,256 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:02:04,276 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:02:04,299 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301130204-dd0a
2015-03-01 13:02:04,304 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:02:04,633 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:02:04,762 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-33375b27-6bff-4084-8846-2c90d6d8e31e
2015-03-01 13:02:04,774 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:02:04,943 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:02:04,962 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56627
2015-03-01 13:02:04,962 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56627.
2015-03-01 13:02:05,102 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:02:05,115 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:02:05,115 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:02:05,118 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:02:05,251 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56626/user/HeartbeatReceiver
2015-03-01 13:02:05,452 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56628
2015-03-01 13:02:05,454 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:02:05,455 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56628 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56628)
2015-03-01 13:02:05,458 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:02:05,846 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:02:05,849 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:02:06,050 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:02:06,051 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:02:06,053 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56628 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:02:06,054 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:02:06,058 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:02:06,308 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:02:06,324 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:02:06,339 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:02:06,339 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:02:06,339 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:02:06,345 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:02:06,352 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:02:06,396 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:02:06,397 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:02:06,404 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:02:06,405 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:02:06,406 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56628 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:02:06,406 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:02:06,408 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:02:06,416 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:02:06,418 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:02:06,443 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:02:06,449 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:02:06,475 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:02:06,483 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:02:06,483 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:02:06,483 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:02:06,483 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:02:06,483 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:02:06,954 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-03-01 13:02:06,969 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 524 ms on localhost (1/1)
2015-03-01 13:02:06,971 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.542 s
2015-03-01 13:02:06,971 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:02:06,978 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.653204 s
2015-03-01 13:03:00,716 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:03:00,745 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:03:00,746 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:03:00,747 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:03:01,118 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:03:01,190 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 13:03:01,455 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56635]
2015-03-01 13:03:01,465 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56635.
2015-03-01 13:03:01,498 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:03:01,518 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:03:01,537 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301130301-023a
2015-03-01 13:03:01,541 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:03:01,882 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:03:02,033 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-98cf83df-7177-4f75-b15c-c5b81f01724b
2015-03-01 13:03:02,046 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:03:02,255 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:03:02,281 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56636
2015-03-01 13:03:02,282 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56636.
2015-03-01 13:03:02,453 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:03:02,469 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:03:02,469 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:03:02,473 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:03:02,644 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56635/user/HeartbeatReceiver
2015-03-01 13:03:02,849 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56637
2015-03-01 13:03:02,851 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:03:02,852 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56637 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56637)
2015-03-01 13:03:02,855 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:03:03,236 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:03:03,238 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:03:03,424 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:03:03,425 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:03:03,427 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56637 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:03:03,428 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:03:03,433 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:03:03,694 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:03:03,709 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:03:03,722 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:03:03,723 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:03:03,724 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:03:03,731 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:03:03,738 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:03:03,780 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:03:03,781 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:03:03,788 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:03:03,789 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:03:03,789 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56637 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:03:03,790 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:03:03,791 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:03:03,799 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:03:03,800 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:03:03,825 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:03:03,831 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:03:03,856 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:03:03,867 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:03:03,867 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:03:03,867 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:03:03,867 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:03:03,867 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:05:22,619 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:05:22,960 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:05:22,961 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:05:22,961 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:05:23,311 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:05:23,376 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 13:05:23,620 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56646]
2015-03-01 13:05:23,632 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56646.
2015-03-01 13:05:23,667 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:05:23,689 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:05:23,710 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301130523-8634
2015-03-01 13:05:23,716 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:05:24,040 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:05:24,182 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-cf57d849-f117-420a-99f6-fb5065d69ad6
2015-03-01 13:05:24,190 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:05:24,349 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:05:24,372 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56647
2015-03-01 13:05:24,372 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56647.
2015-03-01 13:05:24,498 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:05:24,516 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:05:24,516 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:05:24,518 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:05:24,651 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56646/user/HeartbeatReceiver
2015-03-01 13:05:24,836 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56648
2015-03-01 13:05:24,838 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:05:24,840 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56648 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56648)
2015-03-01 13:05:24,843 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:05:25,229 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:05:25,231 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:05:25,417 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:05:25,417 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:05:25,419 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56648 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:05:25,420 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:05:25,424 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:05:25,678 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:05:25,690 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:05:25,704 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:05:25,704 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:05:25,705 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:05:25,713 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:05:25,720 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:05:25,763 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:05:25,763 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:05:25,771 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:05:25,772 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:05:25,773 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56648 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:05:25,773 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:05:25,775 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:05:25,784 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:05:25,786 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:05:25,814 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:05:25,821 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:05:25,849 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:05:25,856 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:05:25,857 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:05:25,857 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:05:25,857 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:05:25,857 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:05:25,908 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""1""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:69)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:05:25,950 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NumberFormatException: For input string: ""1""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:69)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:05:25,954 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:05:25,956 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:05:25,960 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:05:25,964 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.273510 s
2015-03-01 13:11:44,497 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:11:44,835 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:11:44,836 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:11:44,836 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:11:45,311 [sparkDriver-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:11:45,394 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Starting remoting
2015-03-01 13:11:45,710 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56677]
2015-03-01 13:11:45,724 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56677.
2015-03-01 13:11:45,765 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:11:45,787 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:11:45,812 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301131145-ca6a
2015-03-01 13:11:45,819 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:11:46,230 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:11:46,396 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-cb6ac031-0765-4956-833c-be3a9db3af04
2015-03-01 13:11:46,409 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:11:46,619 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:11:46,647 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56678
2015-03-01 13:11:46,647 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56678.
2015-03-01 13:11:46,830 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:11:46,847 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:11:46,848 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:11:46,852 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:11:47,031 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56677/user/HeartbeatReceiver
2015-03-01 13:11:47,331 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56679
2015-03-01 13:11:47,333 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:11:47,336 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56679 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56679)
2015-03-01 13:11:47,339 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:11:47,842 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:11:47,844 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:11:48,067 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:11:48,068 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:11:48,072 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56679 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:11:48,073 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:11:48,079 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:11:48,389 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:11:48,406 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:11:48,424 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:11:48,425 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:11:48,426 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:11:48,433 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:11:48,444 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:11:48,500 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:11:48,500 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:11:48,510 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:11:48,511 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:11:48,512 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56679 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:11:48,513 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:11:48,514 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:11:48,525 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:11:48,527 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:11:48,559 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:11:48,565 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:11:48,600 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:11:48,611 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:11:48,611 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:11:48,611 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:11:48,611 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:11:48,611 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:11:48,676 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""1"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:70)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:11:48,734 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NumberFormatException: For input string: ""1"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:70)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:11:48,737 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:11:48,740 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:11:48,755 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:11:48,763 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.357078 s
2015-03-01 13:24:08,872 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:24:09,187 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:24:09,188 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:24:09,190 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:24:09,577 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:24:09,649 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 13:24:09,913 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56842]
2015-03-01 13:24:09,923 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56842.
2015-03-01 13:24:09,951 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:24:09,973 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:24:09,991 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301132409-87e8
2015-03-01 13:24:09,996 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:24:10,333 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:24:10,467 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-85478f58-ac53-437e-9a27-28f35288b8b4
2015-03-01 13:24:10,479 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:24:10,645 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:24:10,665 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56843
2015-03-01 13:24:10,665 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56843.
2015-03-01 13:24:10,802 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:24:10,816 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:24:10,816 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:24:10,818 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:24:10,957 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56842/user/HeartbeatReceiver
2015-03-01 13:24:11,162 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56844
2015-03-01 13:24:11,164 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:24:11,165 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56844 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56844)
2015-03-01 13:24:11,168 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:24:11,569 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:24:11,571 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:24:11,779 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:24:11,779 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:24:11,781 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56844 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:24:11,782 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:24:11,787 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:24:12,062 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:24:12,074 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:24:12,089 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:24:12,089 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:24:12,089 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:24:12,096 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:24:12,102 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:24:12,148 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:24:12,148 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:24:12,156 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:24:12,157 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:24:12,158 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56844 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:24:12,158 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:24:12,160 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:24:12,169 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:24:12,170 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:24:12,201 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:24:12,209 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:24:12,235 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:24:12,243 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:24:12,243 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:24:12,243 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:24:12,243 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:24:12,243 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:24:12,300 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""1"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:70)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:24:12,339 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NumberFormatException: For input string: ""1"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:70)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:24:12,342 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:24:12,344 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:24:12,347 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:24:12,350 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.275568 s
2015-03-01 13:26:01,236 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:26:01,270 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:26:01,271 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:26:01,272 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:26:01,657 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:26:01,729 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 13:26:01,997 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56849]
2015-03-01 13:26:02,007 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56849.
2015-03-01 13:26:02,038 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:26:02,054 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:26:02,073 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301132602-e4d5
2015-03-01 13:26:02,079 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:26:02,454 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:26:02,588 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-0347ad29-f687-45f7-b729-ff522cc092d3
2015-03-01 13:26:02,601 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:26:02,760 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:26:02,778 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56850
2015-03-01 13:26:02,778 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56850.
2015-03-01 13:26:02,910 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:26:02,928 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:26:02,929 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:26:02,931 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:26:03,063 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56849/user/HeartbeatReceiver
2015-03-01 13:26:03,268 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56851
2015-03-01 13:26:03,270 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:26:03,272 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56851 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56851)
2015-03-01 13:26:03,275 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:26:03,651 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:26:03,653 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:26:03,836 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:26:03,837 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:26:03,839 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56851 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:26:03,840 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:26:03,845 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:26:04,099 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:26:04,116 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:26:04,132 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:26:04,132 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:26:04,133 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:26:04,141 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:26:04,148 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:26:04,194 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:26:04,195 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:26:04,201 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:26:04,202 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:26:04,203 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56851 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:26:04,203 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:26:04,205 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:26:04,213 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:26:04,214 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:26:04,236 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:26:04,241 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:26:04,267 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:26:04,274 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:26:04,275 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:26:04,275 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:26:04,275 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:26:04,275 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:26:04,326 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:504)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:70)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:26:04,365 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:504)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:70)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:26:04,368 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:26:04,369 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:26:04,374 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:26:04,379 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.261925 s
2015-03-01 13:26:27,368 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:26:27,397 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:26:27,398 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:26:27,398 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:26:27,820 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:26:27,896 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 13:26:28,188 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56858]
2015-03-01 13:26:28,200 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56858.
2015-03-01 13:26:28,230 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:26:28,248 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:26:28,276 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301132628-92cc
2015-03-01 13:26:28,281 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:26:28,641 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:26:28,784 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-6ef4a0d6-55e3-4cf6-984c-95bc7811be2a
2015-03-01 13:26:28,793 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:26:28,972 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:26:28,996 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56859
2015-03-01 13:26:28,997 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56859.
2015-03-01 13:26:29,146 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:26:29,164 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:26:29,164 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:26:29,168 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:26:29,320 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56858/user/HeartbeatReceiver
2015-03-01 13:26:29,533 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56860
2015-03-01 13:26:29,535 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:26:29,537 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56860 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56860)
2015-03-01 13:26:29,539 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:26:29,995 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:26:29,998 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:26:30,232 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:26:30,232 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:26:30,235 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56860 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:26:30,236 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:26:30,242 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:26:30,569 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:26:30,585 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:26:30,604 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:26:30,605 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:26:30,606 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:26:30,614 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:26:30,624 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:26:30,667 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:26:30,667 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:26:30,675 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:26:30,676 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:26:30,677 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56860 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:26:30,677 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:26:30,678 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:26:30,690 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:26:30,692 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:26:30,723 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:26:30,733 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:26:30,770 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:26:30,782 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:26:30,782 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:26:30,783 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:26:30,783 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:26:30,783 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:27:38,300 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:27:38,329 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:27:38,330 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:27:38,330 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:27:38,730 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:27:38,799 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 13:27:39,080 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56864]
2015-03-01 13:27:39,091 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56864.
2015-03-01 13:27:39,131 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:27:39,150 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:27:39,172 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301132739-270a
2015-03-01 13:27:39,177 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:27:39,522 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:27:39,673 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-ad88ff1f-8a9c-4dff-a8d6-63ab4a146bba
2015-03-01 13:27:39,684 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:27:39,856 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:27:39,876 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56865
2015-03-01 13:27:39,877 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56865.
2015-03-01 13:27:40,024 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:27:40,036 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:27:40,037 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:27:40,040 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:27:40,173 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56864/user/HeartbeatReceiver
2015-03-01 13:27:40,389 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56866
2015-03-01 13:27:40,391 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:27:40,393 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56866 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56866)
2015-03-01 13:27:40,396 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:27:40,805 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:27:40,808 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:27:41,044 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:27:41,049 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:27:41,054 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56866 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:27:41,058 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:27:41,066 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:27:41,402 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:27:41,416 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:27:41,431 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:27:41,431 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:27:41,432 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:27:41,439 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:27:41,448 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:27:41,496 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:27:41,496 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:27:41,503 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:27:41,504 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:27:41,505 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56866 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:27:41,506 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:27:41,507 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:27:41,516 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:27:41,518 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:27:41,548 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:27:41,556 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:27:41,580 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:27:41,589 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:27:41,589 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:27:41,589 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:27:41,589 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:27:41,589 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:27:41,646 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:504)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:72)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:27:41,687 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NumberFormatException: For input string: ""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:504)
	at java.lang.Integer.parseInt(Integer.java:527)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:72)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:27:41,690 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:27:41,692 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:27:41,695 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:27:41,699 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.282525 s
2015-03-01 13:28:14,851 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:28:15,172 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:28:15,172 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:28:15,173 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:28:15,537 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:28:15,608 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 13:28:15,859 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56873]
2015-03-01 13:28:15,869 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56873.
2015-03-01 13:28:15,903 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:28:15,925 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:28:15,944 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301132815-401a
2015-03-01 13:28:15,948 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:28:16,288 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:28:16,418 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-a9cb5778-a5c6-4d7f-a22f-78073637cd13
2015-03-01 13:28:16,435 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:28:16,595 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:28:16,614 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56874
2015-03-01 13:28:16,614 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56874.
2015-03-01 13:28:16,757 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:28:16,770 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:28:16,770 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:28:16,772 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:28:16,917 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56873/user/HeartbeatReceiver
2015-03-01 13:28:17,100 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56875
2015-03-01 13:28:17,103 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:28:17,104 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56875 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56875)
2015-03-01 13:28:17,107 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:28:17,514 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:28:17,516 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:28:17,707 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:28:17,708 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:28:17,711 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56875 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:28:17,711 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:28:17,718 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:28:17,974 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:28:17,987 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:28:18,001 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:28:18,001 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:28:18,002 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:28:18,008 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:28:18,015 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:28:18,059 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:28:18,060 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:28:18,069 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:28:18,069 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:28:18,070 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56875 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:28:18,071 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:28:18,073 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:28:18,081 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:28:18,083 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:28:18,108 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:28:18,113 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:28:18,138 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:28:18,148 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:28:18,148 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:28:18,148 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:28:18,148 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:28:18,148 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:28:18,201 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:75)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:28:18,238 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:75)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:28:18,241 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:28:18,243 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:28:18,245 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:28:18,250 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.262025 s
2015-03-01 13:30:00,571 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:30:00,608 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:30:00,609 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:30:00,610 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:30:00,995 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:30:01,061 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 13:30:01,319 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56881]
2015-03-01 13:30:01,330 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56881.
2015-03-01 13:30:01,360 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:30:01,379 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:30:01,401 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301133001-1e04
2015-03-01 13:30:01,405 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:30:01,738 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:30:01,873 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-69be22d9-1f3c-4c72-93cb-14fab852b0d7
2015-03-01 13:30:01,885 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:30:02,038 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:30:02,056 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56882
2015-03-01 13:30:02,056 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56882.
2015-03-01 13:30:02,205 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:30:02,218 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:30:02,218 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:30:02,222 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:30:02,347 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56881/user/HeartbeatReceiver
2015-03-01 13:30:02,545 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56883
2015-03-01 13:30:02,547 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:30:02,549 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56883 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56883)
2015-03-01 13:30:02,552 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:30:02,951 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:30:02,953 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:30:03,137 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:30:03,137 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:30:03,139 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56883 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:30:03,140 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:30:03,144 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:30:03,415 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:30:03,430 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:30:03,442 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:30:03,443 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:30:03,443 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:30:03,449 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:30:03,455 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:30:03,494 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:30:03,495 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:30:03,502 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:30:03,502 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:30:03,503 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56883 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:30:03,504 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:30:03,506 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:30:03,517 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:30:03,519 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:30:03,542 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:30:03,548 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:30:03,573 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:30:03,583 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:30:03,583 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:30:03,583 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:30:03,583 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:30:03,583 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:30:03,648 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:75)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:30:03,684 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:75)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:68)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:30:03,687 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:30:03,689 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:30:03,694 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:30:03,697 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.267400 s
2015-03-01 13:30:55,791 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 13:30:55,821 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 13:30:55,822 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 13:30:55,823 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 13:30:56,228 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 13:30:56,304 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 13:30:56,596 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:56889]
2015-03-01 13:30:56,605 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56889.
2015-03-01 13:30:56,636 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 13:30:56,654 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 13:30:56,679 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301133056-78c5
2015-03-01 13:30:56,684 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 13:30:57,033 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 13:30:57,168 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-1eeee6e3-155e-44cb-983f-c6eaa45897f6
2015-03-01 13:30:57,180 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 13:30:57,338 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:30:57,359 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:56890
2015-03-01 13:30:57,359 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 56890.
2015-03-01 13:30:57,507 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 13:30:57,521 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 13:30:57,521 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 13:30:57,523 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 13:30:57,650 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:56889/user/HeartbeatReceiver
2015-03-01 13:30:57,842 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 56891
2015-03-01 13:30:57,844 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 13:30:57,845 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:56891 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 56891)
2015-03-01 13:30:57,847 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 13:30:58,237 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 13:30:58,239 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 13:30:58,433 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 13:30:58,433 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 13:30:58,436 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:56891 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 13:30:58,437 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 13:30:58,441 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 13:30:58,700 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 13:30:58,713 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 13:30:58,724 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 13:30:58,725 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 13:30:58,725 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 13:30:58,731 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 13:30:58,738 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 13:30:58,787 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 13:30:58,787 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 13:30:58,797 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 13:30:58,797 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 13:30:58,798 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:56891 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 13:30:58,799 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 13:30:58,800 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 13:30:58,808 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 13:30:58,810 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 13:30:58,837 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 13:30:58,844 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 13:30:58,871 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 13:30:58,879 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 13:30:58,879 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 13:30:58,879 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 13:30:58,879 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 13:30:58,879 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 13:30:58,933 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:77)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:69)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 13:30:58,965 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:77)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:69)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 13:30:58,968 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 13:30:58,970 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 13:30:58,974 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 13:30:58,978 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.265398 s
2015-03-01 17:34:45,014 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 17:34:45,325 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 17:34:45,326 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 17:34:45,327 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 17:34:45,720 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 17:34:45,786 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 17:34:46,049 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:57983]
2015-03-01 17:34:46,057 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57983.
2015-03-01 17:34:46,085 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 17:34:46,104 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 17:34:46,129 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301173446-37d2
2015-03-01 17:34:46,135 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 17:34:46,650 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 17:34:46,783 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-965d6069-4e78-4e30-ba46-eb472950bf8c
2015-03-01 17:34:46,797 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 17:34:46,960 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:34:46,980 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:57984
2015-03-01 17:34:46,980 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 57984.
2015-03-01 17:34:47,151 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:34:47,164 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 17:34:47,164 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 17:34:47,166 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 17:34:47,308 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:57983/user/HeartbeatReceiver
2015-03-01 17:34:47,511 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 57985
2015-03-01 17:34:47,513 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 17:34:47,515 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:57985 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 57985)
2015-03-01 17:34:47,519 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 17:34:47,934 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 17:34:47,936 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 17:34:48,136 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 17:34:48,137 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 17:34:48,139 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:57985 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 17:34:48,139 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 17:34:48,145 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 17:34:48,438 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 17:34:48,452 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 17:34:48,464 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 17:34:48,465 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 17:34:48,465 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 17:34:48,471 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 17:34:48,478 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 17:34:48,525 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 17:34:48,525 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 17:34:48,534 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 17:34:48,535 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 17:34:48,536 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:57985 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 17:34:48,536 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 17:34:48,538 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 17:34:48,547 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 17:34:48,549 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 17:34:48,576 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 17:34:48,584 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 17:34:48,613 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 17:34:48,620 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 17:34:48,621 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 17:34:48,621 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 17:34:48,621 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 17:34:48,621 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 17:34:48,684 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:77)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:69)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 17:34:48,722 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:77)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:69)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 17:34:48,725 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 17:34:48,727 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 17:34:48,731 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 17:34:48,735 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.282019 s
2015-03-01 17:35:09,685 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 17:35:09,711 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 17:35:09,712 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 17:35:09,713 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 17:35:10,107 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 17:35:10,178 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 17:35:10,428 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:57988]
2015-03-01 17:35:10,437 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57988.
2015-03-01 17:35:10,467 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 17:35:10,486 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 17:35:10,502 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301173510-c08c
2015-03-01 17:35:10,506 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 17:35:10,844 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 17:35:10,975 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-f47c2ea7-e384-46eb-8b3f-69920bce5459
2015-03-01 17:35:10,987 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 17:35:11,145 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:35:11,165 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:57989
2015-03-01 17:35:11,165 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 57989.
2015-03-01 17:35:11,307 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:35:11,320 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 17:35:11,320 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 17:35:11,323 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 17:35:11,451 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:57988/user/HeartbeatReceiver
2015-03-01 17:35:11,664 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 57990
2015-03-01 17:35:11,667 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 17:35:11,669 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:57990 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 57990)
2015-03-01 17:35:11,672 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 17:35:12,063 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 17:35:12,066 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 17:35:12,266 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 17:35:12,267 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 17:35:12,269 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:57990 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 17:35:12,270 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 17:35:12,276 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 17:35:12,532 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 17:35:12,549 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 17:35:12,564 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 17:35:12,564 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 17:35:12,565 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 17:35:12,571 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 17:35:12,577 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 17:35:12,615 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 17:35:12,615 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 17:35:12,622 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 17:35:12,623 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 17:35:12,624 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:57990 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 17:35:12,624 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 17:35:12,626 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 17:35:12,634 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 17:35:12,636 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 17:35:12,658 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 17:35:12,664 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 17:35:12,692 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 17:35:12,702 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 17:35:12,702 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 17:35:12,702 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 17:35:12,702 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 17:35:12,703 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 17:35:12,761 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:77)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:69)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 17:35:12,797 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:77)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:69)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 17:35:12,801 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 17:35:12,803 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 17:35:12,807 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 17:35:12,811 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.261898 s
2015-03-01 17:37:26,111 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 17:37:26,457 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 17:37:26,458 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 17:37:26,459 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 17:37:26,953 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 17:37:27,052 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 17:37:27,381 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:58005]
2015-03-01 17:37:27,393 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 58005.
2015-03-01 17:37:27,432 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 17:37:27,456 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 17:37:27,489 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301173727-e71f
2015-03-01 17:37:27,496 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 17:37:27,922 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 17:37:28,099 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-64b42e63-f8f5-420d-980a-1f933f54fcea
2015-03-01 17:37:28,113 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 17:37:28,330 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:37:28,361 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:58006
2015-03-01 17:37:28,361 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 58006.
2015-03-01 17:37:28,536 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:37:28,554 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 17:37:28,554 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 17:37:28,558 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 17:37:28,756 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:58005/user/HeartbeatReceiver
2015-03-01 17:37:29,069 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58007
2015-03-01 17:37:29,072 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 17:37:29,074 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:58007 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 58007)
2015-03-01 17:37:29,077 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 17:37:29,573 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 17:37:29,576 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 17:37:29,809 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 17:37:29,810 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 17:37:29,813 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58007 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 17:37:29,814 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 17:37:29,823 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 17:37:30,161 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 17:37:30,176 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 17:37:30,195 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 17:37:30,196 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 17:37:30,196 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 17:37:30,204 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 17:37:30,213 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 17:37:30,271 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 17:37:30,272 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 17:37:30,283 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 17:37:30,283 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 17:37:30,284 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58007 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 17:37:30,285 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 17:37:30,287 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 17:37:30,299 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 17:37:30,301 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 17:37:30,332 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 17:37:30,340 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 17:37:30,376 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 17:37:30,385 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 17:37:30,385 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 17:37:30,385 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 17:37:30,385 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 17:37:30,386 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 17:37:30,451 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:80)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:72)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 17:37:30,511 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:80)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:72)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 17:37:30,515 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 17:37:30,517 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 17:37:30,523 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 17:37:30,529 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.352753 s
2015-03-01 17:38:00,173 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 17:38:00,202 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 17:38:00,203 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 17:38:00,203 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 17:38:00,700 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 17:38:00,789 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 17:38:01,126 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:58011]
2015-03-01 17:38:01,139 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 58011.
2015-03-01 17:38:01,183 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 17:38:01,209 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 17:38:01,237 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301173801-b126
2015-03-01 17:38:01,244 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 17:38:01,667 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 17:38:01,855 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-695e1b21-811e-430f-b9b2-02baa0e55ebd
2015-03-01 17:38:01,884 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 17:38:02,130 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:38:02,155 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:58012
2015-03-01 17:38:02,156 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 58012.
2015-03-01 17:38:02,327 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:38:02,345 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 17:38:02,346 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 17:38:02,349 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 17:38:02,559 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:58011/user/HeartbeatReceiver
2015-03-01 17:38:02,784 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58013
2015-03-01 17:38:02,787 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 17:38:02,790 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:58013 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 58013)
2015-03-01 17:38:02,793 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 17:38:03,325 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 17:38:03,327 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 17:38:03,551 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 17:38:03,552 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 17:38:03,554 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58013 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 17:38:03,555 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 17:38:03,562 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:56
2015-03-01 17:38:03,886 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 17:38:03,903 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 17:38:03,921 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 17:38:03,922 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 17:38:03,922 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 17:38:03,931 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 17:38:03,940 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66), which has no missing parents
2015-03-01 17:38:03,999 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 17:38:04,000 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 17:38:04,011 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 17:38:04,013 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 17:38:04,014 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58013 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 17:38:04,014 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 17:38:04,016 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 17:38:04,029 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:66)
2015-03-01 17:38:04,031 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 17:38:04,074 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 17:38:04,082 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 17:38:04,116 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 17:38:04,128 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 17:38:04,128 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 17:38:04,129 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 17:38:04,129 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 17:38:04,129 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 17:38:04,195 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:80)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:72)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 17:38:04,237 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:80)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:72)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:67)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 17:38:04,240 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 17:38:04,242 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 17:38:04,247 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 17:38:04,252 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.348405 s
2015-03-01 17:39:01,057 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 17:39:01,098 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 17:39:01,099 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 17:39:01,100 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 17:39:01,606 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 17:39:01,688 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 17:39:02,030 [sparkDriver-akka.actor.default-dispatcher-5] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:58019]
2015-03-01 17:39:02,042 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 58019.
2015-03-01 17:39:02,087 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 17:39:02,114 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 17:39:02,145 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301173902-fb5d
2015-03-01 17:39:02,152 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 17:39:02,581 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 17:39:02,759 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-ab6b3de6-7c68-4c7f-bae7-49c7d9e94ea6
2015-03-01 17:39:02,776 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 17:39:02,999 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:39:03,032 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:58020
2015-03-01 17:39:03,032 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 58020.
2015-03-01 17:39:03,219 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:39:03,234 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 17:39:03,234 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 17:39:03,238 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 17:39:03,417 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:58019/user/HeartbeatReceiver
2015-03-01 17:39:03,689 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58021
2015-03-01 17:39:03,692 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 17:39:03,694 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:58021 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 58021)
2015-03-01 17:39:03,697 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 17:39:04,236 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 17:39:04,239 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 17:39:04,465 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 17:39:04,466 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 17:39:04,468 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58021 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 17:39:04,469 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 17:39:04,476 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:61
2015-03-01 17:39:04,803 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 17:39:04,821 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 17:39:04,841 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 17:39:04,841 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 17:39:04,842 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 17:39:04,848 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 17:39:04,859 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:71), which has no missing parents
2015-03-01 17:39:04,921 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 17:39:04,922 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 17:39:04,931 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 17:39:04,932 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 17:39:04,933 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58021 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 17:39:04,934 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 17:39:04,936 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 17:39:04,950 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:71)
2015-03-01 17:39:04,953 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 17:39:04,996 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 17:39:05,003 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 17:39:05,042 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 17:39:05,055 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 17:39:05,055 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 17:39:05,055 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 17:39:05,055 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 17:39:05,055 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 17:39:05,121 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:85)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:77)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-03-01 17:39:05,165 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException: 1
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2$$anonfun$apply$1.apply$mcVI$sp(ProfileTrimmer.scala:85)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:77)
	at ml.ProfileTrimmer$$anonfun$loadProfiles$2.apply(ProfileTrimmer.scala:72)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1311)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:910)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-03-01 17:39:05,168 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2015-03-01 17:39:05,170 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 17:39:05,175 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
2015-03-01 17:39:05,182 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 failed: count at StatSpec.scala:14, took 0.361143 s
2015-03-01 17:41:50,713 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 17:41:51,063 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 17:41:51,064 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 17:41:51,064 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 17:41:51,470 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 17:41:51,539 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 17:41:51,811 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:58062]
2015-03-01 17:41:51,820 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 58062.
2015-03-01 17:41:51,850 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 17:41:51,866 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 17:41:51,892 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301174151-319d
2015-03-01 17:41:51,897 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 17:41:52,294 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 17:41:52,429 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-8cad2b20-5a74-4c29-a067-d3389d367c72
2015-03-01 17:41:52,439 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 17:41:52,605 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:41:52,629 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:58063
2015-03-01 17:41:52,629 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 58063.
2015-03-01 17:41:52,763 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 17:41:52,777 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 17:41:52,778 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 17:41:52,781 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 17:41:52,926 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:58062/user/HeartbeatReceiver
2015-03-01 17:41:53,139 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 58064
2015-03-01 17:41:53,141 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 17:41:53,143 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:58064 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 58064)
2015-03-01 17:41:53,145 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 17:41:53,529 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 17:41:53,531 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 17:41:53,729 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 17:41:53,730 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 17:41:53,732 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:58064 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 17:41:53,733 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 17:41:53,737 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:57
2015-03-01 17:41:53,998 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 17:41:54,014 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 17:41:54,029 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 17:41:54,030 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 17:41:54,030 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 17:41:54,036 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 17:41:54,042 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67), which has no missing parents
2015-03-01 17:41:54,080 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 17:41:54,080 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 17:41:54,087 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 17:41:54,088 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 17:41:54,089 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:58064 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 17:41:54,089 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 17:41:54,091 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 17:41:54,102 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67)
2015-03-01 17:41:54,104 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 17:41:54,130 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 17:41:54,138 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 17:41:54,161 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 17:41:54,170 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 17:41:54,170 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 17:41:54,170 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 17:41:54,170 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 17:41:54,170 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 17:41:54,405 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-03-01 17:41:54,421 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 291 ms on localhost (1/1)
2015-03-01 17:41:54,422 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 17:41:54,422 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.307 s
2015-03-01 17:41:54,430 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.414931 s
2015-03-01 19:59:37,932 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 19:59:38,325 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 19:59:38,327 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 19:59:38,328 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 19:59:38,870 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 19:59:38,954 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 19:59:39,321 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:59176]
2015-03-01 19:59:39,334 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 59176.
2015-03-01 19:59:39,379 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 19:59:39,401 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 19:59:39,429 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301195939-fe42
2015-03-01 19:59:39,440 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 19:59:40,106 [ScalaTest-main-running-StatSpec] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 19:59:40,286 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-f05ffdee-74fe-4b21-a5b5-81fddd4af9a9
2015-03-01 19:59:40,301 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 19:59:40,505 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 19:59:40,537 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:59177
2015-03-01 19:59:40,537 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 59177.
2015-03-01 19:59:40,738 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 19:59:40,758 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 19:59:40,758 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 19:59:40,761 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 19:59:40,910 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:59176/user/HeartbeatReceiver
2015-03-01 19:59:41,146 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59178
2015-03-01 19:59:41,148 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 19:59:41,149 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:59178 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 59178)
2015-03-01 19:59:41,153 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 19:59:41,701 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 19:59:41,705 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 19:59:42,000 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 19:59:42,001 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 19:59:42,003 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59178 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 19:59:42,004 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 19:59:42,012 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:57
2015-03-01 19:59:42,371 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 19:59:42,395 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 19:59:42,419 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 19:59:42,420 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 19:59:42,421 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 19:59:42,430 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 19:59:42,441 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67), which has no missing parents
2015-03-01 19:59:42,487 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 19:59:42,487 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 19:59:42,495 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1863) called with curMem=155722, maxMem=1030823608
2015-03-01 19:59:42,496 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1863.0 B, free 982.9 MB)
2015-03-01 19:59:42,497 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59178 (size: 1863.0 B, free: 983.1 MB)
2015-03-01 19:59:42,497 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 19:59:42,499 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 19:59:42,511 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67)
2015-03-01 19:59:42,513 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 19:59:42,552 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 19:59:42,562 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 19:59:42,599 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 19:59:42,614 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 19:59:42,614 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 19:59:42,615 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 19:59:42,615 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 19:59:42,615 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 19:59:42,849 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-03-01 19:59:42,866 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 315 ms on localhost (1/1)
2015-03-01 19:59:42,867 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 19:59:42,868 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.338 s
2015-03-01 19:59:42,876 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.480202 s
2015-03-01 19:59:42,909 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 19:59:42,910 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 19:59:42,910 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 19:59:42,910 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 19:59:42,936 [sparkDriver-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 19:59:42,942 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Starting remoting
2015-03-01 19:59:42,956 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@localhost:59179]
2015-03-01 19:59:42,957 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 59179.
2015-03-01 19:59:42,958 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 19:59:42,959 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 19:59:42,961 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301195942-dcb0
2015-03-01 19:59:42,961 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 19:59:42,963 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-68ed7b55-087d-4a90-be9b-df157b2444cf
2015-03-01 19:59:42,963 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 19:59:42,964 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 19:59:42,966 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:59180
2015-03-01 19:59:42,967 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 59180.
2015-03-01 19:59:42,973 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 19:59:42,978 [ScalaTest-main-running-StatSpec] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at ml.ProfileTrimmer.setup(ProfileTrimmer.scala:49)
	at ml.ProfileTrimmer.<init>(ProfileTrimmer.scala:27)
	at test_ml.StatSpec$$anonfun$2.apply$mcV$sp(StatSpec.scala:19)
	at test_ml.StatSpec$$anonfun$2.apply(StatSpec.scala:18)
	at test_ml.StatSpec$$anonfun$2.apply(StatSpec.scala:18)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1639)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1121)
	at org.scalatest.FlatSpec.withFixture(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$class.invokeWithFixture$1(FlatSpecLike.scala:1636)
	at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1648)
	at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1648)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FlatSpecLike$class.runTest(FlatSpecLike.scala:1648)
	at org.scalatest.FlatSpec.runTest(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1706)
	at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1706)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:390)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:427)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FlatSpecLike$class.runTests(FlatSpecLike.scala:1706)
	at org.scalatest.FlatSpec.runTests(FlatSpec.scala:1683)
	at org.scalatest.Suite$class.run(Suite.scala:1423)
	at org.scalatest.FlatSpec.org$scalatest$FlatSpecLike$$super$run(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1752)
	at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1752)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FlatSpecLike$class.run(FlatSpecLike.scala:1752)
	at org.scalatest.FlatSpec.run(FlatSpec.scala:1683)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at scala.tools.eclipse.scalatest.launching.ScalaTestLauncher$.main(ScalaTestLauncher.scala:58)
	at scala.tools.eclipse.scalatest.launching.ScalaTestLauncher.main(ScalaTestLauncher.scala)
2015-03-01 19:59:42,981 [ScalaTest-main-running-StatSpec] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED org.eclipse.jetty.server.Server@3d05209d: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:194)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:204)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1676)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1667)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:204)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:269)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:269)
	at ml.ProfileTrimmer.setup(ProfileTrimmer.scala:49)
	at ml.ProfileTrimmer.<init>(ProfileTrimmer.scala:27)
	at test_ml.StatSpec$$anonfun$2.apply$mcV$sp(StatSpec.scala:19)
	at test_ml.StatSpec$$anonfun$2.apply(StatSpec.scala:18)
	at test_ml.StatSpec$$anonfun$2.apply(StatSpec.scala:18)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1639)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1121)
	at org.scalatest.FlatSpec.withFixture(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$class.invokeWithFixture$1(FlatSpecLike.scala:1636)
	at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1648)
	at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1648)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FlatSpecLike$class.runTest(FlatSpecLike.scala:1648)
	at org.scalatest.FlatSpec.runTest(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1706)
	at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1706)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:390)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:427)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FlatSpecLike$class.runTests(FlatSpecLike.scala:1706)
	at org.scalatest.FlatSpec.runTests(FlatSpec.scala:1683)
	at org.scalatest.Suite$class.run(Suite.scala:1423)
	at org.scalatest.FlatSpec.org$scalatest$FlatSpecLike$$super$run(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1752)
	at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1752)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FlatSpecLike$class.run(FlatSpecLike.scala:1752)
	at org.scalatest.FlatSpec.run(FlatSpec.scala:1683)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.main(Runner.scala:860)
	at org.scalatest.tools.Runner.main(Runner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at scala.tools.eclipse.scalatest.launching.ScalaTestLauncher$.main(ScalaTestLauncher.scala:58)
	at scala.tools.eclipse.scalatest.launching.ScalaTestLauncher.main(ScalaTestLauncher.scala)
2015-03-01 19:59:42,986 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2015-03-01 19:59:42,986 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2015-03-01 19:59:42,986 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2015-03-01 19:59:42,986 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
2015-03-01 19:59:42,986 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
2015-03-01 19:59:42,986 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2015-03-01 19:59:42,987 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2015-03-01 19:59:42,987 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2015-03-01 19:59:42,987 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2015-03-01 19:59:42,987 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2015-03-01 19:59:42,987 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2015-03-01 19:59:42,987 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2015-03-01 19:59:42,987 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2015-03-01 19:59:42,988 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2015-03-01 19:59:42,988 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2015-03-01 19:59:42,988 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2015-03-01 19:59:42,988 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2015-03-01 19:59:42,988 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2015-03-01 19:59:42,988 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2015-03-01 19:59:42,988 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
2015-03-01 19:59:42,989 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
2015-03-01 19:59:42,989 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
2015-03-01 19:59:42,989 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs,null}
2015-03-01 19:59:43,043 [ScalaTest-main-running-StatSpec] WARN  org.apache.spark.util.Utils - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2015-03-01 19:59:43,044 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 19:59:43,053 [ScalaTest-main-running-StatSpec] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4041
2015-03-01 19:59:43,054 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4041.
2015-03-01 19:59:43,054 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://localhost:4041
2015-03-01 19:59:43,094 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@localhost:59179/user/HeartbeatReceiver
2015-03-01 19:59:43,098 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59181
2015-03-01 19:59:43,098 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 19:59:43,099 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:59181 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 59181)
2015-03-01 19:59:43,099 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 20:00:14,034 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 20:00:14,066 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 20:00:14,067 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 20:00:14,068 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 20:00:14,461 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 20:00:14,540 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-03-01 20:00:14,762 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:59184]
2015-03-01 20:00:14,770 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 59184.
2015-03-01 20:00:14,797 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 20:00:14,827 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 20:00:14,846 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301200014-60ba
2015-03-01 20:00:14,852 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 20:00:15,199 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 20:00:15,334 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-b4d79a16-fdae-4710-bb36-de854723e355
2015-03-01 20:00:15,348 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 20:00:15,507 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 20:00:15,527 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:59185
2015-03-01 20:00:15,527 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 59185.
2015-03-01 20:00:15,668 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 20:00:15,679 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 20:00:15,680 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 20:00:15,682 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 20:00:15,812 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:59184/user/HeartbeatReceiver
2015-03-01 20:00:15,983 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59186
2015-03-01 20:00:15,985 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 20:00:15,986 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:59186 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 59186)
2015-03-01 20:00:15,989 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 20:00:16,494 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 20:00:16,496 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 20:00:16,688 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 20:00:16,689 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 20:00:16,691 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59186 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 20:00:16,692 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 20:00:16,696 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:57
2015-03-01 20:00:16,947 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 20:00:16,963 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 20:00:16,976 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 20:00:16,977 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 20:00:16,977 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 20:00:16,985 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 20:00:16,991 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67), which has no missing parents
2015-03-01 20:00:17,034 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 20:00:17,035 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 20:00:17,042 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 20:00:17,042 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 20:00:17,043 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59186 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 20:00:17,044 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 20:00:17,045 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 20:00:17,054 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67)
2015-03-01 20:00:17,055 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 20:00:17,082 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 20:00:17,089 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 20:00:17,113 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 20:00:17,119 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 20:00:17,119 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 20:00:17,120 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 20:00:17,120 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 20:00:17,120 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 20:00:17,333 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-03-01 20:00:17,352 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 265 ms on localhost (1/1)
2015-03-01 20:00:17,353 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 20:00:17,353 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.288 s
2015-03-01 20:00:17,359 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.395464 s
2015-03-01 20:00:17,458 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: foreach at StatSpec.scala:26
2015-03-01 20:00:17,463 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (groupBy at ProfileTrimmer.scala:91)
2015-03-01 20:00:17,464 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreach at StatSpec.scala:26) with 1 output partitions (allowLocal=false)
2015-03-01 20:00:17,464 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 2(foreach at StatSpec.scala:26)
2015-03-01 20:00:17,465 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 1)
2015-03-01 20:00:17,466 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 1)
2015-03-01 20:00:17,469 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[7] at groupBy at ProfileTrimmer.scala:91), which has no missing parents
2015-03-01 20:00:17,483 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2680) called with curMem=157584, maxMem=1030823608
2015-03-01 20:00:17,483 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.6 KB, free 982.9 MB)
2015-03-01 20:00:17,492 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1573) called with curMem=160264, maxMem=1030823608
2015-03-01 20:00:17,493 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1573.0 B, free 982.9 MB)
2015-03-01 20:00:17,494 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:59186 (size: 1573.0 B, free: 983.1 MB)
2015-03-01 20:00:17,494 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-03-01 20:00:17,495 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-03-01 20:00:17,512 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[7] at groupBy at ProfileTrimmer.scala:91)
2015-03-01 20:00:17,512 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-03-01 20:00:17,519 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1171 bytes)
2015-03-01 20:00:17,520 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-03-01 20:00:17,629 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 813 bytes result sent to driver
2015-03-01 20:00:17,648 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 118 ms on localhost (1/1)
2015-03-01 20:00:17,649 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-03-01 20:00:17,649 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (groupBy at ProfileTrimmer.scala:91) finished in 0.137 s
2015-03-01 20:00:17,650 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-03-01 20:00:17,650 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-03-01 20:00:17,651 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 2)
2015-03-01 20:00:17,651 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-03-01 20:00:17,655 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 2: List()
2015-03-01 20:00:17,659 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[9] at map at ProfileTrimmer.scala:92), which is now runnable
2015-03-01 20:00:17,663 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=161837, maxMem=1030823608
2015-03-01 20:00:17,664 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-03-01 20:00:17,676 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1526) called with curMem=164373, maxMem=1030823608
2015-03-01 20:00:17,677 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1526.0 B, free 982.9 MB)
2015-03-01 20:00:17,678 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:59186 (size: 1526.0 B, free: 983.1 MB)
2015-03-01 20:00:17,678 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-03-01 20:00:17,680 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-03-01 20:00:17,682 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 2 (MappedRDD[9] at map at ProfileTrimmer.scala:92)
2015-03-01 20:00:17,682 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2015-03-01 20:00:17,684 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1056 bytes)
2015-03-01 20:00:17,685 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2015-03-01 20:00:17,709 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-03-01 20:00:17,712 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 9 ms
2015-03-01 20:00:17,778 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 782 bytes result sent to driver
2015-03-01 20:00:17,787 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 97 ms on localhost (1/1)
2015-03-01 20:00:17,788 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-03-01 20:00:17,788 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (foreach at StatSpec.scala:26) finished in 0.106 s
2015-03-01 20:00:17,789 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: foreach at StatSpec.scala:26, took 0.330290 s
2015-03-01 20:06:29,093 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 20:06:29,401 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 20:06:29,402 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 20:06:29,402 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 20:06:29,809 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 20:06:29,894 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-03-01 20:06:30,154 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:59233]
2015-03-01 20:06:30,162 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 59233.
2015-03-01 20:06:30,203 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 20:06:30,255 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 20:06:30,275 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301200630-f6aa
2015-03-01 20:06:30,281 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 20:06:30,626 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 20:06:30,761 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-a07722f5-d602-4d98-8966-2cd7837ce4d5
2015-03-01 20:06:30,774 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 20:06:30,941 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 20:06:30,963 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:59234
2015-03-01 20:06:30,963 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 59234.
2015-03-01 20:06:31,118 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 20:06:31,133 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 20:06:31,133 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 20:06:31,136 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 20:06:31,277 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:59233/user/HeartbeatReceiver
2015-03-01 20:06:31,484 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 59235
2015-03-01 20:06:31,486 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 20:06:31,488 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:59235 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 59235)
2015-03-01 20:06:31,492 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 20:06:32,112 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 20:06:32,114 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 20:06:32,307 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 20:06:32,307 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 20:06:32,309 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:59235 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 20:06:32,310 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 20:06:32,315 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:57
2015-03-01 20:06:32,615 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 20:06:32,632 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 20:06:32,646 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 20:06:32,646 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 20:06:32,646 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 20:06:32,653 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 20:06:32,659 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67), which has no missing parents
2015-03-01 20:06:32,700 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 20:06:32,701 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 20:06:32,709 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 20:06:32,709 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 20:06:32,710 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:59235 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 20:06:32,711 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 20:06:32,712 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 20:06:32,721 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67)
2015-03-01 20:06:32,722 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 20:06:32,750 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 20:06:32,756 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 20:06:32,785 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 20:06:32,796 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 20:06:32,797 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 20:06:32,797 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 20:06:32,797 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 20:06:32,797 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 20:06:33,032 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-03-01 20:06:33,050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 298 ms on localhost (1/1)
2015-03-01 20:06:33,051 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 20:06:33,053 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.318 s
2015-03-01 20:06:33,063 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.430456 s
2015-03-01 20:06:33,158 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: collect at StatSpec.scala:25
2015-03-01 20:06:33,163 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (groupBy at ProfileTrimmer.scala:91)
2015-03-01 20:06:33,164 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at StatSpec.scala:25) with 1 output partitions (allowLocal=false)
2015-03-01 20:06:33,164 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 2(collect at StatSpec.scala:25)
2015-03-01 20:06:33,164 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 1)
2015-03-01 20:06:33,166 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 1)
2015-03-01 20:06:33,169 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[7] at groupBy at ProfileTrimmer.scala:91), which has no missing parents
2015-03-01 20:06:33,178 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2680) called with curMem=157584, maxMem=1030823608
2015-03-01 20:06:33,178 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.6 KB, free 982.9 MB)
2015-03-01 20:06:33,190 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1573) called with curMem=160264, maxMem=1030823608
2015-03-01 20:06:33,191 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1573.0 B, free 982.9 MB)
2015-03-01 20:06:33,195 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:59235 (size: 1573.0 B, free: 983.1 MB)
2015-03-01 20:06:33,196 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-03-01 20:06:33,198 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-03-01 20:06:33,218 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[7] at groupBy at ProfileTrimmer.scala:91)
2015-03-01 20:06:33,218 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-03-01 20:06:33,229 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1171 bytes)
2015-03-01 20:06:33,229 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-03-01 20:06:33,326 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 813 bytes result sent to driver
2015-03-01 20:06:33,344 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 108 ms on localhost (1/1)
2015-03-01 20:06:33,344 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-03-01 20:06:33,345 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (groupBy at ProfileTrimmer.scala:91) finished in 0.125 s
2015-03-01 20:06:33,345 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-03-01 20:06:33,346 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-03-01 20:06:33,346 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 2)
2015-03-01 20:06:33,347 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-03-01 20:06:33,352 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 2: List()
2015-03-01 20:06:33,355 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[9] at map at ProfileTrimmer.scala:92), which is now runnable
2015-03-01 20:06:33,357 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2464) called with curMem=161837, maxMem=1030823608
2015-03-01 20:06:33,358 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 2.4 KB, free 982.9 MB)
2015-03-01 20:06:33,366 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1471) called with curMem=164301, maxMem=1030823608
2015-03-01 20:06:33,366 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1471.0 B, free 982.9 MB)
2015-03-01 20:06:33,367 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:59235 (size: 1471.0 B, free: 983.1 MB)
2015-03-01 20:06:33,367 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-03-01 20:06:33,369 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-03-01 20:06:33,371 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 2 (MappedRDD[9] at map at ProfileTrimmer.scala:92)
2015-03-01 20:06:33,371 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2015-03-01 20:06:33,373 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1056 bytes)
2015-03-01 20:06:33,373 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2015-03-01 20:06:33,395 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-03-01 20:06:33,397 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
2015-03-01 20:06:33,466 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 841 bytes result sent to driver
2015-03-01 20:06:33,474 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 96 ms on localhost (1/1)
2015-03-01 20:06:33,475 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-03-01 20:06:33,475 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (collect at StatSpec.scala:25) finished in 0.103 s
2015-03-01 20:06:33,476 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at StatSpec.scala:25, took 0.317142 s
2015-03-01 21:42:20,889 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ProifleTrimmer
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-03-01 21:42:21,307 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-03-01 21:42:21,308 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-03-01 21:42:21,310 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-03-01 21:42:21,842 [sparkDriver-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-03-01 21:42:21,943 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Starting remoting
2015-03-01 21:42:22,294 [sparkDriver-akka.actor.default-dispatcher-2] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.103:60899]
2015-03-01 21:42:22,306 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 60899.
2015-03-01 21:42:22,355 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-03-01 21:42:22,396 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-03-01 21:42:22,434 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150301214222-5aee
2015-03-01 21:42:22,444 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-03-01 21:42:23,216 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-03-01 21:42:23,372 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-1adac886-2206-496c-967e-b65c8c982a06
2015-03-01 21:42:23,388 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-03-01 21:42:23,608 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 21:42:23,636 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:60900
2015-03-01 21:42:23,636 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 60900.
2015-03-01 21:42:23,858 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-03-01 21:42:23,874 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-03-01 21:42:23,874 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-03-01 21:42:23,877 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.103:4040
2015-03-01 21:42:24,045 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.103:60899/user/HeartbeatReceiver
2015-03-01 21:42:24,290 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 60901
2015-03-01 21:42:24,293 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-03-01 21:42:24,295 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:60901 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 60901)
2015-03-01 21:42:24,299 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-03-01 21:42:25,110 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-03-01 21:42:25,113 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-03-01 21:42:25,392 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-03-01 21:42:25,392 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-03-01 21:42:25,395 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:60901 (size: 13.6 KB, free: 983.1 MB)
2015-03-01 21:42:25,395 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-03-01 21:42:25,403 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at ProfileTrimmer.scala:57
2015-03-01 21:42:25,768 [ScalaTest-main-running-StatSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-03-01 21:42:25,789 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: count at StatSpec.scala:14
2015-03-01 21:42:25,813 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at StatSpec.scala:14) with 1 output partitions (allowLocal=false)
2015-03-01 21:42:25,814 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at StatSpec.scala:14)
2015-03-01 21:42:25,815 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-03-01 21:42:25,825 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-03-01 21:42:25,838 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67), which has no missing parents
2015-03-01 21:42:25,889 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3160) called with curMem=152562, maxMem=1030823608
2015-03-01 21:42:25,890 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 982.9 MB)
2015-03-01 21:42:25,899 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1862) called with curMem=155722, maxMem=1030823608
2015-03-01 21:42:25,900 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1862.0 B, free 982.9 MB)
2015-03-01 21:42:25,901 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:60901 (size: 1862.0 B, free: 983.1 MB)
2015-03-01 21:42:25,902 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-03-01 21:42:25,904 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-03-01 21:42:25,917 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[4] at map at ProfileTrimmer.scala:67)
2015-03-01 21:42:25,919 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-03-01 21:42:25,960 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-03-01 21:42:25,970 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-03-01 21:42:26,000 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-03-01 21:42:26,013 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-03-01 21:42:26,014 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-03-01 21:42:26,014 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-03-01 21:42:26,014 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-03-01 21:42:26,014 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-03-01 21:42:26,262 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1678 bytes result sent to driver
2015-03-01 21:42:26,280 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 321 ms on localhost (1/1)
2015-03-01 21:42:26,281 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at StatSpec.scala:14) finished in 0.348 s
2015-03-01 21:42:26,281 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-03-01 21:42:26,291 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at StatSpec.scala:14, took 0.501609 s
2015-03-01 21:42:26,395 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.SparkContext - Starting job: collect at StatSpec.scala:25
2015-03-01 21:42:26,401 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (groupBy at ProfileTrimmer.scala:91)
2015-03-01 21:42:26,402 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at StatSpec.scala:25) with 1 output partitions (allowLocal=false)
2015-03-01 21:42:26,403 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 2(collect at StatSpec.scala:25)
2015-03-01 21:42:26,403 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 1)
2015-03-01 21:42:26,404 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 1)
2015-03-01 21:42:26,407 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[7] at groupBy at ProfileTrimmer.scala:91), which has no missing parents
2015-03-01 21:42:26,415 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2680) called with curMem=157584, maxMem=1030823608
2015-03-01 21:42:26,415 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.6 KB, free 982.9 MB)
2015-03-01 21:42:26,422 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1573) called with curMem=160264, maxMem=1030823608
2015-03-01 21:42:26,422 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1573.0 B, free 982.9 MB)
2015-03-01 21:42:26,423 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:60901 (size: 1573.0 B, free: 983.1 MB)
2015-03-01 21:42:26,424 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-03-01 21:42:26,425 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-03-01 21:42:26,442 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[7] at groupBy at ProfileTrimmer.scala:91)
2015-03-01 21:42:26,442 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-03-01 21:42:26,451 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1171 bytes)
2015-03-01 21:42:26,451 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-03-01 21:42:26,563 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 813 bytes result sent to driver
2015-03-01 21:42:26,578 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 120 ms on localhost (1/1)
2015-03-01 21:42:26,578 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-03-01 21:42:26,579 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (groupBy at ProfileTrimmer.scala:91) finished in 0.136 s
2015-03-01 21:42:26,579 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-03-01 21:42:26,580 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-03-01 21:42:26,580 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 2)
2015-03-01 21:42:26,580 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-03-01 21:42:26,585 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 2: List()
2015-03-01 21:42:26,590 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[9] at map at ProfileTrimmer.scala:92), which is now runnable
2015-03-01 21:42:26,593 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2464) called with curMem=161837, maxMem=1030823608
2015-03-01 21:42:26,593 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 2.4 KB, free 982.9 MB)
2015-03-01 21:42:26,600 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1471) called with curMem=164301, maxMem=1030823608
2015-03-01 21:42:26,600 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1471.0 B, free 982.9 MB)
2015-03-01 21:42:26,601 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:60901 (size: 1471.0 B, free: 983.1 MB)
2015-03-01 21:42:26,601 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-03-01 21:42:26,602 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-03-01 21:42:26,604 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 2 (MappedRDD[9] at map at ProfileTrimmer.scala:92)
2015-03-01 21:42:26,604 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2015-03-01 21:42:26,606 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1056 bytes)
2015-03-01 21:42:26,606 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2015-03-01 21:42:26,629 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-03-01 21:42:26,631 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
2015-03-01 21:42:26,695 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 841 bytes result sent to driver
2015-03-01 21:42:26,706 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 92 ms on localhost (1/1)
2015-03-01 21:42:26,706 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-03-01 21:42:26,706 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (collect at StatSpec.scala:25) finished in 0.102 s
2015-03-01 21:42:26,707 [ScalaTest-main-running-StatSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at StatSpec.scala:25, took 0.311769 s
2015-04-04 19:32:35,608 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-04 19:32:36,042 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-04 19:32:36,044 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-04 19:32:36,045 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-04 19:32:36,520 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-04 19:32:36,610 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-04-04 19:32:36,945 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:50065]
2015-04-04 19:32:36,958 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50065.
2015-04-04 19:32:36,995 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-04 19:32:37,037 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-04 19:32:37,065 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150404193237-6152
2015-04-04 19:32:37,073 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-04 19:32:37,521 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-04 19:32:37,657 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-7da129ea-f16f-4976-b36c-1f365de39bdd
2015-04-04 19:32:37,671 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-04 19:32:37,878 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-04 19:32:37,902 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:50066
2015-04-04 19:32:37,902 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 50066.
2015-04-04 19:32:39,369 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-04 19:32:39,394 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-04 19:32:39,395 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-04 19:32:39,398 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-04 19:32:39,537 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:50065/user/HeartbeatReceiver
2015-04-04 19:32:39,745 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50067
2015-04-04 19:32:39,747 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-04 19:32:39,749 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:50067 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 50067)
2015-04-04 19:32:39,752 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-04 19:32:40,377 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-04 19:32:40,380 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-04 19:32:40,611 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-04 19:32:40,612 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-04 19:32:40,615 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50067 (size: 13.6 KB, free: 983.1 MB)
2015-04-04 19:32:40,617 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-04 19:32:40,624 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-04 19:32:40,991 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-04 19:32:41,015 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemRecSpec.scala:12
2015-04-04 19:32:41,041 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemRecSpec.scala:12) with 1 output partitions (allowLocal=false)
2015-04-04 19:32:41,042 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at ItemRecSpec.scala:12)
2015-04-04 19:32:41,043 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-04 19:32:41,052 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-04 19:32:41,063 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[6] at keyBy at ItemRecSpec.scala:12), which has no missing parents
2015-04-04 19:32:41,096 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3552) called with curMem=152562, maxMem=1030823608
2015-04-04 19:32:41,097 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 982.9 MB)
2015-04-04 19:32:41,106 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2020) called with curMem=156114, maxMem=1030823608
2015-04-04 19:32:41,107 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2020.0 B, free 982.9 MB)
2015-04-04 19:32:41,108 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50067 (size: 2020.0 B, free: 983.1 MB)
2015-04-04 19:32:41,109 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-04 19:32:41,110 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-04 19:32:41,122 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[6] at keyBy at ItemRecSpec.scala:12)
2015-04-04 19:32:41,123 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-04-04 19:32:41,158 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1357 bytes)
2015-04-04 19:32:41,166 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-04 19:32:41,196 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-04 19:32:41,209 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-04 19:32:41,209 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-04 19:32:41,209 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-04 19:32:41,209 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-04 19:32:41,210 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-04 19:32:41,651 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1679 bytes result sent to driver
2015-04-04 19:32:41,666 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 508 ms on localhost (1/1)
2015-04-04 19:32:41,667 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at ItemRecSpec.scala:12) finished in 0.530 s
2015-04-04 19:32:41,667 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-04 19:32:41,676 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemRecSpec.scala:12, took 0.660708 s
2015-04-04 19:42:25,822 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-04 19:42:26,138 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-04 19:42:26,139 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-04 19:42:26,139 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-04 19:42:26,474 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-04 19:42:26,543 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-04-04 19:42:26,755 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:50306]
2015-04-04 19:42:26,762 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 50306.
2015-04-04 19:42:26,797 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-04 19:42:26,814 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-04 19:42:26,833 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150404194226-4a3e
2015-04-04 19:42:26,839 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-04 19:42:27,151 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-04 19:42:27,277 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-3f63287e-9b38-44cc-b88d-f517257aa944
2015-04-04 19:42:27,288 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-04 19:42:27,445 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-04 19:42:27,463 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:50307
2015-04-04 19:42:27,463 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 50307.
2015-04-04 19:42:27,594 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-04 19:42:27,610 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-04 19:42:27,610 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-04 19:42:27,613 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-04 19:42:27,738 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:50306/user/HeartbeatReceiver
2015-04-04 19:42:27,910 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 50308
2015-04-04 19:42:27,912 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-04 19:42:27,913 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:50308 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 50308)
2015-04-04 19:42:27,916 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-04 19:42:28,372 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-04 19:42:28,375 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-04 19:42:28,552 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-04 19:42:28,553 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-04 19:42:28,555 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:50308 (size: 13.6 KB, free: 983.1 MB)
2015-04-04 19:42:28,556 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-04 19:42:28,561 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-04 19:42:28,786 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-04 19:42:28,838 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemRecSpec.scala:12
2015-04-04 19:42:28,861 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (distinct at ItemRecSpec.scala:12)
2015-04-04 19:42:28,863 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemRecSpec.scala:12) with 1 output partitions (allowLocal=false)
2015-04-04 19:42:28,864 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 1(count at ItemRecSpec.scala:12)
2015-04-04 19:42:28,864 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 0)
2015-04-04 19:42:28,869 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 0)
2015-04-04 19:42:28,880 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[7] at distinct at ItemRecSpec.scala:12), which has no missing parents
2015-04-04 19:42:28,921 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4440) called with curMem=152562, maxMem=1030823608
2015-04-04 19:42:28,922 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 982.9 MB)
2015-04-04 19:42:28,930 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2607) called with curMem=157002, maxMem=1030823608
2015-04-04 19:42:28,931 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-04 19:42:28,931 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:50308 (size: 2.5 KB, free: 983.1 MB)
2015-04-04 19:42:28,932 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-04 19:42:28,933 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-04 19:42:28,941 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[7] at distinct at ItemRecSpec.scala:12)
2015-04-04 19:42:28,942 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-04-04 19:42:28,969 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1346 bytes)
2015-04-04 19:42:28,976 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-04 19:42:29,002 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-04 19:42:29,008 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-04 19:42:29,009 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-04 19:42:29,009 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-04 19:42:29,009 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-04 19:42:29,009 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-04 19:42:29,875 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1871 bytes result sent to driver
2015-04-04 19:42:29,894 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 918 ms on localhost (1/1)
2015-04-04 19:42:29,896 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-04 19:42:29,897 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (distinct at ItemRecSpec.scala:12) finished in 0.945 s
2015-04-04 19:42:29,897 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-04 19:42:29,898 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-04 19:42:29,898 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 1)
2015-04-04 19:42:29,899 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-04 19:42:29,902 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 1: List()
2015-04-04 19:42:29,905 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[9] at distinct at ItemRecSpec.scala:12), which is now runnable
2015-04-04 19:42:29,909 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=159609, maxMem=1030823608
2015-04-04 19:42:29,910 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-04 19:42:29,917 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1545) called with curMem=162145, maxMem=1030823608
2015-04-04 19:42:29,918 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1545.0 B, free 982.9 MB)
2015-04-04 19:42:29,919 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:50308 (size: 1545.0 B, free: 983.1 MB)
2015-04-04 19:42:29,919 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-04 19:42:29,920 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-04-04 19:42:29,922 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[9] at distinct at ItemRecSpec.scala:12)
2015-04-04 19:42:29,922 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-04-04 19:42:29,924 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-04 19:42:29,924 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-04-04 19:42:29,948 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-04 19:42:29,949 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2015-04-04 19:42:30,023 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 783 bytes result sent to driver
2015-04-04 19:42:30,029 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on localhost (1/1)
2015-04-04 19:42:30,029 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (count at ItemRecSpec.scala:12) finished in 0.106 s
2015-04-04 19:42:30,029 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-04-04 19:42:30,035 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemRecSpec.scala:12, took 1.197328 s
2015-04-04 19:42:30,070 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2015-04-04 19:42:30,070 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2015-04-04 19:42:30,070 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2015-04-04 19:42:30,071 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
2015-04-04 19:42:30,071 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
2015-04-04 19:42:30,071 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2015-04-04 19:42:30,071 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2015-04-04 19:42:30,071 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2015-04-04 19:42:30,072 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2015-04-04 19:42:30,072 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2015-04-04 19:42:30,072 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2015-04-04 19:42:30,072 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2015-04-04 19:42:30,072 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2015-04-04 19:42:30,073 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2015-04-04 19:42:30,073 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2015-04-04 19:42:30,073 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2015-04-04 19:42:30,073 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2015-04-04 19:42:30,073 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2015-04-04 19:42:30,074 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2015-04-04 19:42:30,074 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
2015-04-04 19:42:30,074 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
2015-04-04 19:42:30,074 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
2015-04-04 19:42:30,074 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs,null}
2015-04-04 19:42:30,128 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.0.104:4040
2015-04-04 19:42:30,130 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Stopping DAGScheduler
2015-04-04 19:42:31,189 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.MapOutputTrackerMasterActor - MapOutputTrackerActor stopped!
2015-04-04 19:42:31,198 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore cleared
2015-04-04 19:42:31,199 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
2015-04-04 19:42:31,200 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2015-04-04 19:42:31,201 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2015-04-04 19:42:31,218 [sparkDriver-akka.actor.default-dispatcher-16] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
2015-04-04 19:42:31,221 [sparkDriver-akka.actor.default-dispatcher-16] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
2015-04-05 10:36:25,197 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-05 10:36:25,530 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-05 10:36:25,532 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-05 10:36:25,533 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-05 10:36:25,949 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-05 10:36:26,024 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-04-05 10:36:26,296 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:52927]
2015-04-05 10:36:26,306 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 52927.
2015-04-05 10:36:26,352 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-05 10:36:26,389 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-05 10:36:26,416 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150405103626-8f60
2015-04-05 10:36:26,424 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-05 10:36:26,817 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-05 10:36:26,946 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-41b37b3e-5784-4ed2-8169-3bc025e0484c
2015-04-05 10:36:26,959 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-05 10:36:27,132 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 10:36:27,153 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:52928
2015-04-05 10:36:27,153 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 52928.
2015-04-05 10:36:27,308 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 10:36:27,320 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-05 10:36:27,320 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-05 10:36:27,323 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-05 10:36:27,456 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:52927/user/HeartbeatReceiver
2015-04-05 10:36:27,652 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 52929
2015-04-05 10:36:27,655 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-05 10:36:27,656 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:52929 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 52929)
2015-04-05 10:36:27,659 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-05 10:36:28,179 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-05 10:36:28,180 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-05 10:36:28,370 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-05 10:36:28,371 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-05 10:36:28,374 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:52929 (size: 13.6 KB, free: 983.1 MB)
2015-04-05 10:36:28,374 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-05 10:36:28,379 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-05 10:36:28,642 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-05 10:36:28,656 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemBasedRecommender.scala:129
2015-04-05 10:36:28,676 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemBasedRecommender.scala:129) with 96 output partitions (allowLocal=false)
2015-04-05 10:36:28,677 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at ItemBasedRecommender.scala:129)
2015-04-05 10:36:28,677 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-05 10:36:28,751 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 10:36:28,759 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (FlatMappedRDD[5] at flatMap at DataRepo.scala:60), which has no missing parents
2015-04-05 10:36:28,794 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3368) called with curMem=152562, maxMem=1030823608
2015-04-05 10:36:28,795 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 10:36:28,804 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1928) called with curMem=155930, maxMem=1030823608
2015-04-05 10:36:28,805 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1928.0 B, free 982.9 MB)
2015-04-05 10:36:28,806 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:52929 (size: 1928.0 B, free: 983.1 MB)
2015-04-05 10:36:28,806 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 10:36:28,807 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-05 10:36:28,844 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 96 missing tasks from Stage 0 (FlatMappedRDD[5] at flatMap at DataRepo.scala:60)
2015-04-05 10:36:28,847 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 96 tasks
2015-04-05 10:36:28,871 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:28,879 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-05 10:36:28,906 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:0+33554432
2015-04-05 10:36:28,913 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-05 10:36:28,913 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-05 10:36:28,914 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-05 10:36:28,914 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-05 10:36:28,914 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-05 10:36:31,346 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1680 bytes result sent to driver
2015-04-05 10:36:31,348 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:31,349 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2015-04-05 10:36:31,355 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:33554432+33554432
2015-04-05 10:36:31,365 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2488 ms on localhost (1/96)
2015-04-05 10:36:33,423 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1680 bytes result sent to driver
2015-04-05 10:36:33,425 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:33,425 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
2015-04-05 10:36:33,432 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:67108864+33554432
2015-04-05 10:36:33,433 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 2079 ms on localhost (2/96)
2015-04-05 10:36:35,521 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1680 bytes result sent to driver
2015-04-05 10:36:35,523 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:35,524 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
2015-04-05 10:36:35,530 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:100663296+33554432
2015-04-05 10:36:35,531 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 2101 ms on localhost (3/96)
2015-04-05 10:36:37,485 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1680 bytes result sent to driver
2015-04-05 10:36:37,487 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:37,487 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
2015-04-05 10:36:37,491 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:134217728+33554432
2015-04-05 10:36:37,492 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1965 ms on localhost (4/96)
2015-04-05 10:36:39,402 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1843 bytes result sent to driver
2015-04-05 10:36:39,404 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 0.0 (TID 5, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:39,404 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 0.0 (TID 5)
2015-04-05 10:36:39,410 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:167772160+33554432
2015-04-05 10:36:39,412 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 1919 ms on localhost (5/96)
2015-04-05 10:36:41,345 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 0.0 (TID 5). 1680 bytes result sent to driver
2015-04-05 10:36:41,346 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 0.0 (TID 6, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:41,347 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 0.0 (TID 6)
2015-04-05 10:36:41,354 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:201326592+33554432
2015-04-05 10:36:41,354 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 0.0 (TID 5) in 1945 ms on localhost (6/96)
2015-04-05 10:36:43,529 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 0.0 (TID 6). 1680 bytes result sent to driver
2015-04-05 10:36:43,532 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 0.0 (TID 7, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:43,535 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 0.0 (TID 7)
2015-04-05 10:36:43,541 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 0.0 (TID 6) in 2189 ms on localhost (7/96)
2015-04-05 10:36:43,541 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:234881024+33554432
2015-04-05 10:36:45,770 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 0.0 (TID 7). 1680 bytes result sent to driver
2015-04-05 10:36:45,772 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 0.0 (TID 8, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:45,773 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 0.0 (TID 8)
2015-04-05 10:36:45,777 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:268435456+33554432
2015-04-05 10:36:45,779 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 0.0 (TID 7) in 2244 ms on localhost (8/96)
2015-04-05 10:36:48,018 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 0.0 (TID 8). 1843 bytes result sent to driver
2015-04-05 10:36:48,019 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 0.0 (TID 9, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:48,020 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 0.0 (TID 9)
2015-04-05 10:36:48,025 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:301989888+33554432
2015-04-05 10:36:48,025 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 0.0 (TID 8) in 2249 ms on localhost (9/96)
2015-04-05 10:36:49,728 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 0.0 (TID 9). 1680 bytes result sent to driver
2015-04-05 10:36:49,730 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 0.0 (TID 10, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:49,730 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 10.0 in stage 0.0 (TID 10)
2015-04-05 10:36:49,735 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:335544320+33554432
2015-04-05 10:36:49,736 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 0.0 (TID 9) in 1711 ms on localhost (10/96)
2015-04-05 10:36:51,416 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 10.0 in stage 0.0 (TID 10). 1680 bytes result sent to driver
2015-04-05 10:36:51,418 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 0.0 (TID 11, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:51,418 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 11.0 in stage 0.0 (TID 11)
2015-04-05 10:36:51,423 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:369098752+33554432
2015-04-05 10:36:51,426 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 0.0 (TID 10) in 1689 ms on localhost (11/96)
2015-04-05 10:36:53,125 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 11.0 in stage 0.0 (TID 11). 1680 bytes result sent to driver
2015-04-05 10:36:53,127 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 0.0 (TID 12, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:53,127 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 12.0 in stage 0.0 (TID 12)
2015-04-05 10:36:53,132 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:402653184+33554432
2015-04-05 10:36:53,134 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 0.0 (TID 11) in 1711 ms on localhost (12/96)
2015-04-05 10:36:54,962 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 12.0 in stage 0.0 (TID 12). 1680 bytes result sent to driver
2015-04-05 10:36:54,964 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 0.0 (TID 13, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:54,964 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 13.0 in stage 0.0 (TID 13)
2015-04-05 10:36:54,968 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:436207616+33554432
2015-04-05 10:36:54,968 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 0.0 (TID 12) in 1838 ms on localhost (13/96)
2015-04-05 10:36:56,730 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 13.0 in stage 0.0 (TID 13). 1680 bytes result sent to driver
2015-04-05 10:36:56,731 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 0.0 (TID 14, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:56,732 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 14.0 in stage 0.0 (TID 14)
2015-04-05 10:36:56,735 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:469762048+33554432
2015-04-05 10:36:56,736 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 0.0 (TID 13) in 1770 ms on localhost (14/96)
2015-04-05 10:36:58,394 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 14.0 in stage 0.0 (TID 14). 1843 bytes result sent to driver
2015-04-05 10:36:58,396 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 0.0 (TID 15, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:36:58,396 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 15.0 in stage 0.0 (TID 15)
2015-04-05 10:36:58,401 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 0.0 (TID 14) in 1667 ms on localhost (15/96)
2015-04-05 10:36:58,401 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:503316480+33554432
2015-04-05 10:37:00,122 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 15.0 in stage 0.0 (TID 15). 1680 bytes result sent to driver
2015-04-05 10:37:00,124 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 0.0 (TID 16, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:00,125 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 16.0 in stage 0.0 (TID 16)
2015-04-05 10:37:00,130 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:536870912+33554432
2015-04-05 10:37:00,131 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 0.0 (TID 15) in 1730 ms on localhost (16/96)
2015-04-05 10:37:01,882 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 16.0 in stage 0.0 (TID 16). 1680 bytes result sent to driver
2015-04-05 10:37:01,883 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 0.0 (TID 17, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:01,883 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 17.0 in stage 0.0 (TID 17)
2015-04-05 10:37:01,888 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:570425344+33554432
2015-04-05 10:37:01,889 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 0.0 (TID 16) in 1761 ms on localhost (17/96)
2015-04-05 10:37:03,576 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 17.0 in stage 0.0 (TID 17). 1680 bytes result sent to driver
2015-04-05 10:37:03,577 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 0.0 (TID 18, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:03,578 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 18.0 in stage 0.0 (TID 18)
2015-04-05 10:37:03,581 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:603979776+33554432
2015-04-05 10:37:03,582 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 0.0 (TID 17) in 1696 ms on localhost (18/96)
2015-04-05 10:37:05,303 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 18.0 in stage 0.0 (TID 18). 1680 bytes result sent to driver
2015-04-05 10:37:05,304 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 0.0 (TID 19, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:05,305 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 19.0 in stage 0.0 (TID 19)
2015-04-05 10:37:05,310 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:637534208+33554432
2015-04-05 10:37:05,311 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 0.0 (TID 18) in 1728 ms on localhost (19/96)
2015-04-05 10:37:06,934 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 19.0 in stage 0.0 (TID 19). 1680 bytes result sent to driver
2015-04-05 10:37:06,936 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 0.0 (TID 20, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:06,936 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 20.0 in stage 0.0 (TID 20)
2015-04-05 10:37:06,941 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:671088640+33554432
2015-04-05 10:37:06,942 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 0.0 (TID 19) in 1632 ms on localhost (20/96)
2015-04-05 10:37:08,636 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 20.0 in stage 0.0 (TID 20). 1843 bytes result sent to driver
2015-04-05 10:37:08,637 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 0.0 (TID 21, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:08,637 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 21.0 in stage 0.0 (TID 21)
2015-04-05 10:37:08,641 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:704643072+33554432
2015-04-05 10:37:08,642 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 0.0 (TID 20) in 1703 ms on localhost (21/96)
2015-04-05 10:37:10,327 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 21.0 in stage 0.0 (TID 21). 1680 bytes result sent to driver
2015-04-05 10:37:10,328 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 0.0 (TID 22, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:10,328 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 22.0 in stage 0.0 (TID 22)
2015-04-05 10:37:10,333 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:738197504+33554432
2015-04-05 10:37:10,334 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 0.0 (TID 21) in 1692 ms on localhost (22/96)
2015-04-05 10:37:11,963 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 22.0 in stage 0.0 (TID 22). 1680 bytes result sent to driver
2015-04-05 10:37:11,964 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 0.0 (TID 23, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:11,964 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 23.0 in stage 0.0 (TID 23)
2015-04-05 10:37:11,968 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:771751936+33554432
2015-04-05 10:37:11,968 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 0.0 (TID 22) in 1637 ms on localhost (23/96)
2015-04-05 10:37:13,607 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 23.0 in stage 0.0 (TID 23). 1680 bytes result sent to driver
2015-04-05 10:37:13,609 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 24.0 in stage 0.0 (TID 24, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:13,610 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 24.0 in stage 0.0 (TID 24)
2015-04-05 10:37:13,613 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:805306368+33554432
2015-04-05 10:37:13,614 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 23.0 in stage 0.0 (TID 23) in 1647 ms on localhost (24/96)
2015-04-05 10:37:15,341 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 24.0 in stage 0.0 (TID 24). 1680 bytes result sent to driver
2015-04-05 10:37:15,342 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 25.0 in stage 0.0 (TID 25, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:15,342 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 25.0 in stage 0.0 (TID 25)
2015-04-05 10:37:15,347 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:838860800+33554432
2015-04-05 10:37:15,349 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 24.0 in stage 0.0 (TID 24) in 1735 ms on localhost (25/96)
2015-04-05 10:37:16,989 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 25.0 in stage 0.0 (TID 25). 1680 bytes result sent to driver
2015-04-05 10:37:16,990 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 26.0 in stage 0.0 (TID 26, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:16,991 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 26.0 in stage 0.0 (TID 26)
2015-04-05 10:37:16,996 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:872415232+33554432
2015-04-05 10:37:16,996 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 25.0 in stage 0.0 (TID 25) in 1649 ms on localhost (26/96)
2015-04-05 10:37:18,652 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 26.0 in stage 0.0 (TID 26). 1843 bytes result sent to driver
2015-04-05 10:37:18,653 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 27.0 in stage 0.0 (TID 27, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:18,654 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 27.0 in stage 0.0 (TID 27)
2015-04-05 10:37:18,657 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:905969664+33554432
2015-04-05 10:37:18,658 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 26.0 in stage 0.0 (TID 26) in 1665 ms on localhost (27/96)
2015-04-05 10:37:20,327 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 27.0 in stage 0.0 (TID 27). 1680 bytes result sent to driver
2015-04-05 10:37:20,328 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 28.0 in stage 0.0 (TID 28, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:20,328 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 28.0 in stage 0.0 (TID 28)
2015-04-05 10:37:20,332 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:939524096+33554432
2015-04-05 10:37:20,332 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 27.0 in stage 0.0 (TID 27) in 1675 ms on localhost (28/96)
2015-04-05 10:37:22,040 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 28.0 in stage 0.0 (TID 28). 1680 bytes result sent to driver
2015-04-05 10:37:22,042 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 29.0 in stage 0.0 (TID 29, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:22,042 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 29.0 in stage 0.0 (TID 29)
2015-04-05 10:37:22,046 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:973078528+33554432
2015-04-05 10:37:22,046 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 28.0 in stage 0.0 (TID 28) in 1715 ms on localhost (29/96)
2015-04-05 10:37:23,696 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 29.0 in stage 0.0 (TID 29). 1680 bytes result sent to driver
2015-04-05 10:37:23,697 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 30.0 in stage 0.0 (TID 30, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:23,698 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 30.0 in stage 0.0 (TID 30)
2015-04-05 10:37:23,702 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1006632960+33554432
2015-04-05 10:37:23,702 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 29.0 in stage 0.0 (TID 29) in 1657 ms on localhost (30/96)
2015-04-05 10:37:25,396 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 30.0 in stage 0.0 (TID 30). 1680 bytes result sent to driver
2015-04-05 10:37:25,398 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 31.0 in stage 0.0 (TID 31, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:25,398 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 31.0 in stage 0.0 (TID 31)
2015-04-05 10:37:25,402 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1040187392+33554432
2015-04-05 10:37:25,402 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 30.0 in stage 0.0 (TID 30) in 1701 ms on localhost (31/96)
2015-04-05 10:37:27,094 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 31.0 in stage 0.0 (TID 31). 1680 bytes result sent to driver
2015-04-05 10:37:27,096 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 32.0 in stage 0.0 (TID 32, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:27,096 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 32.0 in stage 0.0 (TID 32)
2015-04-05 10:37:27,099 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1073741824+33554432
2015-04-05 10:37:27,099 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 31.0 in stage 0.0 (TID 31) in 1699 ms on localhost (32/96)
2015-04-05 10:37:28,755 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 32.0 in stage 0.0 (TID 32). 1843 bytes result sent to driver
2015-04-05 10:37:28,757 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 33.0 in stage 0.0 (TID 33, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:28,757 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 33.0 in stage 0.0 (TID 33)
2015-04-05 10:37:28,761 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1107296256+33554432
2015-04-05 10:37:28,762 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 32.0 in stage 0.0 (TID 32) in 1662 ms on localhost (33/96)
2015-04-05 10:37:30,483 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 33.0 in stage 0.0 (TID 33). 1680 bytes result sent to driver
2015-04-05 10:37:30,484 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 34.0 in stage 0.0 (TID 34, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:30,485 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 34.0 in stage 0.0 (TID 34)
2015-04-05 10:37:30,489 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1140850688+33554432
2015-04-05 10:37:30,490 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 33.0 in stage 0.0 (TID 33) in 1729 ms on localhost (34/96)
2015-04-05 10:37:32,145 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 34.0 in stage 0.0 (TID 34). 1680 bytes result sent to driver
2015-04-05 10:37:32,147 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 35.0 in stage 0.0 (TID 35, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:32,147 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 35.0 in stage 0.0 (TID 35)
2015-04-05 10:37:32,150 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1174405120+33554432
2015-04-05 10:37:32,151 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 34.0 in stage 0.0 (TID 34) in 1663 ms on localhost (35/96)
2015-04-05 10:37:33,784 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 35.0 in stage 0.0 (TID 35). 1680 bytes result sent to driver
2015-04-05 10:37:33,785 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 36.0 in stage 0.0 (TID 36, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:33,786 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 36.0 in stage 0.0 (TID 36)
2015-04-05 10:37:33,789 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1207959552+33554432
2015-04-05 10:37:33,789 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 35.0 in stage 0.0 (TID 35) in 1640 ms on localhost (36/96)
2015-04-05 10:37:35,453 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 36.0 in stage 0.0 (TID 36). 1680 bytes result sent to driver
2015-04-05 10:37:35,455 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 37.0 in stage 0.0 (TID 37, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:35,455 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 37.0 in stage 0.0 (TID 37)
2015-04-05 10:37:35,459 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1241513984+33554432
2015-04-05 10:37:35,460 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 36.0 in stage 0.0 (TID 36) in 1670 ms on localhost (37/96)
2015-04-05 10:37:37,115 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 37.0 in stage 0.0 (TID 37). 1680 bytes result sent to driver
2015-04-05 10:37:37,117 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 38.0 in stage 0.0 (TID 38, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:37,117 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 38.0 in stage 0.0 (TID 38)
2015-04-05 10:37:37,120 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1275068416+33554432
2015-04-05 10:37:37,120 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 37.0 in stage 0.0 (TID 37) in 1663 ms on localhost (38/96)
2015-04-05 10:37:38,793 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 38.0 in stage 0.0 (TID 38). 1843 bytes result sent to driver
2015-04-05 10:37:38,794 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 39.0 in stage 0.0 (TID 39, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:38,794 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 39.0 in stage 0.0 (TID 39)
2015-04-05 10:37:38,797 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1308622848+33554432
2015-04-05 10:37:38,798 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 38.0 in stage 0.0 (TID 38) in 1678 ms on localhost (39/96)
2015-04-05 10:37:40,444 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 39.0 in stage 0.0 (TID 39). 1680 bytes result sent to driver
2015-04-05 10:37:40,446 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 40.0 in stage 0.0 (TID 40, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:40,446 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 40.0 in stage 0.0 (TID 40)
2015-04-05 10:37:40,449 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1342177280+33554432
2015-04-05 10:37:40,450 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 39.0 in stage 0.0 (TID 39) in 1653 ms on localhost (40/96)
2015-04-05 10:37:42,119 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 40.0 in stage 0.0 (TID 40). 1680 bytes result sent to driver
2015-04-05 10:37:42,120 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 41.0 in stage 0.0 (TID 41, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:42,120 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 41.0 in stage 0.0 (TID 41)
2015-04-05 10:37:42,124 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1375731712+33554432
2015-04-05 10:37:42,125 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 40.0 in stage 0.0 (TID 40) in 1675 ms on localhost (41/96)
2015-04-05 10:37:43,817 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 41.0 in stage 0.0 (TID 41). 1680 bytes result sent to driver
2015-04-05 10:37:43,818 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 42.0 in stage 0.0 (TID 42, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:43,819 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 42.0 in stage 0.0 (TID 42)
2015-04-05 10:37:43,821 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1409286144+33554432
2015-04-05 10:37:43,822 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 41.0 in stage 0.0 (TID 41) in 1700 ms on localhost (42/96)
2015-04-05 10:37:45,580 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 42.0 in stage 0.0 (TID 42). 1680 bytes result sent to driver
2015-04-05 10:37:45,581 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 43.0 in stage 0.0 (TID 43, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:45,581 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 43.0 in stage 0.0 (TID 43)
2015-04-05 10:37:45,586 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1442840576+33554432
2015-04-05 10:37:45,587 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 42.0 in stage 0.0 (TID 42) in 1764 ms on localhost (43/96)
2015-04-05 10:37:47,215 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 43.0 in stage 0.0 (TID 43). 1680 bytes result sent to driver
2015-04-05 10:37:47,216 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 44.0 in stage 0.0 (TID 44, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:47,216 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 44.0 in stage 0.0 (TID 44)
2015-04-05 10:37:47,220 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1476395008+33554432
2015-04-05 10:37:47,221 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 43.0 in stage 0.0 (TID 43) in 1636 ms on localhost (44/96)
2015-04-05 10:37:48,877 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 44.0 in stage 0.0 (TID 44). 1843 bytes result sent to driver
2015-04-05 10:37:48,878 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 45.0 in stage 0.0 (TID 45, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:48,878 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 45.0 in stage 0.0 (TID 45)
2015-04-05 10:37:48,881 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1509949440+33554432
2015-04-05 10:37:48,882 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 44.0 in stage 0.0 (TID 44) in 1663 ms on localhost (45/96)
2015-04-05 10:37:50,558 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 45.0 in stage 0.0 (TID 45). 1680 bytes result sent to driver
2015-04-05 10:37:50,562 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 46.0 in stage 0.0 (TID 46, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:50,563 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 46.0 in stage 0.0 (TID 46)
2015-04-05 10:37:50,566 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1543503872+33554432
2015-04-05 10:37:50,566 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 45.0 in stage 0.0 (TID 45) in 1686 ms on localhost (46/96)
2015-04-05 10:37:52,247 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 46.0 in stage 0.0 (TID 46). 1680 bytes result sent to driver
2015-04-05 10:37:52,249 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 47.0 in stage 0.0 (TID 47, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:52,250 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 47.0 in stage 0.0 (TID 47)
2015-04-05 10:37:52,254 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1577058304+33554432
2015-04-05 10:37:52,254 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 46.0 in stage 0.0 (TID 46) in 1689 ms on localhost (47/96)
2015-04-05 10:37:53,920 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 47.0 in stage 0.0 (TID 47). 1680 bytes result sent to driver
2015-04-05 10:37:53,922 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 48.0 in stage 0.0 (TID 48, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:53,922 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 48.0 in stage 0.0 (TID 48)
2015-04-05 10:37:53,926 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1610612736+33554432
2015-04-05 10:37:53,926 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 47.0 in stage 0.0 (TID 47) in 1674 ms on localhost (48/96)
2015-04-05 10:37:55,588 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 48.0 in stage 0.0 (TID 48). 1680 bytes result sent to driver
2015-04-05 10:37:55,590 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 49.0 in stage 0.0 (TID 49, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:55,590 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 49.0 in stage 0.0 (TID 49)
2015-04-05 10:37:55,593 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1644167168+33554432
2015-04-05 10:37:55,593 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 48.0 in stage 0.0 (TID 48) in 1669 ms on localhost (49/96)
2015-04-05 10:37:57,267 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 49.0 in stage 0.0 (TID 49). 1680 bytes result sent to driver
2015-04-05 10:37:57,270 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 50.0 in stage 0.0 (TID 50, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:57,270 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 50.0 in stage 0.0 (TID 50)
2015-04-05 10:37:57,273 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1677721600+33554432
2015-04-05 10:37:57,274 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 49.0 in stage 0.0 (TID 49) in 1681 ms on localhost (50/96)
2015-04-05 10:37:59,097 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 50.0 in stage 0.0 (TID 50). 1843 bytes result sent to driver
2015-04-05 10:37:59,099 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 51.0 in stage 0.0 (TID 51, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:37:59,099 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 51.0 in stage 0.0 (TID 51)
2015-04-05 10:37:59,103 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1711276032+33554432
2015-04-05 10:37:59,104 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 50.0 in stage 0.0 (TID 50) in 1831 ms on localhost (51/96)
2015-04-05 10:38:00,931 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 51.0 in stage 0.0 (TID 51). 1680 bytes result sent to driver
2015-04-05 10:38:00,933 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 52.0 in stage 0.0 (TID 52, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:00,934 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 52.0 in stage 0.0 (TID 52)
2015-04-05 10:38:00,937 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1744830464+33554432
2015-04-05 10:38:00,937 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 51.0 in stage 0.0 (TID 51) in 1835 ms on localhost (52/96)
2015-04-05 10:38:02,840 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 52.0 in stage 0.0 (TID 52). 1680 bytes result sent to driver
2015-04-05 10:38:02,843 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 53.0 in stage 0.0 (TID 53, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:02,843 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 53.0 in stage 0.0 (TID 53)
2015-04-05 10:38:02,847 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1778384896+33554432
2015-04-05 10:38:02,847 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 52.0 in stage 0.0 (TID 52) in 1911 ms on localhost (53/96)
2015-04-05 10:38:04,490 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 53.0 in stage 0.0 (TID 53). 1680 bytes result sent to driver
2015-04-05 10:38:04,491 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 54.0 in stage 0.0 (TID 54, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:04,492 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 54.0 in stage 0.0 (TID 54)
2015-04-05 10:38:04,495 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1811939328+33554432
2015-04-05 10:38:04,495 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 53.0 in stage 0.0 (TID 53) in 1650 ms on localhost (54/96)
2015-04-05 10:38:06,159 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 54.0 in stage 0.0 (TID 54). 1680 bytes result sent to driver
2015-04-05 10:38:06,161 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 55.0 in stage 0.0 (TID 55, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:06,162 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 55.0 in stage 0.0 (TID 55)
2015-04-05 10:38:06,166 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1845493760+33554432
2015-04-05 10:38:06,166 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 54.0 in stage 0.0 (TID 54) in 1671 ms on localhost (55/96)
2015-04-05 10:38:07,844 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 55.0 in stage 0.0 (TID 55). 1843 bytes result sent to driver
2015-04-05 10:38:07,845 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 56.0 in stage 0.0 (TID 56, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:07,846 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 56.0 in stage 0.0 (TID 56)
2015-04-05 10:38:07,851 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1879048192+33554432
2015-04-05 10:38:07,851 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 55.0 in stage 0.0 (TID 55) in 1686 ms on localhost (56/96)
2015-04-05 10:38:09,457 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 56.0 in stage 0.0 (TID 56). 1680 bytes result sent to driver
2015-04-05 10:38:09,459 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 57.0 in stage 0.0 (TID 57, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:09,460 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 57.0 in stage 0.0 (TID 57)
2015-04-05 10:38:09,463 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1912602624+33554432
2015-04-05 10:38:09,464 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 56.0 in stage 0.0 (TID 56) in 1616 ms on localhost (57/96)
2015-04-05 10:38:11,135 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 57.0 in stage 0.0 (TID 57). 1680 bytes result sent to driver
2015-04-05 10:38:11,137 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 58.0 in stage 0.0 (TID 58, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:11,137 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 58.0 in stage 0.0 (TID 58)
2015-04-05 10:38:11,140 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1946157056+33554432
2015-04-05 10:38:11,141 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 57.0 in stage 0.0 (TID 57) in 1679 ms on localhost (58/96)
2015-04-05 10:38:12,846 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 58.0 in stage 0.0 (TID 58). 1680 bytes result sent to driver
2015-04-05 10:38:12,848 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 59.0 in stage 0.0 (TID 59, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:12,848 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 59.0 in stage 0.0 (TID 59)
2015-04-05 10:38:12,853 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:1979711488+33554432
2015-04-05 10:38:12,853 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 58.0 in stage 0.0 (TID 58) in 1712 ms on localhost (59/96)
2015-04-05 10:38:14,559 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 59.0 in stage 0.0 (TID 59). 1680 bytes result sent to driver
2015-04-05 10:38:14,560 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 60.0 in stage 0.0 (TID 60, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:14,561 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 60.0 in stage 0.0 (TID 60)
2015-04-05 10:38:14,563 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2013265920+33554432
2015-04-05 10:38:14,564 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 59.0 in stage 0.0 (TID 59) in 1714 ms on localhost (60/96)
2015-04-05 10:38:16,364 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 60.0 in stage 0.0 (TID 60). 1680 bytes result sent to driver
2015-04-05 10:38:16,367 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 61.0 in stage 0.0 (TID 61, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:16,367 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 61.0 in stage 0.0 (TID 61)
2015-04-05 10:38:16,371 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2046820352+33554432
2015-04-05 10:38:16,372 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 60.0 in stage 0.0 (TID 60) in 1807 ms on localhost (61/96)
2015-04-05 10:38:18,023 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 61.0 in stage 0.0 (TID 61). 1843 bytes result sent to driver
2015-04-05 10:38:18,024 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 62.0 in stage 0.0 (TID 62, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:18,025 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 62.0 in stage 0.0 (TID 62)
2015-04-05 10:38:18,029 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2080374784+33554432
2015-04-05 10:38:18,030 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 61.0 in stage 0.0 (TID 61) in 1659 ms on localhost (62/96)
2015-04-05 10:38:19,737 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 62.0 in stage 0.0 (TID 62). 1680 bytes result sent to driver
2015-04-05 10:38:19,738 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 63.0 in stage 0.0 (TID 63, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:19,738 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 63.0 in stage 0.0 (TID 63)
2015-04-05 10:38:19,743 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2113929216+33554432
2015-04-05 10:38:19,743 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 62.0 in stage 0.0 (TID 62) in 1714 ms on localhost (63/96)
2015-04-05 10:38:21,436 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 63.0 in stage 0.0 (TID 63). 1680 bytes result sent to driver
2015-04-05 10:38:21,438 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 64.0 in stage 0.0 (TID 64, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:21,439 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 64.0 in stage 0.0 (TID 64)
2015-04-05 10:38:21,442 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2147483648+33554432
2015-04-05 10:38:21,443 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 63.0 in stage 0.0 (TID 63) in 1701 ms on localhost (64/96)
2015-04-05 10:38:23,110 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 64.0 in stage 0.0 (TID 64). 1680 bytes result sent to driver
2015-04-05 10:38:23,113 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 65.0 in stage 0.0 (TID 65, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:23,113 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 65.0 in stage 0.0 (TID 65)
2015-04-05 10:38:23,117 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2181038080+33554432
2015-04-05 10:38:23,117 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 64.0 in stage 0.0 (TID 64) in 1676 ms on localhost (65/96)
2015-04-05 10:38:24,792 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 65.0 in stage 0.0 (TID 65). 1680 bytes result sent to driver
2015-04-05 10:38:24,793 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 66.0 in stage 0.0 (TID 66, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:24,793 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 66.0 in stage 0.0 (TID 66)
2015-04-05 10:38:24,798 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2214592512+33554432
2015-04-05 10:38:24,798 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 65.0 in stage 0.0 (TID 65) in 1682 ms on localhost (66/96)
2015-04-05 10:38:26,502 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 66.0 in stage 0.0 (TID 66). 1680 bytes result sent to driver
2015-04-05 10:38:26,504 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 67.0 in stage 0.0 (TID 67, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:26,504 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 67.0 in stage 0.0 (TID 67)
2015-04-05 10:38:26,509 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2248146944+33554432
2015-04-05 10:38:26,510 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 66.0 in stage 0.0 (TID 66) in 1712 ms on localhost (67/96)
2015-04-05 10:38:28,158 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 67.0 in stage 0.0 (TID 67). 1843 bytes result sent to driver
2015-04-05 10:38:28,159 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 68.0 in stage 0.0 (TID 68, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:28,159 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 68.0 in stage 0.0 (TID 68)
2015-04-05 10:38:28,162 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2281701376+33554432
2015-04-05 10:38:28,163 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 67.0 in stage 0.0 (TID 67) in 1656 ms on localhost (68/96)
2015-04-05 10:38:29,835 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 68.0 in stage 0.0 (TID 68). 1680 bytes result sent to driver
2015-04-05 10:38:29,837 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 69.0 in stage 0.0 (TID 69, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:29,837 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 69.0 in stage 0.0 (TID 69)
2015-04-05 10:38:29,840 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2315255808+33554432
2015-04-05 10:38:29,841 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 68.0 in stage 0.0 (TID 68) in 1679 ms on localhost (69/96)
2015-04-05 10:38:31,462 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 69.0 in stage 0.0 (TID 69). 1680 bytes result sent to driver
2015-04-05 10:38:31,464 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 70.0 in stage 0.0 (TID 70, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:31,464 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 70.0 in stage 0.0 (TID 70)
2015-04-05 10:38:31,469 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2348810240+33554432
2015-04-05 10:38:31,470 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 69.0 in stage 0.0 (TID 69) in 1628 ms on localhost (70/96)
2015-04-05 10:38:33,182 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 70.0 in stage 0.0 (TID 70). 1680 bytes result sent to driver
2015-04-05 10:38:33,183 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 71.0 in stage 0.0 (TID 71, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:33,183 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 71.0 in stage 0.0 (TID 71)
2015-04-05 10:38:33,187 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2382364672+33554432
2015-04-05 10:38:33,187 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 70.0 in stage 0.0 (TID 70) in 1720 ms on localhost (71/96)
2015-04-05 10:38:34,907 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 71.0 in stage 0.0 (TID 71). 1680 bytes result sent to driver
2015-04-05 10:38:34,910 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 72.0 in stage 0.0 (TID 72, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:34,910 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 72.0 in stage 0.0 (TID 72)
2015-04-05 10:38:34,913 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2415919104+33554432
2015-04-05 10:38:34,914 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 71.0 in stage 0.0 (TID 71) in 1728 ms on localhost (72/96)
2015-04-05 10:38:37,044 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 72.0 in stage 0.0 (TID 72). 1680 bytes result sent to driver
2015-04-05 10:38:37,045 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 73.0 in stage 0.0 (TID 73, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:37,046 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 73.0 in stage 0.0 (TID 73)
2015-04-05 10:38:37,050 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2449473536+33554432
2015-04-05 10:38:37,050 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 72.0 in stage 0.0 (TID 72) in 2138 ms on localhost (73/96)
2015-04-05 10:38:39,215 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 73.0 in stage 0.0 (TID 73). 1843 bytes result sent to driver
2015-04-05 10:38:39,218 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 74.0 in stage 0.0 (TID 74, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:39,219 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 74.0 in stage 0.0 (TID 74)
2015-04-05 10:38:39,223 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 73.0 in stage 0.0 (TID 73) in 2174 ms on localhost (74/96)
2015-04-05 10:38:39,223 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2483027968+33554432
2015-04-05 10:38:41,370 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 74.0 in stage 0.0 (TID 74). 1680 bytes result sent to driver
2015-04-05 10:38:41,373 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 75.0 in stage 0.0 (TID 75, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:41,374 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 75.0 in stage 0.0 (TID 75)
2015-04-05 10:38:41,379 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2516582400+33554432
2015-04-05 10:38:41,379 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 74.0 in stage 0.0 (TID 74) in 2158 ms on localhost (75/96)
2015-04-05 10:38:43,583 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 75.0 in stage 0.0 (TID 75). 1680 bytes result sent to driver
2015-04-05 10:38:43,585 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 76.0 in stage 0.0 (TID 76, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:43,585 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 76.0 in stage 0.0 (TID 76)
2015-04-05 10:38:43,590 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 75.0 in stage 0.0 (TID 75) in 2214 ms on localhost (76/96)
2015-04-05 10:38:43,590 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2550136832+33554432
2015-04-05 10:38:45,913 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 76.0 in stage 0.0 (TID 76). 1680 bytes result sent to driver
2015-04-05 10:38:45,914 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 77.0 in stage 0.0 (TID 77, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:45,915 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 77.0 in stage 0.0 (TID 77)
2015-04-05 10:38:45,919 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2583691264+33554432
2015-04-05 10:38:45,920 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 76.0 in stage 0.0 (TID 76) in 2332 ms on localhost (77/96)
2015-04-05 10:38:48,137 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 77.0 in stage 0.0 (TID 77). 1843 bytes result sent to driver
2015-04-05 10:38:48,141 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 78.0 in stage 0.0 (TID 78, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:48,142 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 78.0 in stage 0.0 (TID 78)
2015-04-05 10:38:48,147 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2617245696+33554432
2015-04-05 10:38:48,147 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 77.0 in stage 0.0 (TID 77) in 2229 ms on localhost (78/96)
2015-04-05 10:38:50,440 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 78.0 in stage 0.0 (TID 78). 1680 bytes result sent to driver
2015-04-05 10:38:50,442 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 79.0 in stage 0.0 (TID 79, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:50,442 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 79.0 in stage 0.0 (TID 79)
2015-04-05 10:38:50,446 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2650800128+33554432
2015-04-05 10:38:50,447 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 78.0 in stage 0.0 (TID 78) in 2302 ms on localhost (79/96)
2015-04-05 10:38:53,051 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 79.0 in stage 0.0 (TID 79). 1680 bytes result sent to driver
2015-04-05 10:38:53,052 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 80.0 in stage 0.0 (TID 80, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:53,052 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 80.0 in stage 0.0 (TID 80)
2015-04-05 10:38:53,056 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2684354560+33554432
2015-04-05 10:38:53,056 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 79.0 in stage 0.0 (TID 79) in 2611 ms on localhost (80/96)
2015-04-05 10:38:55,463 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 80.0 in stage 0.0 (TID 80). 1680 bytes result sent to driver
2015-04-05 10:38:55,464 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 81.0 in stage 0.0 (TID 81, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:55,464 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 81.0 in stage 0.0 (TID 81)
2015-04-05 10:38:55,467 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2717908992+33554432
2015-04-05 10:38:55,468 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 80.0 in stage 0.0 (TID 80) in 2413 ms on localhost (81/96)
2015-04-05 10:38:57,961 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 81.0 in stage 0.0 (TID 81). 1843 bytes result sent to driver
2015-04-05 10:38:57,964 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 82.0 in stage 0.0 (TID 82, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:38:57,964 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 82.0 in stage 0.0 (TID 82)
2015-04-05 10:38:57,967 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2751463424+33554432
2015-04-05 10:38:57,969 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 81.0 in stage 0.0 (TID 81) in 2501 ms on localhost (82/96)
2015-04-05 10:39:00,448 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 82.0 in stage 0.0 (TID 82). 1680 bytes result sent to driver
2015-04-05 10:39:00,449 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 83.0 in stage 0.0 (TID 83, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:00,450 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 83.0 in stage 0.0 (TID 83)
2015-04-05 10:39:00,454 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2785017856+33554432
2015-04-05 10:39:00,454 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 82.0 in stage 0.0 (TID 82) in 2487 ms on localhost (83/96)
2015-04-05 10:39:02,681 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 83.0 in stage 0.0 (TID 83). 1680 bytes result sent to driver
2015-04-05 10:39:02,683 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 84.0 in stage 0.0 (TID 84, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:02,683 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 84.0 in stage 0.0 (TID 84)
2015-04-05 10:39:02,687 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2818572288+33554432
2015-04-05 10:39:02,688 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 83.0 in stage 0.0 (TID 83) in 2234 ms on localhost (84/96)
2015-04-05 10:39:04,433 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 84.0 in stage 0.0 (TID 84). 1680 bytes result sent to driver
2015-04-05 10:39:04,434 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 85.0 in stage 0.0 (TID 85, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:04,434 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 85.0 in stage 0.0 (TID 85)
2015-04-05 10:39:04,437 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2852126720+33554432
2015-04-05 10:39:04,437 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 84.0 in stage 0.0 (TID 84) in 1752 ms on localhost (85/96)
2015-04-05 10:39:06,149 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 85.0 in stage 0.0 (TID 85). 1680 bytes result sent to driver
2015-04-05 10:39:06,151 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 86.0 in stage 0.0 (TID 86, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:06,153 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 86.0 in stage 0.0 (TID 86)
2015-04-05 10:39:06,156 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2885681152+33554432
2015-04-05 10:39:06,157 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 85.0 in stage 0.0 (TID 85) in 1719 ms on localhost (86/96)
2015-04-05 10:39:07,919 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 86.0 in stage 0.0 (TID 86). 1843 bytes result sent to driver
2015-04-05 10:39:07,920 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 87.0 in stage 0.0 (TID 87, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:07,921 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 87.0 in stage 0.0 (TID 87)
2015-04-05 10:39:07,926 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 86.0 in stage 0.0 (TID 86) in 1771 ms on localhost (87/96)
2015-04-05 10:39:07,935 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2919235584+33554432
2015-04-05 10:39:10,537 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 87.0 in stage 0.0 (TID 87). 1680 bytes result sent to driver
2015-04-05 10:39:10,539 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 88.0 in stage 0.0 (TID 88, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:10,539 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 88.0 in stage 0.0 (TID 88)
2015-04-05 10:39:10,543 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 87.0 in stage 0.0 (TID 87) in 2620 ms on localhost (88/96)
2015-04-05 10:39:10,544 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2952790016+33554432
2015-04-05 10:39:12,367 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 88.0 in stage 0.0 (TID 88). 1680 bytes result sent to driver
2015-04-05 10:39:12,368 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 89.0 in stage 0.0 (TID 89, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:12,369 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 89.0 in stage 0.0 (TID 89)
2015-04-05 10:39:12,372 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:2986344448+33554432
2015-04-05 10:39:12,372 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 88.0 in stage 0.0 (TID 88) in 1831 ms on localhost (89/96)
2015-04-05 10:39:14,111 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 89.0 in stage 0.0 (TID 89). 1680 bytes result sent to driver
2015-04-05 10:39:14,112 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 90.0 in stage 0.0 (TID 90, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:14,113 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 90.0 in stage 0.0 (TID 90)
2015-04-05 10:39:14,116 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:3019898880+33554432
2015-04-05 10:39:14,117 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 89.0 in stage 0.0 (TID 89) in 1744 ms on localhost (90/96)
2015-04-05 10:39:15,823 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 90.0 in stage 0.0 (TID 90). 1680 bytes result sent to driver
2015-04-05 10:39:15,825 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 91.0 in stage 0.0 (TID 91, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:15,825 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 91.0 in stage 0.0 (TID 91)
2015-04-05 10:39:15,828 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:3053453312+33554432
2015-04-05 10:39:15,829 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 90.0 in stage 0.0 (TID 90) in 1714 ms on localhost (91/96)
2015-04-05 10:39:17,525 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 91.0 in stage 0.0 (TID 91). 1680 bytes result sent to driver
2015-04-05 10:39:17,526 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 92.0 in stage 0.0 (TID 92, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:17,527 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 92.0 in stage 0.0 (TID 92)
2015-04-05 10:39:17,529 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:3087007744+33554432
2015-04-05 10:39:17,530 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 91.0 in stage 0.0 (TID 91) in 1702 ms on localhost (92/96)
2015-04-05 10:39:19,359 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 92.0 in stage 0.0 (TID 92). 1843 bytes result sent to driver
2015-04-05 10:39:19,360 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 93.0 in stage 0.0 (TID 93, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:19,361 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 93.0 in stage 0.0 (TID 93)
2015-04-05 10:39:19,363 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:3120562176+33554432
2015-04-05 10:39:19,365 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 92.0 in stage 0.0 (TID 92) in 1835 ms on localhost (93/96)
2015-04-05 10:39:21,063 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 93.0 in stage 0.0 (TID 93). 1680 bytes result sent to driver
2015-04-05 10:39:21,066 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 94.0 in stage 0.0 (TID 94, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:21,066 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 94.0 in stage 0.0 (TID 94)
2015-04-05 10:39:21,069 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:3154116608+33554432
2015-04-05 10:39:21,071 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 93.0 in stage 0.0 (TID 93) in 1707 ms on localhost (94/96)
2015-04-05 10:39:22,828 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 94.0 in stage 0.0 (TID 94). 1680 bytes result sent to driver
2015-04-05 10:39:22,829 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 95.0 in stage 0.0 (TID 95, localhost, PROCESS_LOCAL, 1352 bytes)
2015-04-05 10:39:22,830 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 95.0 in stage 0.0 (TID 95)
2015-04-05 10:39:22,833 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 94.0 in stage 0.0 (TID 94) in 1765 ms on localhost (95/96)
2015-04-05 10:39:22,833 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:3187671040+32252456
2015-04-05 10:39:24,493 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 95.0 in stage 0.0 (TID 95). 1680 bytes result sent to driver
2015-04-05 10:39:24,502 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 95.0 in stage 0.0 (TID 95) in 1670 ms on localhost (96/96)
2015-04-05 10:39:24,503 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-05 10:39:24,503 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at ItemBasedRecommender.scala:129) finished in 175.647 s
2015-04-05 10:39:24,514 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemBasedRecommender.scala:129, took 175.857308 s
2015-04-05 10:39:24,560 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemBasedRecommender.scala:130
2015-04-05 10:39:24,566 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (distinct at ItemBasedRecommender.scala:130)
2015-04-05 10:39:24,567 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at ItemBasedRecommender.scala:130) with 96 output partitions (allowLocal=false)
2015-04-05 10:39:24,567 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 2(count at ItemBasedRecommender.scala:130)
2015-04-05 10:39:24,567 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 1)
2015-04-05 10:39:24,577 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 1)
2015-04-05 10:39:24,623 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[7] at distinct at ItemBasedRecommender.scala:130), which has no missing parents
2015-04-05 10:39:24,637 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4416) called with curMem=157858, maxMem=1030823608
2015-04-05 10:39:24,637 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 982.9 MB)
2015-04-05 10:39:24,642 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2607) called with curMem=162274, maxMem=1030823608
2015-04-05 10:39:24,642 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 10:39:24,644 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:52929 (size: 2.5 KB, free: 983.1 MB)
2015-04-05 10:39:24,644 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 10:39:24,645 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-04-05 10:39:24,679 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 96 missing tasks from Stage 1 (MappedRDD[7] at distinct at ItemBasedRecommender.scala:130)
2015-04-05 10:39:24,680 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 96 tasks
2015-04-05 10:39:24,684 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 96, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:24,684 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 96)
2015-04-05 10:39:24,693 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:0+33554432
2015-04-05 10:39:29,630 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 96). 2129 bytes result sent to driver
2015-04-05 10:39:29,631 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 97, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:29,632 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 97)
2015-04-05 10:39:29,636 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:33554432+33554432
2015-04-05 10:39:29,653 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 96) in 4949 ms on localhost (1/96)
2015-04-05 10:39:34,092 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 97). 1966 bytes result sent to driver
2015-04-05 10:39:34,094 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 98, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:34,095 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 98)
2015-04-05 10:39:34,098 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 97) in 4465 ms on localhost (2/96)
2015-04-05 10:39:34,099 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:67108864+33554432
2015-04-05 10:39:38,490 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 98). 2129 bytes result sent to driver
2015-04-05 10:39:38,491 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 99, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:38,492 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 1.0 (TID 99)
2015-04-05 10:39:38,494 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 98) in 4399 ms on localhost (3/96)
2015-04-05 10:39:38,495 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:100663296+33554432
2015-04-05 10:39:42,708 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 1.0 (TID 99). 1966 bytes result sent to driver
2015-04-05 10:39:42,710 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 1.0 (TID 100, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:42,711 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 1.0 (TID 100)
2015-04-05 10:39:42,714 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 99) in 4219 ms on localhost (4/96)
2015-04-05 10:39:42,714 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:134217728+33554432
2015-04-05 10:39:43,271 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 1
2015-04-05 10:39:43,274 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
2015-04-05 10:39:43,275 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 of size 1928 dropped from memory (free 1030660655)
2015-04-05 10:39:43,278 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:52929 in memory (size: 1928.0 B, free: 983.1 MB)
2015-04-05 10:39:43,278 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 10:39:43,278 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1
2015-04-05 10:39:43,278 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 of size 3368 dropped from memory (free 1030664023)
2015-04-05 10:39:43,282 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 1
2015-04-05 10:39:47,086 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 1.0 (TID 100). 1966 bytes result sent to driver
2015-04-05 10:39:47,087 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 1.0 (TID 101, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:47,088 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 1.0 (TID 101)
2015-04-05 10:39:47,090 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 1.0 (TID 100) in 4379 ms on localhost (5/96)
2015-04-05 10:39:47,091 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:167772160+33554432
2015-04-05 10:39:51,214 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 1.0 (TID 101). 2129 bytes result sent to driver
2015-04-05 10:39:51,215 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 1.0 (TID 102, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:51,215 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 1.0 (TID 102)
2015-04-05 10:39:51,218 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 1.0 (TID 101) in 4128 ms on localhost (6/96)
2015-04-05 10:39:51,218 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:201326592+33554432
2015-04-05 10:39:55,322 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 1.0 (TID 102). 1966 bytes result sent to driver
2015-04-05 10:39:55,324 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 1.0 (TID 103, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:55,324 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 1.0 (TID 103)
2015-04-05 10:39:55,326 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 1.0 (TID 102) in 4110 ms on localhost (7/96)
2015-04-05 10:39:55,328 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:234881024+33554432
2015-04-05 10:39:59,373 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 1.0 (TID 103). 2129 bytes result sent to driver
2015-04-05 10:39:59,374 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 1.0 (TID 104, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:39:59,374 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 1.0 (TID 104)
2015-04-05 10:39:59,376 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 1.0 (TID 103) in 4051 ms on localhost (8/96)
2015-04-05 10:39:59,377 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:268435456+33554432
2015-04-05 10:40:03,485 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 1.0 (TID 104). 1966 bytes result sent to driver
2015-04-05 10:40:03,486 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 1.0 (TID 105, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:03,486 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 1.0 (TID 105)
2015-04-05 10:40:03,489 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 1.0 (TID 104) in 4113 ms on localhost (9/96)
2015-04-05 10:40:03,490 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:301989888+33554432
2015-04-05 10:40:07,603 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 1.0 (TID 105). 1966 bytes result sent to driver
2015-04-05 10:40:07,605 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 1.0 (TID 106, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:07,605 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 10.0 in stage 1.0 (TID 106)
2015-04-05 10:40:07,607 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 1.0 (TID 105) in 4120 ms on localhost (10/96)
2015-04-05 10:40:07,608 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:335544320+33554432
2015-04-05 10:40:11,649 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 10.0 in stage 1.0 (TID 106). 2129 bytes result sent to driver
2015-04-05 10:40:11,650 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 1.0 (TID 107, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:11,651 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 11.0 in stage 1.0 (TID 107)
2015-04-05 10:40:11,652 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 1.0 (TID 106) in 4046 ms on localhost (11/96)
2015-04-05 10:40:11,653 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:369098752+33554432
2015-04-05 10:40:15,669 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 11.0 in stage 1.0 (TID 107). 1966 bytes result sent to driver
2015-04-05 10:40:15,670 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 12.0 in stage 1.0 (TID 108, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:15,671 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 12.0 in stage 1.0 (TID 108)
2015-04-05 10:40:15,673 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 1.0 (TID 107) in 4022 ms on localhost (12/96)
2015-04-05 10:40:15,674 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:402653184+33554432
2015-04-05 10:40:19,839 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 12.0 in stage 1.0 (TID 108). 2129 bytes result sent to driver
2015-04-05 10:40:19,840 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 13.0 in stage 1.0 (TID 109, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:19,841 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 13.0 in stage 1.0 (TID 109)
2015-04-05 10:40:19,842 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 12.0 in stage 1.0 (TID 108) in 4171 ms on localhost (13/96)
2015-04-05 10:40:19,843 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:436207616+33554432
2015-04-05 10:40:23,909 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 13.0 in stage 1.0 (TID 109). 1966 bytes result sent to driver
2015-04-05 10:40:23,910 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 14.0 in stage 1.0 (TID 110, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:23,910 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 14.0 in stage 1.0 (TID 110)
2015-04-05 10:40:23,911 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 13.0 in stage 1.0 (TID 109) in 4071 ms on localhost (14/96)
2015-04-05 10:40:23,913 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:469762048+33554432
2015-04-05 10:40:28,062 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 14.0 in stage 1.0 (TID 110). 2129 bytes result sent to driver
2015-04-05 10:40:28,063 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 15.0 in stage 1.0 (TID 111, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:28,064 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 15.0 in stage 1.0 (TID 111)
2015-04-05 10:40:28,065 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 14.0 in stage 1.0 (TID 110) in 4155 ms on localhost (15/96)
2015-04-05 10:40:28,067 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:503316480+33554432
2015-04-05 10:40:32,137 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 15.0 in stage 1.0 (TID 111). 1966 bytes result sent to driver
2015-04-05 10:40:32,138 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 16.0 in stage 1.0 (TID 112, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:32,138 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 16.0 in stage 1.0 (TID 112)
2015-04-05 10:40:32,141 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 15.0 in stage 1.0 (TID 111) in 4075 ms on localhost (16/96)
2015-04-05 10:40:32,142 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:536870912+33554432
2015-04-05 10:40:36,145 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 16.0 in stage 1.0 (TID 112). 1966 bytes result sent to driver
2015-04-05 10:40:36,147 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 17.0 in stage 1.0 (TID 113, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:36,147 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 17.0 in stage 1.0 (TID 113)
2015-04-05 10:40:36,149 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 16.0 in stage 1.0 (TID 112) in 4010 ms on localhost (17/96)
2015-04-05 10:40:36,149 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:570425344+33554432
2015-04-05 10:40:40,196 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 17.0 in stage 1.0 (TID 113). 2129 bytes result sent to driver
2015-04-05 10:40:40,197 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 18.0 in stage 1.0 (TID 114, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:40,197 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 18.0 in stage 1.0 (TID 114)
2015-04-05 10:40:40,199 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 17.0 in stage 1.0 (TID 113) in 4051 ms on localhost (18/96)
2015-04-05 10:40:40,200 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:603979776+33554432
2015-04-05 10:40:44,303 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 18.0 in stage 1.0 (TID 114). 1966 bytes result sent to driver
2015-04-05 10:40:44,304 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 19.0 in stage 1.0 (TID 115, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:44,304 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 19.0 in stage 1.0 (TID 115)
2015-04-05 10:40:44,306 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 18.0 in stage 1.0 (TID 114) in 4108 ms on localhost (19/96)
2015-04-05 10:40:44,307 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:637534208+33554432
2015-04-05 10:40:48,452 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 19.0 in stage 1.0 (TID 115). 2129 bytes result sent to driver
2015-04-05 10:40:48,453 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 20.0 in stage 1.0 (TID 116, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:48,453 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 20.0 in stage 1.0 (TID 116)
2015-04-05 10:40:48,455 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 19.0 in stage 1.0 (TID 115) in 4150 ms on localhost (20/96)
2015-04-05 10:40:48,456 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:671088640+33554432
2015-04-05 10:40:52,567 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 20.0 in stage 1.0 (TID 116). 1966 bytes result sent to driver
2015-04-05 10:40:52,568 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 21.0 in stage 1.0 (TID 117, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:52,568 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 21.0 in stage 1.0 (TID 117)
2015-04-05 10:40:52,570 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 20.0 in stage 1.0 (TID 116) in 4116 ms on localhost (21/96)
2015-04-05 10:40:52,571 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:704643072+33554432
2015-04-05 10:40:56,634 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 21.0 in stage 1.0 (TID 117). 1966 bytes result sent to driver
2015-04-05 10:40:56,635 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 22.0 in stage 1.0 (TID 118, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:40:56,636 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 22.0 in stage 1.0 (TID 118)
2015-04-05 10:40:56,637 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 21.0 in stage 1.0 (TID 117) in 4069 ms on localhost (22/96)
2015-04-05 10:40:56,638 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:738197504+33554432
2015-04-05 10:41:00,768 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 22.0 in stage 1.0 (TID 118). 2129 bytes result sent to driver
2015-04-05 10:41:00,769 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 23.0 in stage 1.0 (TID 119, localhost, PROCESS_LOCAL, 1341 bytes)
2015-04-05 10:41:00,770 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 23.0 in stage 1.0 (TID 119)
2015-04-05 10:41:00,771 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 22.0 in stage 1.0 (TID 118) in 4136 ms on localhost (23/96)
2015-04-05 10:41:00,772 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_100.csv:771751936+33554432
2015-04-05 10:45:55,911 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-05 10:45:56,253 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-05 10:45:56,255 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-05 10:45:56,256 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-05 10:45:56,796 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-05 10:45:56,892 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-04-05 10:45:57,241 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:53002]
2015-04-05 10:45:57,252 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53002.
2015-04-05 10:45:57,301 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-05 10:45:57,329 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-05 10:45:57,355 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150405104557-83d0
2015-04-05 10:45:57,365 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-05 10:45:57,842 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-05 10:45:57,985 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-4e105172-6812-491c-9730-4e3ce62c2d31
2015-04-05 10:45:58,001 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-05 10:45:58,202 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 10:45:58,230 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:53003
2015-04-05 10:45:58,230 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 53003.
2015-04-05 10:45:58,432 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 10:45:58,448 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-05 10:45:58,449 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-05 10:45:58,452 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-05 10:45:58,617 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:53002/user/HeartbeatReceiver
2015-04-05 10:45:58,831 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 53004
2015-04-05 10:45:58,834 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-05 10:45:58,835 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:53004 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 53004)
2015-04-05 10:45:58,838 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-05 10:45:59,527 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-05 10:45:59,530 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-05 10:45:59,877 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-05 10:45:59,878 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-05 10:45:59,882 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:53004 (size: 13.6 KB, free: 983.1 MB)
2015-04-05 10:45:59,883 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-05 10:45:59,890 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-05 10:46:00,256 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-05 10:46:00,283 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemBasedRecommender.scala:129
2015-04-05 10:46:00,307 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemBasedRecommender.scala:129) with 4 output partitions (allowLocal=false)
2015-04-05 10:46:00,307 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(count at ItemBasedRecommender.scala:129)
2015-04-05 10:46:00,308 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-05 10:46:00,320 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 10:46:00,332 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (FlatMappedRDD[5] at flatMap at DataRepo.scala:60), which has no missing parents
2015-04-05 10:46:00,373 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3376) called with curMem=152562, maxMem=1030823608
2015-04-05 10:46:00,373 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 10:46:00,382 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1939) called with curMem=155938, maxMem=1030823608
2015-04-05 10:46:00,383 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1939.0 B, free 982.9 MB)
2015-04-05 10:46:00,384 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:53004 (size: 1939.0 B, free: 983.1 MB)
2015-04-05 10:46:00,384 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 10:46:00,386 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-05 10:46:00,397 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 0 (FlatMappedRDD[5] at flatMap at DataRepo.scala:60)
2015-04-05 10:46:00,399 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
2015-04-05 10:46:00,432 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:00,441 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-05 10:46:00,475 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:0+33554432
2015-04-05 10:46:00,491 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-05 10:46:00,491 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-05 10:46:00,491 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-05 10:46:00,491 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-05 10:46:00,492 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-05 10:46:02,919 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1680 bytes result sent to driver
2015-04-05 10:46:02,923 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:02,924 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
2015-04-05 10:46:02,933 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:46:02,941 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2506 ms on localhost (1/4)
2015-04-05 10:46:04,957 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1680 bytes result sent to driver
2015-04-05 10:46:04,959 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:04,960 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
2015-04-05 10:46:04,967 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:46:04,968 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 2040 ms on localhost (2/4)
2015-04-05 10:46:06,912 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1680 bytes result sent to driver
2015-04-05 10:46:06,914 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:06,915 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
2015-04-05 10:46:06,922 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:46:06,923 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 1958 ms on localhost (3/4)
2015-04-05 10:46:08,768 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1680 bytes result sent to driver
2015-04-05 10:46:08,780 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 1861 ms on localhost (4/4)
2015-04-05 10:46:08,781 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-05 10:46:08,781 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (count at ItemBasedRecommender.scala:129) finished in 8.369 s
2015-04-05 10:46:08,789 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemBasedRecommender.scala:129, took 8.506035 s
2015-04-05 10:46:08,840 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemBasedRecommender.scala:130
2015-04-05 10:46:08,845 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (distinct at ItemBasedRecommender.scala:130)
2015-04-05 10:46:08,846 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (count at ItemBasedRecommender.scala:130) with 4 output partitions (allowLocal=false)
2015-04-05 10:46:08,846 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 2(count at ItemBasedRecommender.scala:130)
2015-04-05 10:46:08,846 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 1)
2015-04-05 10:46:08,848 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 1)
2015-04-05 10:46:08,854 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[7] at distinct at ItemBasedRecommender.scala:130), which has no missing parents
2015-04-05 10:46:08,865 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4424) called with curMem=157877, maxMem=1030823608
2015-04-05 10:46:08,865 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 982.9 MB)
2015-04-05 10:46:08,871 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2607) called with curMem=162301, maxMem=1030823608
2015-04-05 10:46:08,872 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 10:46:08,873 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:53004 (size: 2.5 KB, free: 983.1 MB)
2015-04-05 10:46:08,873 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 10:46:08,874 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-04-05 10:46:08,878 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 1 (MappedRDD[7] at distinct at ItemBasedRecommender.scala:130)
2015-04-05 10:46:08,879 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 4 tasks
2015-04-05 10:46:08,880 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:08,880 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
2015-04-05 10:46:08,888 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:0+33554432
2015-04-05 10:46:13,280 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 1874 bytes result sent to driver
2015-04-05 10:46:13,282 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 5, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:13,283 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 1.0 (TID 5)
2015-04-05 10:46:13,291 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:46:13,303 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 4405 ms on localhost (1/4)
2015-04-05 10:46:17,731 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 1.0 (TID 5). 2037 bytes result sent to driver
2015-04-05 10:46:17,733 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 6, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:17,735 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 1.0 (TID 6)
2015-04-05 10:46:17,742 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 5) in 4454 ms on localhost (2/4)
2015-04-05 10:46:17,745 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:46:21,925 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 1.0 (TID 6). 1874 bytes result sent to driver
2015-04-05 10:46:21,926 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 7, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:21,927 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 1.0 (TID 7)
2015-04-05 10:46:21,933 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 6) in 4195 ms on localhost (3/4)
2015-04-05 10:46:21,934 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:46:25,389 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 1.0 (TID 7). 2037 bytes result sent to driver
2015-04-05 10:46:25,398 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 7) in 3468 ms on localhost (4/4)
2015-04-05 10:46:25,398 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-04-05 10:46:25,398 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (distinct at ItemBasedRecommender.scala:130) finished in 16.519 s
2015-04-05 10:46:25,399 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 10:46:25,399 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 10:46:25,400 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 2)
2015-04-05 10:46:25,400 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 10:46:25,406 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 2: List()
2015-04-05 10:46:25,409 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[9] at distinct at ItemBasedRecommender.scala:130), which is now runnable
2015-04-05 10:46:25,412 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=164908, maxMem=1030823608
2015-04-05 10:46:25,412 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 10:46:25,418 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1546) called with curMem=167444, maxMem=1030823608
2015-04-05 10:46:25,419 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1546.0 B, free 982.9 MB)
2015-04-05 10:46:25,420 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:53004 (size: 1546.0 B, free: 983.1 MB)
2015-04-05 10:46:25,420 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-04-05 10:46:25,421 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-04-05 10:46:25,423 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 2 (MappedRDD[9] at distinct at ItemBasedRecommender.scala:130)
2015-04-05 10:46:25,423 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 4 tasks
2015-04-05 10:46:25,425 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 8, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:25,425 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 8)
2015-04-05 10:46:25,448 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:25,451 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 7 ms
2015-04-05 10:46:25,634 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 8). 783 bytes result sent to driver
2015-04-05 10:46:25,636 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 9, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:25,636 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 9)
2015-04-05 10:46:25,641 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:25,642 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 10:46:25,642 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 8) in 212 ms on localhost (1/4)
2015-04-05 10:46:25,731 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 9). 783 bytes result sent to driver
2015-04-05 10:46:25,733 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 10, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:25,734 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 10)
2015-04-05 10:46:25,739 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:25,739 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:46:25,740 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 9) in 98 ms on localhost (2/4)
2015-04-05 10:46:25,803 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 10). 783 bytes result sent to driver
2015-04-05 10:46:25,804 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 11, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:25,804 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 11)
2015-04-05 10:46:25,808 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 10) in 72 ms on localhost (3/4)
2015-04-05 10:46:25,808 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:25,808 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:46:25,880 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 11). 783 bytes result sent to driver
2015-04-05 10:46:25,891 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 11) in 83 ms on localhost (4/4)
2015-04-05 10:46:25,893 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-04-05 10:46:25,896 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (count at ItemBasedRecommender.scala:130) finished in 0.471 s
2015-04-05 10:46:25,897 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: count at ItemBasedRecommender.scala:130, took 17.056416 s
2015-04-05 10:46:25,933 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemBasedRecommender.scala:131
2015-04-05 10:46:25,936 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (distinct at ItemBasedRecommender.scala:131)
2015-04-05 10:46:25,939 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (count at ItemBasedRecommender.scala:131) with 4 output partitions (allowLocal=false)
2015-04-05 10:46:25,939 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 4(count at ItemBasedRecommender.scala:131)
2015-04-05 10:46:25,939 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 3)
2015-04-05 10:46:25,941 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 3)
2015-04-05 10:46:25,948 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 3 (MappedRDD[11] at distinct at ItemBasedRecommender.scala:131), which has no missing parents
2015-04-05 10:46:25,951 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4424) called with curMem=168990, maxMem=1030823608
2015-04-05 10:46:25,952 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 4.3 KB, free 982.9 MB)
2015-04-05 10:46:25,957 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2612) called with curMem=173414, maxMem=1030823608
2015-04-05 10:46:25,958 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.6 KB, free 982.9 MB)
2015-04-05 10:46:25,959 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:53004 (size: 2.6 KB, free: 983.0 MB)
2015-04-05 10:46:25,960 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
2015-04-05 10:46:25,961 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:838
2015-04-05 10:46:25,963 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 3 (MappedRDD[11] at distinct at ItemBasedRecommender.scala:131)
2015-04-05 10:46:25,963 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
2015-04-05 10:46:25,965 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 12, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:25,966 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 12)
2015-04-05 10:46:25,971 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:0+33554432
2015-04-05 10:46:26,136 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 3
2015-04-05 10:46:26,139 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_3_piece0
2015-04-05 10:46:26,139 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 of size 1546 dropped from memory (free 1030649128)
2015-04-05 10:46:26,141 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:53004 in memory (size: 1546.0 B, free: 983.0 MB)
2015-04-05 10:46:26,141 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-04-05 10:46:26,142 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_3
2015-04-05 10:46:26,142 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 of size 2536 dropped from memory (free 1030651664)
2015-04-05 10:46:26,145 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 3
2015-04-05 10:46:30,761 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 12). 1874 bytes result sent to driver
2015-04-05 10:46:30,763 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 13, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:30,764 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 13)
2015-04-05 10:46:30,770 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 12) in 4800 ms on localhost (1/4)
2015-04-05 10:46:30,771 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:46:35,180 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 13). 2037 bytes result sent to driver
2015-04-05 10:46:35,181 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 14, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:35,181 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 14)
2015-04-05 10:46:35,186 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 13) in 4420 ms on localhost (2/4)
2015-04-05 10:46:35,187 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:46:39,629 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 14). 1874 bytes result sent to driver
2015-04-05 10:46:39,630 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 15, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:46:39,631 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 15)
2015-04-05 10:46:39,635 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 14) in 4452 ms on localhost (3/4)
2015-04-05 10:46:39,637 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:46:44,150 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 15). 1874 bytes result sent to driver
2015-04-05 10:46:44,157 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 15) in 4524 ms on localhost (4/4)
2015-04-05 10:46:44,157 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 3 (distinct at ItemBasedRecommender.scala:131) finished in 18.193 s
2015-04-05 10:46:44,157 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 10:46:44,157 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2015-04-05 10:46:44,157 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 10:46:44,158 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 4)
2015-04-05 10:46:44,158 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 10:46:44,161 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 4: List()
2015-04-05 10:46:44,161 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 4 (MappedRDD[13] at distinct at ItemBasedRecommender.scala:131), which is now runnable
2015-04-05 10:46:44,163 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=171944, maxMem=1030823608
2015-04-05 10:46:44,164 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 10:46:44,168 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1539) called with curMem=174480, maxMem=1030823608
2015-04-05 10:46:44,168 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1539.0 B, free 982.9 MB)
2015-04-05 10:46:44,169 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:53004 (size: 1539.0 B, free: 983.0 MB)
2015-04-05 10:46:44,169 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
2015-04-05 10:46:44,170 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:838
2015-04-05 10:46:44,171 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 4 (MappedRDD[13] at distinct at ItemBasedRecommender.scala:131)
2015-04-05 10:46:44,171 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 4 tasks
2015-04-05 10:46:44,172 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 16, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:44,173 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 16)
2015-04-05 10:46:44,178 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:44,178 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:46:44,250 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 16). 783 bytes result sent to driver
2015-04-05 10:46:44,252 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 17, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:44,253 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 17)
2015-04-05 10:46:44,258 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 16) in 80 ms on localhost (1/4)
2015-04-05 10:46:44,258 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:44,259 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 10:46:44,307 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 17). 783 bytes result sent to driver
2015-04-05 10:46:44,309 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 18, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:44,310 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 18)
2015-04-05 10:46:44,315 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:44,315 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:46:44,323 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 17) in 66 ms on localhost (2/4)
2015-04-05 10:46:44,414 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 18). 783 bytes result sent to driver
2015-04-05 10:46:44,416 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 19, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:46:44,417 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 4.0 (TID 19)
2015-04-05 10:46:44,420 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 18) in 108 ms on localhost (3/4)
2015-04-05 10:46:44,422 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:46:44,423 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 10:46:44,456 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 4.0 (TID 19). 783 bytes result sent to driver
2015-04-05 10:46:44,460 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 4 (count at ItemBasedRecommender.scala:131) finished in 0.289 s
2015-04-05 10:46:44,460 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 19) in 42 ms on localhost (4/4)
2015-04-05 10:46:44,460 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2015-04-05 10:46:44,461 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: count at ItemBasedRecommender.scala:131, took 18.527544 s
2015-04-05 10:46:44,490 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemBasedRecommender.scala:137
2015-04-05 10:46:44,492 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (count at ItemBasedRecommender.scala:137) with 4 output partitions (allowLocal=false)
2015-04-05 10:46:44,492 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 5(count at ItemBasedRecommender.scala:137)
2015-04-05 10:46:44,492 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-05 10:46:44,496 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 10:46:44,497 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 5 (MappedRDD[17] at values at ItemBasedRecommender.scala:135), which has no missing parents
2015-04-05 10:46:44,501 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3784) called with curMem=176019, maxMem=1030823608
2015-04-05 10:46:44,501 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 982.9 MB)
2015-04-05 10:46:44,505 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2171) called with curMem=179803, maxMem=1030823608
2015-04-05 10:46:44,506 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 982.9 MB)
2015-04-05 10:46:44,506 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:53004 (size: 2.1 KB, free: 983.0 MB)
2015-04-05 10:46:44,506 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
2015-04-05 10:46:44,507 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:838
2015-04-05 10:46:44,509 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 5 (MappedRDD[17] at values at ItemBasedRecommender.scala:135)
2015-04-05 10:46:44,509 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
2015-04-05 10:46:44,510 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 20, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:44,511 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 20)
2015-04-05 10:46:44,519 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_17_0 not found, computing it
2015-04-05 10:46:44,519 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:0+33554432
2015-04-05 10:46:44,700 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 5
2015-04-05 10:46:44,700 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_5_piece0
2015-04-05 10:46:44,700 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 of size 1539 dropped from memory (free 1030643173)
2015-04-05 10:46:44,701 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on localhost:53004 in memory (size: 1539.0 B, free: 983.0 MB)
2015-04-05 10:46:44,701 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
2015-04-05 10:46:44,702 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_5
2015-04-05 10:46:44,702 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5 of size 2536 dropped from memory (free 1030645709)
2015-04-05 10:46:44,702 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 5
2015-04-05 10:46:47,073 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 4
2015-04-05 10:46:47,074 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_4
2015-04-05 10:46:47,074 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4 of size 4424 dropped from memory (free 1030650133)
2015-04-05 10:46:47,074 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_4_piece0
2015-04-05 10:46:47,074 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 of size 2612 dropped from memory (free 1030652745)
2015-04-05 10:46:47,075 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on localhost:53004 in memory (size: 2.6 KB, free: 983.1 MB)
2015-04-05 10:46:47,076 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
2015-04-05 10:46:47,076 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 4
2015-04-05 10:46:47,081 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned shuffle 1
2015-04-05 10:46:47,082 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 2
2015-04-05 10:46:47,083 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_2
2015-04-05 10:46:47,083 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 of size 4424 dropped from memory (free 1030657169)
2015-04-05 10:46:47,083 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
2015-04-05 10:46:47,083 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 of size 2607 dropped from memory (free 1030659776)
2015-04-05 10:46:47,084 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:53004 in memory (size: 2.5 KB, free: 983.1 MB)
2015-04-05 10:46:47,084 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 10:46:47,085 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 2
2015-04-05 10:46:47,085 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned shuffle 0
2015-04-05 10:46:47,086 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 1
2015-04-05 10:46:47,087 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
2015-04-05 10:46:47,087 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 of size 1939 dropped from memory (free 1030661715)
2015-04-05 10:46:47,088 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:53004 in memory (size: 1939.0 B, free: 983.1 MB)
2015-04-05 10:46:47,089 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 10:46:47,090 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1
2015-04-05 10:46:47,090 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 of size 3376 dropped from memory (free 1030665091)
2015-04-05 10:46:47,091 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 1
2015-04-05 10:46:49,212 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(120604983) called with curMem=158517, maxMem=1030823608
2015-04-05 10:46:49,212 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block rdd_17_0 stored as values in memory (estimated size 115.0 MB, free 867.9 MB)
2015-04-05 10:46:49,213 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerInfo - Added rdd_17_0 in memory on localhost:53004 (size: 115.0 MB, free: 868.0 MB)
2015-04-05 10:46:49,213 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block rdd_17_0
2015-04-05 10:46:49,335 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 20). 2412 bytes result sent to driver
2015-04-05 10:46:49,336 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 21, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:49,336 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 21)
2015-04-05 10:46:49,341 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 20) in 4829 ms on localhost (1/4)
2015-04-05 10:46:49,342 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_17_1 not found, computing it
2015-04-05 10:46:49,342 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:46:52,855 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(120745168) called with curMem=120763500, maxMem=1030823608
2015-04-05 10:46:52,856 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block rdd_17_1 stored as values in memory (estimated size 115.2 MB, free 752.7 MB)
2015-04-05 10:46:52,856 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added rdd_17_1 in memory on localhost:53004 (size: 115.2 MB, free: 752.9 MB)
2015-04-05 10:46:52,857 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block rdd_17_1
2015-04-05 10:46:52,998 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 21). 2249 bytes result sent to driver
2015-04-05 10:46:53,000 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 22, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:53,000 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 22)
2015-04-05 10:46:53,005 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 21) in 3666 ms on localhost (2/4)
2015-04-05 10:46:53,006 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_17_2 not found, computing it
2015-04-05 10:46:53,007 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:46:56,581 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(120745168) called with curMem=241508668, maxMem=1030823608
2015-04-05 10:46:56,582 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block rdd_17_2 stored as values in memory (estimated size 115.2 MB, free 637.6 MB)
2015-04-05 10:46:56,582 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerInfo - Added rdd_17_2 in memory on localhost:53004 (size: 115.2 MB, free: 637.7 MB)
2015-04-05 10:46:56,583 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block rdd_17_2
2015-04-05 10:46:56,711 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 22). 2412 bytes result sent to driver
2015-04-05 10:46:56,712 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 23, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:56,713 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 23)
2015-04-05 10:46:56,716 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 22) in 3714 ms on localhost (3/4)
2015-04-05 10:46:56,720 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_17_3 not found, computing it
2015-04-05 10:46:56,720 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:46:59,211 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(105138016) called with curMem=362253836, maxMem=1030823608
2015-04-05 10:46:59,212 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block rdd_17_3 stored as values in memory (estimated size 100.3 MB, free 537.3 MB)
2015-04-05 10:46:59,213 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added rdd_17_3 in memory on localhost:53004 (size: 100.3 MB, free: 537.5 MB)
2015-04-05 10:46:59,214 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block rdd_17_3
2015-04-05 10:46:59,329 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 23). 2249 bytes result sent to driver
2015-04-05 10:46:59,334 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 23) in 2621 ms on localhost (4/4)
2015-04-05 10:46:59,335 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 5 (count at ItemBasedRecommender.scala:137) finished in 14.825 s
2015-04-05 10:46:59,335 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2015-04-05 10:46:59,335 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: count at ItemBasedRecommender.scala:137, took 14.844413 s
2015-04-05 10:46:59,339 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemBasedRecommender.scala:138
2015-04-05 10:46:59,340 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at ItemBasedRecommender.scala:138) with 4 output partitions (allowLocal=false)
2015-04-05 10:46:59,340 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 6(count at ItemBasedRecommender.scala:138)
2015-04-05 10:46:59,340 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-05 10:46:59,345 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 10:46:59,345 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 6 (MappedRDD[15] at values at ItemBasedRecommender.scala:134), which has no missing parents
2015-04-05 10:46:59,347 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3784) called with curMem=467391852, maxMem=1030823608
2015-04-05 10:46:59,347 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 3.7 KB, free 537.3 MB)
2015-04-05 10:46:59,351 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2172) called with curMem=467395636, maxMem=1030823608
2015-04-05 10:46:59,351 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.1 KB, free 537.3 MB)
2015-04-05 10:46:59,352 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on localhost:53004 (size: 2.1 KB, free: 537.5 MB)
2015-04-05 10:46:59,352 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
2015-04-05 10:46:59,353 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:838
2015-04-05 10:46:59,355 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 6 (MappedRDD[15] at values at ItemBasedRecommender.scala:134)
2015-04-05 10:46:59,355 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 4 tasks
2015-04-05 10:46:59,356 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 24, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:46:59,356 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 24)
2015-04-05 10:46:59,360 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_0 not found, computing it
2015-04-05 10:46:59,360 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:0+33554432
2015-04-05 10:47:03,097 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 6
2015-04-05 10:47:03,098 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_6_piece0
2015-04-05 10:47:03,098 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 of size 2171 dropped from memory (free 563427971)
2015-04-05 10:47:03,099 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on localhost:53004 in memory (size: 2.1 KB, free: 537.5 MB)
2015-04-05 10:47:03,100 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
2015-04-05 10:47:03,100 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_6
2015-04-05 10:47:03,100 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6 of size 3784 dropped from memory (free 563431755)
2015-04-05 10:47:03,101 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 6
2015-04-05 10:47:12,538 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(482326976) called with curMem=467391853, maxMem=1030823608
2015-04-05 10:47:12,539 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block rdd_15_0 stored as values in memory (estimated size 460.0 MB, free 77.3 MB)
2015-04-05 10:47:12,539 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added rdd_15_0 in memory on localhost:53004 (size: 460.0 MB, free: 77.5 MB)
2015-04-05 10:47:12,540 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block rdd_15_0
2015-04-05 10:47:13,080 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 24). 2412 bytes result sent to driver
2015-04-05 10:47:13,081 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 25, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:13,082 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 25)
2015-04-05 10:47:13,085 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 24) in 13727 ms on localhost (1/4)
2015-04-05 10:47:13,086 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_1 not found, computing it
2015-04-05 10:47:13,086 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:47:13,688 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138216541) called with curMem=949718829, maxMem=1030823608
2015-04-05 10:47:13,689 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - 3 blocks selected for dropping
2015-04-05 10:47:13,698 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Dropping block broadcast_0_piece0 from memory
2015-04-05 10:47:13,699 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Writing block broadcast_0_piece0 to disk
2015-04-05 10:47:13,701 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 of size 13887 dropped from memory (free 81118666)
2015-04-05 10:47:13,702 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 on disk on localhost:53004 (size: 13.6 KB)
2015-04-05 10:47:13,702 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-05 10:47:13,703 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Dropping block rdd_17_0 from memory
2015-04-05 10:47:13,703 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block rdd_17_0 of size 120604983 dropped from memory (free 201723649)
2015-04-05 10:47:13,704 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerInfo - Removed rdd_17_0 on localhost:53004 in memory (size: 115.0 MB, free: 192.5 MB)
2015-04-05 10:47:13,704 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block rdd_17_0
2015-04-05 10:47:13,704 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Dropping block rdd_17_1 from memory
2015-04-05 10:47:13,704 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Block rdd_17_1 of size 120745168 dropped from memory (free 322468817)
2015-04-05 10:47:13,705 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerInfo - Removed rdd_17_1 on localhost:53004 in memory (size: 115.2 MB, free: 307.7 MB)
2015-04-05 10:47:13,705 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block rdd_17_1
2015-04-05 10:47:15,004 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_1 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:15,005 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.2 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:19,068 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 25). 2616 bytes result sent to driver
2015-04-05 10:47:19,070 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 26, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:19,070 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 26)
2015-04-05 10:47:19,074 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 25) in 5991 ms on localhost (2/4)
2015-04-05 10:47:19,075 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_2 not found, computing it
2015-04-05 10:47:19,075 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:47:20,745 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_2 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:20,745 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.2 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:22,358 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 26). 1729 bytes result sent to driver
2015-04-05 10:47:22,359 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 27, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:22,359 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 27)
2015-04-05 10:47:22,362 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 26) in 3291 ms on localhost (3/4)
2015-04-05 10:47:22,364 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_3 not found, computing it
2015-04-05 10:47:22,364 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:47:23,972 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_3 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:23,973 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.2 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:25,272 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 27). 1729 bytes result sent to driver
2015-04-05 10:47:25,276 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 27) in 2915 ms on localhost (4/4)
2015-04-05 10:47:25,277 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 6 (count at ItemBasedRecommender.scala:138) finished in 25.921 s
2015-04-05 10:47:25,277 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2015-04-05 10:47:25,277 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at ItemBasedRecommender.scala:138, took 25.937421 s
2015-04-05 10:47:25,280 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at CooccurrenceAnalysis.scala:37
2015-04-05 10:47:25,281 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 (count at CooccurrenceAnalysis.scala:37) with 4 output partitions (allowLocal=false)
2015-04-05 10:47:25,281 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 7(count at CooccurrenceAnalysis.scala:37)
2015-04-05 10:47:25,281 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-05 10:47:25,295 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 10:47:25,296 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 7 (MappedRDD[15] at values at ItemBasedRecommender.scala:134), which has no missing parents
2015-04-05 10:47:25,298 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3784) called with curMem=708354791, maxMem=1030823608
2015-04-05 10:47:25,298 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 307.5 MB)
2015-04-05 10:47:25,301 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2172) called with curMem=708358575, maxMem=1030823608
2015-04-05 10:47:25,302 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.1 KB, free 307.5 MB)
2015-04-05 10:47:25,303 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on localhost:53004 (size: 2.1 KB, free: 307.7 MB)
2015-04-05 10:47:25,303 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_8_piece0
2015-04-05 10:47:25,303 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:838
2015-04-05 10:47:25,304 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 7 (MappedRDD[15] at values at ItemBasedRecommender.scala:134)
2015-04-05 10:47:25,305 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 4 tasks
2015-04-05 10:47:25,309 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 28, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:25,309 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 28)
2015-04-05 10:47:25,314 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Found block rdd_15_0 locally
2015-04-05 10:47:25,845 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 28). 1680 bytes result sent to driver
2015-04-05 10:47:25,846 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 29, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:25,846 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 29)
2015-04-05 10:47:25,849 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 28) in 538 ms on localhost (1/4)
2015-04-05 10:47:25,851 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_1 not found, computing it
2015-04-05 10:47:25,851 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:47:27,413 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_1 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:27,414 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.2 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:28,956 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 29). 1892 bytes result sent to driver
2015-04-05 10:47:28,958 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 30, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:28,958 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 30)
2015-04-05 10:47:28,961 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 29) in 3113 ms on localhost (2/4)
2015-04-05 10:47:28,963 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_2 not found, computing it
2015-04-05 10:47:28,963 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:47:30,478 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_2 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:30,479 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.2 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:32,021 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 30). 1729 bytes result sent to driver
2015-04-05 10:47:32,023 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 31, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:32,023 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 7.0 (TID 31)
2015-04-05 10:47:32,026 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 30) in 3066 ms on localhost (3/4)
2015-04-05 10:47:32,028 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_3 not found, computing it
2015-04-05 10:47:32,028 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:47:33,550 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_3 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:33,550 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.2 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:34,765 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 7.0 (TID 31). 1729 bytes result sent to driver
2015-04-05 10:47:34,768 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 7.0 (TID 31) in 2744 ms on localhost (4/4)
2015-04-05 10:47:34,768 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 7 (count at CooccurrenceAnalysis.scala:37) finished in 9.460 s
2015-04-05 10:47:34,769 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2015-04-05 10:47:34,769 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 5 finished: count at CooccurrenceAnalysis.scala:37, took 9.488688 s
2015-04-05 10:47:34,772 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at CooccurrenceAnalysis.scala:37
2015-04-05 10:47:34,773 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 6 (count at CooccurrenceAnalysis.scala:37) with 4 output partitions (allowLocal=false)
2015-04-05 10:47:34,773 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 8(count at CooccurrenceAnalysis.scala:37)
2015-04-05 10:47:34,773 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-05 10:47:34,777 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 10:47:34,777 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 8 (MappedRDD[15] at values at ItemBasedRecommender.scala:134), which has no missing parents
2015-04-05 10:47:34,779 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3784) called with curMem=708360747, maxMem=1030823608
2015-04-05 10:47:34,779 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 3.7 KB, free 307.5 MB)
2015-04-05 10:47:34,782 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2172) called with curMem=708364531, maxMem=1030823608
2015-04-05 10:47:34,782 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.1 KB, free 307.5 MB)
2015-04-05 10:47:34,783 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on localhost:53004 (size: 2.1 KB, free: 307.7 MB)
2015-04-05 10:47:34,783 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_9_piece0
2015-04-05 10:47:34,784 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:838
2015-04-05 10:47:34,785 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 8 (MappedRDD[15] at values at ItemBasedRecommender.scala:134)
2015-04-05 10:47:34,786 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 4 tasks
2015-04-05 10:47:34,787 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 32, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:34,787 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 32)
2015-04-05 10:47:34,791 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Found block rdd_15_0 locally
2015-04-05 10:47:35,216 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 32). 1680 bytes result sent to driver
2015-04-05 10:47:35,217 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 33, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:35,217 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 33)
2015-04-05 10:47:35,220 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 32) in 431 ms on localhost (1/4)
2015-04-05 10:47:35,221 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_1 not found, computing it
2015-04-05 10:47:35,222 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:47:36,776 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_1 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:36,776 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:38,318 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 8.0 (TID 33). 1892 bytes result sent to driver
2015-04-05 10:47:38,320 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 8.0 (TID 34, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:38,321 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 8.0 (TID 34)
2015-04-05 10:47:38,325 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 8.0 (TID 33) in 3105 ms on localhost (2/4)
2015-04-05 10:47:38,327 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_2 not found, computing it
2015-04-05 10:47:38,327 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:47:39,860 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_2 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:39,860 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:41,383 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 8.0 (TID 34). 1729 bytes result sent to driver
2015-04-05 10:47:41,384 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 8.0 (TID 35, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:41,384 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 8.0 (TID 35)
2015-04-05 10:47:41,387 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 8.0 (TID 34) in 3066 ms on localhost (3/4)
2015-04-05 10:47:41,388 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_3 not found, computing it
2015-04-05 10:47:41,388 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:47:42,923 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_3 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:42,923 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:44,107 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 8.0 (TID 35). 1729 bytes result sent to driver
2015-04-05 10:47:44,111 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 8.0 (TID 35) in 2726 ms on localhost (4/4)
2015-04-05 10:47:44,111 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2015-04-05 10:47:44,111 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 8 (count at CooccurrenceAnalysis.scala:37) finished in 9.325 s
2015-04-05 10:47:44,112 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 6 finished: count at CooccurrenceAnalysis.scala:37, took 9.339512 s
2015-04-05 10:47:44,115 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at CooccurrenceAnalysis.scala:39
2015-04-05 10:47:44,116 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 7 (count at CooccurrenceAnalysis.scala:39) with 4 output partitions (allowLocal=false)
2015-04-05 10:47:44,116 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 9(count at CooccurrenceAnalysis.scala:39)
2015-04-05 10:47:44,116 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2015-04-05 10:47:44,120 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 10:47:44,120 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 9 (MappedRDD[15] at values at ItemBasedRecommender.scala:134), which has no missing parents
2015-04-05 10:47:44,122 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3784) called with curMem=708366703, maxMem=1030823608
2015-04-05 10:47:44,122 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 3.7 KB, free 307.5 MB)
2015-04-05 10:47:44,125 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2172) called with curMem=708370487, maxMem=1030823608
2015-04-05 10:47:44,125 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.1 KB, free 307.5 MB)
2015-04-05 10:47:44,126 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on localhost:53004 (size: 2.1 KB, free: 307.7 MB)
2015-04-05 10:47:44,126 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_10_piece0
2015-04-05 10:47:44,127 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:838
2015-04-05 10:47:44,128 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 9 (MappedRDD[15] at values at ItemBasedRecommender.scala:134)
2015-04-05 10:47:44,128 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 4 tasks
2015-04-05 10:47:44,130 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 36, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:44,130 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 36)
2015-04-05 10:47:44,133 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Found block rdd_15_0 locally
2015-04-05 10:47:44,534 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 36). 1680 bytes result sent to driver
2015-04-05 10:47:44,536 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 37, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:44,536 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 37)
2015-04-05 10:47:44,538 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 36) in 407 ms on localhost (1/4)
2015-04-05 10:47:44,540 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_1 not found, computing it
2015-04-05 10:47:44,540 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:47:46,124 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_1 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:46,125 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:47,671 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 37). 1729 bytes result sent to driver
2015-04-05 10:47:47,673 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 9.0 (TID 38, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:47,673 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 9.0 (TID 38)
2015-04-05 10:47:47,678 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 37) in 3139 ms on localhost (2/4)
2015-04-05 10:47:47,679 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_2 not found, computing it
2015-04-05 10:47:47,679 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:47:49,194 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_2 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:49,194 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:50,755 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 9.0 (TID 38). 1892 bytes result sent to driver
2015-04-05 10:47:50,756 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 9.0 (TID 39, localhost, PROCESS_LOCAL, 1361 bytes)
2015-04-05 10:47:50,757 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 9.0 (TID 39)
2015-04-05 10:47:50,759 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 9.0 (TID 38) in 3085 ms on localhost (3/4)
2015-04-05 10:47:50,760 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_3 not found, computing it
2015-04-05 10:47:50,760 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:47:52,291 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_3 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:52,292 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:47:53,523 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 9.0 (TID 39). 1729 bytes result sent to driver
2015-04-05 10:47:53,527 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 9.0 (TID 39) in 2768 ms on localhost (4/4)
2015-04-05 10:47:53,527 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 9 (count at CooccurrenceAnalysis.scala:39) finished in 9.398 s
2015-04-05 10:47:53,527 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2015-04-05 10:47:53,527 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 7 finished: count at CooccurrenceAnalysis.scala:39, took 9.412419 s
2015-04-05 10:47:53,551 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: toArray at CooccurrenceAnalysis.scala:144
2015-04-05 10:47:53,553 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (map at CooccurrenceAnalysis.scala:42)
2015-04-05 10:47:53,554 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 8 (toArray at CooccurrenceAnalysis.scala:144) with 4 output partitions (allowLocal=false)
2015-04-05 10:47:53,554 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 11(toArray at CooccurrenceAnalysis.scala:144)
2015-04-05 10:47:53,554 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 10)
2015-04-05 10:47:53,555 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 10)
2015-04-05 10:47:53,562 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 10 (MappedRDD[18] at map at CooccurrenceAnalysis.scala:42), which has no missing parents
2015-04-05 10:47:53,564 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4584) called with curMem=708372659, maxMem=1030823608
2015-04-05 10:47:53,564 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_11 stored as values in memory (estimated size 4.5 KB, free 307.5 MB)
2015-04-05 10:47:53,567 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2727) called with curMem=708377243, maxMem=1030823608
2015-04-05 10:47:53,568 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.7 KB, free 307.5 MB)
2015-04-05 10:47:53,569 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_11_piece0 in memory on localhost:53004 (size: 2.7 KB, free: 307.7 MB)
2015-04-05 10:47:53,569 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_11_piece0
2015-04-05 10:47:53,570 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.SparkContext - Created broadcast 11 from broadcast at DAGScheduler.scala:838
2015-04-05 10:47:53,571 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 10 (MappedRDD[18] at map at CooccurrenceAnalysis.scala:42)
2015-04-05 10:47:53,571 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 4 tasks
2015-04-05 10:47:53,573 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 40, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:47:53,573 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 40)
2015-04-05 10:47:53,578 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Found block rdd_15_0 locally
2015-04-05 10:47:56,510 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 40). 1874 bytes result sent to driver
2015-04-05 10:47:56,512 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 41, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:47:56,513 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 41)
2015-04-05 10:47:56,516 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 40) in 2940 ms on localhost (1/4)
2015-04-05 10:47:56,517 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_1 not found, computing it
2015-04-05 10:47:56,518 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:47:58,030 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_1 in memory! (computed 218.9 MB so far)
2015-04-05 10:47:58,030 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:48:02,585 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 41). 2086 bytes result sent to driver
2015-04-05 10:48:02,587 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 10.0 (TID 42, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:48:02,587 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 10.0 (TID 42)
2015-04-05 10:48:02,591 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 41) in 6077 ms on localhost (2/4)
2015-04-05 10:48:02,592 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_2 not found, computing it
2015-04-05 10:48:02,592 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:48:04,279 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_2 in memory! (computed 218.9 MB so far)
2015-04-05 10:48:04,279 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:48:08,782 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 10.0 (TID 42). 2086 bytes result sent to driver
2015-04-05 10:48:08,784 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 10.0 (TID 43, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:48:08,784 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 10.0 (TID 43)
2015-04-05 10:48:08,787 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 10.0 (TID 42) in 6198 ms on localhost (3/4)
2015-04-05 10:48:08,790 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_3 not found, computing it
2015-04-05 10:48:08,790 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:48:10,401 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_3 in memory! (computed 218.9 MB so far)
2015-04-05 10:48:10,401 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.6 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:48:14,344 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 10
2015-04-05 10:48:14,344 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_10
2015-04-05 10:48:14,344 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_10 of size 3784 dropped from memory (free 322447422)
2015-04-05 10:48:14,344 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_10_piece0
2015-04-05 10:48:14,345 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_10_piece0 of size 2172 dropped from memory (free 322449594)
2015-04-05 10:48:14,346 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_10_piece0 on localhost:53004 in memory (size: 2.1 KB, free: 307.7 MB)
2015-04-05 10:48:14,346 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_10_piece0
2015-04-05 10:48:14,346 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 10
2015-04-05 10:48:14,347 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 9
2015-04-05 10:48:14,347 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_9
2015-04-05 10:48:14,347 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_9 of size 3784 dropped from memory (free 322453378)
2015-04-05 10:48:14,347 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_9_piece0
2015-04-05 10:48:14,347 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_9_piece0 of size 2172 dropped from memory (free 322455550)
2015-04-05 10:48:14,348 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_9_piece0 on localhost:53004 in memory (size: 2.1 KB, free: 307.7 MB)
2015-04-05 10:48:14,348 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_9_piece0
2015-04-05 10:48:14,349 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 9
2015-04-05 10:48:14,349 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 8
2015-04-05 10:48:14,350 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_8
2015-04-05 10:48:14,350 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_8 of size 3784 dropped from memory (free 322459334)
2015-04-05 10:48:14,350 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_8_piece0
2015-04-05 10:48:14,350 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_8_piece0 of size 2172 dropped from memory (free 322461506)
2015-04-05 10:48:14,351 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on localhost:53004 in memory (size: 2.1 KB, free: 307.7 MB)
2015-04-05 10:48:14,351 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_8_piece0
2015-04-05 10:48:14,351 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 8
2015-04-05 10:48:14,352 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 7
2015-04-05 10:48:14,352 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_7
2015-04-05 10:48:14,352 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_7 of size 3784 dropped from memory (free 322465290)
2015-04-05 10:48:14,352 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_7_piece0
2015-04-05 10:48:14,352 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_7_piece0 of size 2172 dropped from memory (free 322467462)
2015-04-05 10:48:14,353 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on localhost:53004 in memory (size: 2.1 KB, free: 307.7 MB)
2015-04-05 10:48:14,353 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
2015-04-05 10:48:14,354 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 7
2015-04-05 10:48:17,019 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 10.0 (TID 43). 1923 bytes result sent to driver
2015-04-05 10:48:17,024 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 10.0 (TID 43) in 8238 ms on localhost (4/4)
2015-04-05 10:48:17,024 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 10 (map at CooccurrenceAnalysis.scala:42) finished in 23.452 s
2015-04-05 10:48:17,024 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 10:48:17,024 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2015-04-05 10:48:17,024 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 10:48:17,024 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 11)
2015-04-05 10:48:17,024 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 10:48:17,025 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 11: List()
2015-04-05 10:48:17,026 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 11 (ShuffledRDD[19] at reduceByKey at CooccurrenceAnalysis.scala:42), which is now runnable
2015-04-05 10:48:17,027 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2104) called with curMem=708356146, maxMem=1030823608
2015-04-05 10:48:17,027 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_12 stored as values in memory (estimated size 2.1 KB, free 307.5 MB)
2015-04-05 10:48:17,031 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1312) called with curMem=708358250, maxMem=1030823608
2015-04-05 10:48:17,032 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_12_piece0 stored as bytes in memory (estimated size 1312.0 B, free 307.5 MB)
2015-04-05 10:48:17,032 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_12_piece0 in memory on localhost:53004 (size: 1312.0 B, free: 307.7 MB)
2015-04-05 10:48:17,033 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_12_piece0
2015-04-05 10:48:17,033 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.SparkContext - Created broadcast 12 from broadcast at DAGScheduler.scala:838
2015-04-05 10:48:17,034 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 11 (ShuffledRDD[19] at reduceByKey at CooccurrenceAnalysis.scala:42)
2015-04-05 10:48:17,034 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 4 tasks
2015-04-05 10:48:17,035 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 44, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:48:17,035 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 44)
2015-04-05 10:48:17,037 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:48:17,037 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:48:17,063 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 44). 12519 bytes result sent to driver
2015-04-05 10:48:17,064 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 45, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:48:17,064 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 45)
2015-04-05 10:48:17,066 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:48:17,066 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:48:17,067 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 44) in 29 ms on localhost (1/4)
2015-04-05 10:48:17,102 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 45). 12527 bytes result sent to driver
2015-04-05 10:48:17,103 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 11.0 (TID 46, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:48:17,103 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 11.0 (TID 46)
2015-04-05 10:48:17,106 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:48:17,106 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 10:48:17,106 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 45) in 40 ms on localhost (2/4)
2015-04-05 10:48:17,134 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 11.0 (TID 46). 12527 bytes result sent to driver
2015-04-05 10:48:17,136 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 11.0 (TID 47, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 10:48:17,136 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 11.0 (TID 47)
2015-04-05 10:48:17,140 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:48:17,140 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 11.0 (TID 46) in 34 ms on localhost (3/4)
2015-04-05 10:48:17,140 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:48:17,169 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 11.0 (TID 47). 12527 bytes result sent to driver
2015-04-05 10:48:17,173 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 11.0 (TID 47) in 35 ms on localhost (4/4)
2015-04-05 10:48:17,173 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 11 (toArray at CooccurrenceAnalysis.scala:144) finished in 0.139 s
2015-04-05 10:48:17,173 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2015-04-05 10:48:17,174 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 8 finished: toArray at CooccurrenceAnalysis.scala:144, took 23.622063 s
2015-04-05 10:48:17,238 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(636336) called with curMem=708359562, maxMem=1030823608
2015-04-05 10:48:17,238 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_13 stored as values in memory (estimated size 621.4 KB, free 306.9 MB)
2015-04-05 10:48:17,263 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(25207) called with curMem=708995898, maxMem=1030823608
2015-04-05 10:48:17,264 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_13_piece0 stored as bytes in memory (estimated size 24.6 KB, free 306.9 MB)
2015-04-05 10:48:17,265 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_13_piece0 in memory on localhost:53004 (size: 24.6 KB, free: 307.6 MB)
2015-04-05 10:48:17,266 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_13_piece0
2015-04-05 10:48:17,267 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 13 from broadcast at CooccurrenceAnalysis.scala:44
2015-04-05 10:48:17,438 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: first at ItemBasedRecommender.scala:144
2015-04-05 10:48:17,440 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 20 (groupBy at CooccurrenceAnalysis.scala:48)
2015-04-05 10:48:17,440 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (flatMap at CooccurrenceAnalysis.scala:48)
2015-04-05 10:48:17,441 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 25 (flatMap at CooccurrenceAnalysis.scala:69)
2015-04-05 10:48:17,441 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 28 (groupBy at ItemBasedRecommender.scala:143)
2015-04-05 10:48:17,442 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 9 (first at ItemBasedRecommender.scala:144) with 1 output partitions (allowLocal=true)
2015-04-05 10:48:17,442 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 16(first at ItemBasedRecommender.scala:144)
2015-04-05 10:48:17,442 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 15)
2015-04-05 10:48:17,443 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 15)
2015-04-05 10:48:17,454 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 12 (MappedRDD[20] at groupBy at CooccurrenceAnalysis.scala:48), which has no missing parents
2015-04-05 10:48:17,456 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4728) called with curMem=709021105, maxMem=1030823608
2015-04-05 10:48:17,457 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_14 stored as values in memory (estimated size 4.6 KB, free 306.9 MB)
2015-04-05 10:48:17,460 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2739) called with curMem=709025833, maxMem=1030823608
2015-04-05 10:48:17,461 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.7 KB, free 306.9 MB)
2015-04-05 10:48:17,462 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_14_piece0 in memory on localhost:53004 (size: 2.7 KB, free: 307.6 MB)
2015-04-05 10:48:17,462 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_14_piece0
2015-04-05 10:48:17,463 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.SparkContext - Created broadcast 14 from broadcast at DAGScheduler.scala:838
2015-04-05 10:48:17,465 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 12 (MappedRDD[20] at groupBy at CooccurrenceAnalysis.scala:48)
2015-04-05 10:48:17,465 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 12.0 with 4 tasks
2015-04-05 10:48:17,467 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 12.0 (TID 48, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:48:17,467 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 12.0 (TID 48)
2015-04-05 10:48:17,471 [Executor task launch worker-0] INFO  org.apache.spark.storage.BlockManager - Found block rdd_15_0 locally
2015-04-05 10:48:17,962 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 13
2015-04-05 10:48:17,962 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_13_piece0
2015-04-05 10:48:17,962 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_13_piece0 of size 25207 dropped from memory (free 321820243)
2015-04-05 10:48:17,963 [sparkDriver-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_13_piece0 on localhost:53004 in memory (size: 24.6 KB, free: 307.7 MB)
2015-04-05 10:48:17,964 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_13_piece0
2015-04-05 10:48:17,964 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_13
2015-04-05 10:48:17,964 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_13 of size 636336 dropped from memory (free 322456579)
2015-04-05 10:48:17,965 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 13
2015-04-05 10:48:17,966 [sparkDriver-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 12
2015-04-05 10:48:17,966 [sparkDriver-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_12
2015-04-05 10:48:17,967 [sparkDriver-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_12 of size 2104 dropped from memory (free 322458683)
2015-04-05 10:48:17,967 [sparkDriver-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_12_piece0
2015-04-05 10:48:17,967 [sparkDriver-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_12_piece0 of size 1312 dropped from memory (free 322459995)
2015-04-05 10:48:17,968 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_12_piece0 on localhost:53004 in memory (size: 1312.0 B, free: 307.7 MB)
2015-04-05 10:48:17,969 [sparkDriver-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_12_piece0
2015-04-05 10:48:17,969 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 12
2015-04-05 10:48:38,011 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 12.0 (TID 48). 2037 bytes result sent to driver
2015-04-05 10:48:38,012 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 12.0 (TID 49, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:48:38,013 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 12.0 (TID 49)
2015-04-05 10:48:38,016 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 12.0 (TID 48) in 20547 ms on localhost (1/4)
2015-04-05 10:48:38,017 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_1 not found, computing it
2015-04-05 10:48:38,017 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:33554432+33554432
2015-04-05 10:48:39,692 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_1 in memory! (computed 218.9 MB so far)
2015-04-05 10:48:39,692 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:49:01,780 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 12.0 (TID 49). 2086 bytes result sent to driver
2015-04-05 10:49:01,781 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 12.0 (TID 50, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:49:01,782 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 12.0 (TID 50)
2015-04-05 10:49:01,785 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 12.0 (TID 49) in 23770 ms on localhost (2/4)
2015-04-05 10:49:01,787 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_2 not found, computing it
2015-04-05 10:49:01,787 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:67108864+33554432
2015-04-05 10:49:03,323 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_2 in memory! (computed 218.9 MB so far)
2015-04-05 10:49:03,323 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.3 MB. Storage limit = 983.1 MB.
2015-04-05 10:49:06,562 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 11
2015-04-05 10:49:06,563 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_11
2015-04-05 10:49:06,563 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_11 of size 4584 dropped from memory (free 322464579)
2015-04-05 10:49:06,563 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_11_piece0
2015-04-05 10:49:06,563 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_11_piece0 of size 2727 dropped from memory (free 322467306)
2015-04-05 10:49:06,564 [sparkDriver-akka.actor.default-dispatcher-24] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_11_piece0 on localhost:53004 in memory (size: 2.7 KB, free: 307.7 MB)
2015-04-05 10:49:06,565 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_11_piece0
2015-04-05 10:49:06,565 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 11
2015-04-05 10:49:06,566 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned shuffle 2
2015-04-05 10:49:26,879 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 12.0 (TID 50). 2086 bytes result sent to driver
2015-04-05 10:49:26,880 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 12.0 (TID 51, localhost, PROCESS_LOCAL, 1350 bytes)
2015-04-05 10:49:26,881 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 12.0 (TID 51)
2015-04-05 10:49:26,884 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 12.0 (TID 50) in 25100 ms on localhost (3/4)
2015-04-05 10:49:26,886 [Executor task launch worker-0] INFO  org.apache.spark.CacheManager - Partition rdd_15_3 not found, computing it
2015-04-05 10:49:26,886 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/URM_sample_10000.csv:100663296+29223420
2015-04-05 10:49:28,555 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_15_3 in memory! (computed 218.9 MB so far)
2015-04-05 10:49:28,555 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 675.5 MB (blocks) + 218.7 MB (scratch space shared across 1 thread(s)) = 894.2 MB. Storage limit = 983.1 MB.
2015-04-05 10:49:47,436 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 12.0 (TID 51). 2086 bytes result sent to driver
2015-04-05 10:49:47,440 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 12.0 (TID 51) in 20557 ms on localhost (4/4)
2015-04-05 10:49:47,440 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 12 (groupBy at CooccurrenceAnalysis.scala:48) finished in 89.974 s
2015-04-05 10:49:47,440 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 10:49:47,440 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2015-04-05 10:49:47,440 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 10:49:47,441 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 15, Stage 16, Stage 13, Stage 14)
2015-04-05 10:49:47,441 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 10:49:47,442 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 15: List(Stage 14)
2015-04-05 10:49:47,443 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 16: List(Stage 15)
2015-04-05 10:49:47,444 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 13: List()
2015-04-05 10:49:47,445 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 14: List(Stage 13)
2015-04-05 10:49:47,446 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 13 (FlatMappedRDD[22] at flatMap at CooccurrenceAnalysis.scala:48), which is now runnable
2015-04-05 10:49:47,447 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2504) called with curMem=708356302, maxMem=1030823608
2015-04-05 10:49:47,447 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_15 stored as values in memory (estimated size 2.4 KB, free 307.5 MB)
2015-04-05 10:49:47,450 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1466) called with curMem=708358806, maxMem=1030823608
2015-04-05 10:49:47,450 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_15_piece0 stored as bytes in memory (estimated size 1466.0 B, free 307.5 MB)
2015-04-05 10:49:47,451 [sparkDriver-akka.actor.default-dispatcher-24] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_15_piece0 in memory on localhost:53004 (size: 1466.0 B, free: 307.7 MB)
2015-04-05 10:49:47,451 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_15_piece0
2015-04-05 10:49:47,452 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 15 from broadcast at DAGScheduler.scala:838
2015-04-05 10:49:47,452 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from Stage 13 (FlatMappedRDD[22] at flatMap at CooccurrenceAnalysis.scala:48)
2015-04-05 10:49:47,453 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 13.0 with 4 tasks
2015-04-05 10:49:47,454 [sparkDriver-akka.actor.default-dispatcher-24] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 13.0 (TID 52, localhost, PROCESS_LOCAL, 1045 bytes)
2015-04-05 10:49:47,454 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 13.0 (TID 52)
2015-04-05 10:49:47,456 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:49:47,456 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 10:49:56,964 [Executor task launch worker-0] INFO  org.apache.spark.util.collection.ExternalAppendOnlyMap - Thread 63 spilling in-memory map of 329.3 MB to disk (1 time so far)
2015-04-05 10:50:09,294 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 14
2015-04-05 10:50:09,294 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_14_piece0
2015-04-05 10:50:09,294 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_14_piece0 of size 2739 dropped from memory (free 322466075)
2015-04-05 10:50:09,295 [sparkDriver-akka.actor.default-dispatcher-24] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_14_piece0 on localhost:53004 in memory (size: 2.7 KB, free: 307.7 MB)
2015-04-05 10:50:09,295 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_14_piece0
2015-04-05 10:50:09,295 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_14
2015-04-05 10:50:09,295 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_14 of size 4728 dropped from memory (free 322470803)
2015-04-05 10:50:09,296 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 14
2015-04-05 10:54:52,338 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 13.0 (TID 52)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:168)
	at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:176)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:113)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:28)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at org.apache.spark.util.collection.CompactBuffer.flatMap(CompactBuffer.scala:28)
	at ml.CooccurrenceAnalysis$$anonfun$7.apply(CooccurrenceAnalysis.scala:49)
	at ml.CooccurrenceAnalysis$$anonfun$7.apply(CooccurrenceAnalysis.scala:48)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-04-05 10:54:52,340 [sparkDriver-akka.actor.default-dispatcher-18] ERROR akka.actor.ActorSystemImpl - exception on LARS timer thread
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:409)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
	at java.lang.Thread.run(Thread.java:745)
2015-04-05 10:54:52,345 [sparkDriver-akka.actor.default-dispatcher-18] INFO  akka.actor.ActorSystemImpl - starting new LARS thread
2015-04-05 10:54:52,346 [sparkDriver-akka.actor.default-dispatcher-18] ERROR akka.actor.ActorSystemImpl - Uncaught fatal error from thread [sparkDriver-scheduler-1] shutting down ActorSystem [sparkDriver]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:409)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
	at java.lang.Thread.run(Thread.java:745)
2015-04-05 10:54:52,338 [Driver Heartbeater] WARN  org.apache.spark.util.AkkaUtils - Error sending message in 1 attempts
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.util.AkkaUtils$.askWithReply(AkkaUtils.scala:187)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:398)
2015-04-05 10:54:52,371 [Executor task launch worker-0] ERROR org.apache.spark.util.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:168)
	at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:176)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:113)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:28)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at org.apache.spark.util.collection.CompactBuffer.flatMap(CompactBuffer.scala:28)
	at ml.CooccurrenceAnalysis$$anonfun$7.apply(CooccurrenceAnalysis.scala:49)
	at ml.CooccurrenceAnalysis$$anonfun$7.apply(CooccurrenceAnalysis.scala:48)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-04-05 10:54:52,374 [sparkDriver-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 13.0 (TID 53, localhost, PROCESS_LOCAL, 1045 bytes)
2015-04-05 10:54:52,379 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 13.0 (TID 52, localhost): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:168)
	at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:48)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:176)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:113)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:28)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at org.apache.spark.util.collection.CompactBuffer.flatMap(CompactBuffer.scala:28)
	at ml.CooccurrenceAnalysis$$anonfun$7.apply(CooccurrenceAnalysis.scala:49)
	at ml.CooccurrenceAnalysis$$anonfun$7.apply(CooccurrenceAnalysis.scala:48)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:58)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

2015-04-05 10:54:52,383 [task-result-getter-0] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 13.0 failed 1 times; aborting job
2015-04-05 10:54:52,395 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 9 failed: first at ItemBasedRecommender.scala:144, took 394.955810 s
2015-04-05 10:54:52,417 [Executor task launch worker-1] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 13.0 (TID 53)
2015-04-05 10:54:52,439 [sparkDriver-akka.actor.default-dispatcher-32] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
2015-04-05 10:54:52,439 [sparkDriver-akka.actor.default-dispatcher-32] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
2015-04-05 10:54:52,502 [Executor task launch worker-1] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 4 non-empty blocks out of 4 blocks
2015-04-05 10:54:52,503 [Executor task launch worker-1] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 10:54:52,506 [Executor task launch worker-1] ERROR org.apache.spark.storage.ShuffleBlockFetcherIterator - Error occurred while fetching local blocks
java.io.FileNotFoundException: /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150405104557-83d0/0a/shuffle_6_0_0.index (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.spark.shuffle.IndexShuffleBlockManager.getBlockData(IndexShuffleBlockManager.scala:109)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:305)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:235)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:268)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:115)
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:76)
	at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
2015-04-05 10:54:52,507 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2015-04-05 10:54:52,507 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2015-04-05 10:54:52,507 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2015-04-05 10:54:52,507 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
2015-04-05 10:54:52,508 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
2015-04-05 10:54:52,508 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2015-04-05 10:54:52,508 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2015-04-05 10:54:52,508 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2015-04-05 10:54:52,508 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2015-04-05 10:54:52,509 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2015-04-05 10:54:52,509 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2015-04-05 10:54:52,509 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2015-04-05 10:54:52,509 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2015-04-05 10:54:52,509 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2015-04-05 10:54:52,510 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2015-04-05 10:54:52,510 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2015-04-05 10:54:52,510 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2015-04-05 10:54:52,510 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2015-04-05 10:54:52,510 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2015-04-05 10:54:52,510 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
2015-04-05 10:54:52,511 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
2015-04-05 10:54:52,511 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
2015-04-05 10:54:52,511 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs,null}
2015-04-05 10:54:52,523 [sparkDriver-akka.actor.default-dispatcher-32] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Remoting shut down.
2015-04-05 12:00:10,851 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-05 12:00:11,175 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-05 12:00:11,176 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-05 12:00:11,176 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-05 12:00:11,558 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-05 12:00:11,638 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-04-05 12:00:11,871 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:53534]
2015-04-05 12:00:11,879 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53534.
2015-04-05 12:00:11,911 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-05 12:00:11,950 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-05 12:00:11,974 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150405120011-021a
2015-04-05 12:00:11,980 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-05 12:00:12,297 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-05 12:00:12,430 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-9aaa7e57-8816-47dc-8176-f1091c72a66e
2015-04-05 12:00:12,441 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-05 12:00:12,603 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:00:12,623 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:53535
2015-04-05 12:00:12,623 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 53535.
2015-04-05 12:00:12,765 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:00:12,778 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-05 12:00:12,778 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-05 12:00:12,780 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-05 12:00:12,904 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:53534/user/HeartbeatReceiver
2015-04-05 12:00:13,086 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 53536
2015-04-05 12:00:13,088 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-05 12:00:13,089 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:53536 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 53536)
2015-04-05 12:00:13,092 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-05 12:00:13,583 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-05 12:00:13,586 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-05 12:00:13,785 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-05 12:00:13,786 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-05 12:00:13,788 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:53536 (size: 13.6 KB, free: 983.1 MB)
2015-04-05 12:00:13,789 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-05 12:00:13,795 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-05 12:00:14,087 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-05 12:00:14,128 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemRecSpec.scala:15
2015-04-05 12:00:14,149 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (distinct at ItemRecSpec.scala:15)
2015-04-05 12:00:14,153 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemRecSpec.scala:15) with 1 output partitions (allowLocal=false)
2015-04-05 12:00:14,154 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 1(count at ItemRecSpec.scala:15)
2015-04-05 12:00:14,154 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 0)
2015-04-05 12:00:14,160 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 0)
2015-04-05 12:00:14,173 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:15), which has no missing parents
2015-04-05 12:00:14,218 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4632) called with curMem=152562, maxMem=1030823608
2015-04-05 12:00:14,219 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 982.9 MB)
2015-04-05 12:00:14,228 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2741) called with curMem=157194, maxMem=1030823608
2015-04-05 12:00:14,228 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 982.9 MB)
2015-04-05 12:00:14,230 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:53536 (size: 2.7 KB, free: 983.1 MB)
2015-04-05 12:00:14,230 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 12:00:14,231 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-05 12:00:14,240 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:15)
2015-04-05 12:00:14,241 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-04-05 12:00:14,265 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1346 bytes)
2015-04-05 12:00:14,272 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-05 12:00:14,309 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-05 12:00:14,318 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-05 12:00:14,318 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-05 12:00:14,318 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-05 12:00:14,318 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-05 12:00:14,318 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-05 12:00:15,260 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1871 bytes result sent to driver
2015-04-05 12:00:15,278 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1006 ms on localhost (1/1)
2015-04-05 12:00:15,279 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-05 12:00:15,280 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (distinct at ItemRecSpec.scala:15) finished in 1.029 s
2015-04-05 12:00:15,281 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:00:15,281 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:00:15,281 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 1)
2015-04-05 12:00:15,282 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:00:15,285 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 1: List()
2015-04-05 12:00:15,288 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:15), which is now runnable
2015-04-05 12:00:15,292 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=159935, maxMem=1030823608
2015-04-05 12:00:15,293 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 12:00:15,299 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1545) called with curMem=162471, maxMem=1030823608
2015-04-05 12:00:15,300 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1545.0 B, free 982.9 MB)
2015-04-05 12:00:15,301 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:53536 (size: 1545.0 B, free: 983.1 MB)
2015-04-05 12:00:15,301 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 12:00:15,303 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-04-05 12:00:15,306 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:15)
2015-04-05 12:00:15,306 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-04-05 12:00:15,308 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:00:15,309 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-04-05 12:00:15,327 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:00:15,329 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 6 ms
2015-04-05 12:00:15,404 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 783 bytes result sent to driver
2015-04-05 12:00:15,411 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 99 ms on localhost (1/1)
2015-04-05 12:00:15,411 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (count at ItemRecSpec.scala:15) finished in 0.103 s
2015-04-05 12:00:15,411 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-04-05 12:00:15,417 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemRecSpec.scala:15, took 1.288313 s
2015-04-05 12:00:15,490 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: foreach at ItemRecSpec.scala:17
2015-04-05 12:00:15,491 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 12 (map at Strategy.scala:15)
2015-04-05 12:00:15,492 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreach at ItemRecSpec.scala:17) with 1 output partitions (allowLocal=false)
2015-04-05 12:00:15,492 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 3(foreach at ItemRecSpec.scala:17)
2015-04-05 12:00:15,492 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 2)
2015-04-05 12:00:15,494 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 2)
2015-04-05 12:00:15,498 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[12] at map at Strategy.scala:15), which has no missing parents
2015-04-05 12:00:15,501 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4568) called with curMem=164016, maxMem=1030823608
2015-04-05 12:00:15,502 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 982.9 MB)
2015-04-05 12:00:15,507 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2663) called with curMem=168584, maxMem=1030823608
2015-04-05 12:00:15,508 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 982.9 MB)
2015-04-05 12:00:15,509 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:53536 (size: 2.6 KB, free: 983.1 MB)
2015-04-05 12:00:15,509 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-04-05 12:00:15,510 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-04-05 12:00:15,512 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 2 (MappedRDD[12] at map at Strategy.scala:15)
2015-04-05 12:00:15,512 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2015-04-05 12:00:15,513 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1346 bytes)
2015-04-05 12:00:15,514 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2015-04-05 12:00:15,521 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-05 12:00:15,740 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 2
2015-04-05 12:00:15,742 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_2
2015-04-05 12:00:15,743 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 of size 2536 dropped from memory (free 1030654897)
2015-04-05 12:00:15,743 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
2015-04-05 12:00:15,743 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 of size 1545 dropped from memory (free 1030656442)
2015-04-05 12:00:15,745 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:53536 in memory (size: 1545.0 B, free: 983.1 MB)
2015-04-05 12:00:15,745 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 12:00:15,748 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 2
2015-04-05 12:00:15,749 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 1
2015-04-05 12:00:15,750 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
2015-04-05 12:00:15,750 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 of size 2741 dropped from memory (free 1030659183)
2015-04-05 12:00:15,751 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:53536 in memory (size: 2.7 KB, free: 983.1 MB)
2015-04-05 12:00:15,751 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 12:00:15,752 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1
2015-04-05 12:00:15,752 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 of size 4632 dropped from memory (free 1030663815)
2015-04-05 12:00:15,752 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 1
2015-04-05 12:00:15,757 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned shuffle 0
2015-04-05 12:01:02,677 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2034 bytes result sent to driver
2015-04-05 12:01:02,686 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 47167 ms on localhost (1/1)
2015-04-05 12:01:02,687 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (map at Strategy.scala:15) finished in 47.174 s
2015-04-05 12:01:02,687 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:01:02,687 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-04-05 12:01:02,687 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:01:02,687 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 3)
2015-04-05 12:01:02,687 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:01:02,689 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 3: List()
2015-04-05 12:01:02,689 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 3 (FlatMappedRDD[15] at flatMap at Strategy.scala:23), which is now runnable
2015-04-05 12:01:02,691 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2800) called with curMem=159793, maxMem=1030823608
2015-04-05 12:01:02,691 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 2.7 KB, free 982.9 MB)
2015-04-05 12:01:02,699 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1678) called with curMem=162593, maxMem=1030823608
2015-04-05 12:01:02,700 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1678.0 B, free 982.9 MB)
2015-04-05 12:01:02,701 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:53536 (size: 1678.0 B, free: 983.1 MB)
2015-04-05 12:01:02,702 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
2015-04-05 12:01:02,703 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:838
2015-04-05 12:01:02,703 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 3 (FlatMappedRDD[15] at flatMap at Strategy.scala:23)
2015-04-05 12:01:02,704 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
2015-04-05 12:01:02,705 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:01:02,705 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
2015-04-05 12:01:02,712 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:01:02,712 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 12:01:07,096 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 782 bytes result sent to driver
2015-04-05 12:01:07,104 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 4394 ms on localhost (1/1)
2015-04-05 12:01:07,104 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2015-04-05 12:01:07,106 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 3 (foreach at ItemRecSpec.scala:17) finished in 4.401 s
2015-04-05 12:01:07,106 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: foreach at ItemRecSpec.scala:17, took 51.616293 s
2015-04-05 12:01:07,129 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2015-04-05 12:01:07,129 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2015-04-05 12:01:07,130 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2015-04-05 12:01:07,130 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
2015-04-05 12:01:07,130 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
2015-04-05 12:01:07,130 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2015-04-05 12:01:07,130 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2015-04-05 12:01:07,131 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2015-04-05 12:01:07,131 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2015-04-05 12:01:07,131 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2015-04-05 12:01:07,131 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2015-04-05 12:01:07,131 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2015-04-05 12:01:07,131 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2015-04-05 12:01:07,132 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2015-04-05 12:01:07,132 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2015-04-05 12:01:07,132 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2015-04-05 12:01:07,132 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2015-04-05 12:01:07,132 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2015-04-05 12:01:07,132 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2015-04-05 12:01:07,133 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
2015-04-05 12:01:07,133 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
2015-04-05 12:01:07,133 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
2015-04-05 12:01:07,133 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs,null}
2015-04-05 12:01:07,187 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.0.104:4040
2015-04-05 12:01:07,188 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Stopping DAGScheduler
2015-04-05 12:01:08,247 [sparkDriver-akka.actor.default-dispatcher-16] INFO  org.apache.spark.MapOutputTrackerMasterActor - MapOutputTrackerActor stopped!
2015-04-05 12:01:08,263 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore cleared
2015-04-05 12:01:08,264 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
2015-04-05 12:01:08,264 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2015-04-05 12:01:08,267 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2015-04-05 12:01:08,273 [sparkDriver-akka.actor.default-dispatcher-15] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
2015-04-05 12:01:08,275 [sparkDriver-akka.actor.default-dispatcher-15] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
2015-04-05 12:02:50,932 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-05 12:02:51,205 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-05 12:02:51,206 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-05 12:02:51,207 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-05 12:02:51,627 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-05 12:02:51,702 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-04-05 12:02:51,930 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:53603]
2015-04-05 12:02:51,937 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53603.
2015-04-05 12:02:51,970 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-05 12:02:52,003 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-05 12:02:52,023 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150405120252-faf3
2015-04-05 12:02:52,029 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-05 12:02:52,352 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-05 12:02:52,502 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-3a35b7e9-d13f-4174-b7c4-d69e6ab67423
2015-04-05 12:02:52,510 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-05 12:02:52,679 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:02:52,698 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:53604
2015-04-05 12:02:52,698 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 53604.
2015-04-05 12:02:52,840 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:02:52,852 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-05 12:02:52,852 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-05 12:02:52,855 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-05 12:02:52,988 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:53603/user/HeartbeatReceiver
2015-04-05 12:02:53,169 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 53605
2015-04-05 12:02:53,171 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-05 12:02:53,173 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:53605 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 53605)
2015-04-05 12:02:53,176 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-05 12:02:53,684 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-05 12:02:53,686 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-05 12:02:53,877 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-05 12:02:53,878 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-05 12:02:53,881 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:53605 (size: 13.6 KB, free: 983.1 MB)
2015-04-05 12:02:53,881 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-05 12:02:53,886 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-05 12:02:54,164 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-05 12:02:54,201 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemRecSpec.scala:15
2015-04-05 12:02:54,221 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (distinct at ItemRecSpec.scala:15)
2015-04-05 12:02:54,225 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemRecSpec.scala:15) with 1 output partitions (allowLocal=false)
2015-04-05 12:02:54,225 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 1(count at ItemRecSpec.scala:15)
2015-04-05 12:02:54,226 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 0)
2015-04-05 12:02:54,230 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 0)
2015-04-05 12:02:54,245 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:15), which has no missing parents
2015-04-05 12:02:54,289 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4632) called with curMem=152562, maxMem=1030823608
2015-04-05 12:02:54,290 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 982.9 MB)
2015-04-05 12:02:54,299 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2743) called with curMem=157194, maxMem=1030823608
2015-04-05 12:02:54,300 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 982.9 MB)
2015-04-05 12:02:54,301 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:53605 (size: 2.7 KB, free: 983.1 MB)
2015-04-05 12:02:54,302 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 12:02:54,303 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-05 12:02:54,312 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:15)
2015-04-05 12:02:54,313 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-04-05 12:02:54,338 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1346 bytes)
2015-04-05 12:02:54,344 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-05 12:02:54,378 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-05 12:02:54,386 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-05 12:02:54,386 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-05 12:02:54,386 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-05 12:02:54,386 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-05 12:02:54,386 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-05 12:02:55,275 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1871 bytes result sent to driver
2015-04-05 12:02:55,293 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 952 ms on localhost (1/1)
2015-04-05 12:02:55,294 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-05 12:02:55,295 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (distinct at ItemRecSpec.scala:15) finished in 0.974 s
2015-04-05 12:02:55,296 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:02:55,296 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:02:55,297 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 1)
2015-04-05 12:02:55,297 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:02:55,302 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 1: List()
2015-04-05 12:02:55,304 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:15), which is now runnable
2015-04-05 12:02:55,309 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=159937, maxMem=1030823608
2015-04-05 12:02:55,309 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 12:02:55,317 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1545) called with curMem=162473, maxMem=1030823608
2015-04-05 12:02:55,318 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1545.0 B, free 982.9 MB)
2015-04-05 12:02:55,319 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:53605 (size: 1545.0 B, free: 983.1 MB)
2015-04-05 12:02:55,319 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 12:02:55,320 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-04-05 12:02:55,323 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:15)
2015-04-05 12:02:55,323 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-04-05 12:02:55,325 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:02:55,325 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-04-05 12:02:55,350 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:02:55,352 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 8 ms
2015-04-05 12:02:55,421 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 783 bytes result sent to driver
2015-04-05 12:02:55,427 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 99 ms on localhost (1/1)
2015-04-05 12:02:55,427 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (count at ItemRecSpec.scala:15) finished in 0.104 s
2015-04-05 12:02:55,427 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-04-05 12:02:55,437 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemRecSpec.scala:15, took 1.235672 s
2015-04-05 12:02:55,501 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: foreach at ItemRecSpec.scala:17
2015-04-05 12:02:55,503 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 12 (map at Strategy.scala:15)
2015-04-05 12:02:55,504 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreach at ItemRecSpec.scala:17) with 1 output partitions (allowLocal=false)
2015-04-05 12:02:55,504 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 3(foreach at ItemRecSpec.scala:17)
2015-04-05 12:02:55,504 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 2)
2015-04-05 12:02:55,506 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 2)
2015-04-05 12:02:55,510 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[12] at map at Strategy.scala:15), which has no missing parents
2015-04-05 12:02:55,514 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4568) called with curMem=164018, maxMem=1030823608
2015-04-05 12:02:55,515 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 982.9 MB)
2015-04-05 12:02:55,522 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2665) called with curMem=168586, maxMem=1030823608
2015-04-05 12:02:55,522 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 982.9 MB)
2015-04-05 12:02:55,523 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:53605 (size: 2.6 KB, free: 983.1 MB)
2015-04-05 12:02:55,523 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-04-05 12:02:55,524 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-04-05 12:02:55,525 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 2 (MappedRDD[12] at map at Strategy.scala:15)
2015-04-05 12:02:55,525 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2015-04-05 12:02:55,527 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1346 bytes)
2015-04-05 12:02:55,528 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2015-04-05 12:02:55,536 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-05 12:02:55,764 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 2
2015-04-05 12:02:55,766 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_2
2015-04-05 12:02:55,767 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 of size 2536 dropped from memory (free 1030654893)
2015-04-05 12:02:55,767 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
2015-04-05 12:02:55,767 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 of size 1545 dropped from memory (free 1030656438)
2015-04-05 12:02:55,769 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on localhost:53605 in memory (size: 1545.0 B, free: 983.1 MB)
2015-04-05 12:02:55,769 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 12:02:55,772 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 2
2015-04-05 12:02:55,773 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 1
2015-04-05 12:02:55,773 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
2015-04-05 12:02:55,773 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 of size 2743 dropped from memory (free 1030659181)
2015-04-05 12:02:55,774 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on localhost:53605 in memory (size: 2.7 KB, free: 983.1 MB)
2015-04-05 12:02:55,774 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 12:02:55,774 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_1
2015-04-05 12:02:55,774 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 of size 4632 dropped from memory (free 1030663813)
2015-04-05 12:02:55,775 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 1
2015-04-05 12:02:55,780 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned shuffle 0
2015-04-05 12:03:38,646 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 2034 bytes result sent to driver
2015-04-05 12:03:38,656 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 43123 ms on localhost (1/1)
2015-04-05 12:03:38,657 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-04-05 12:03:38,657 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (map at Strategy.scala:15) finished in 43.130 s
2015-04-05 12:03:38,657 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:03:38,657 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:03:38,657 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 3)
2015-04-05 12:03:38,657 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:03:38,660 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 3: List()
2015-04-05 12:03:38,660 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 3 (MappedRDD[16] at keyBy at ItemRecSpec.scala:17), which is now runnable
2015-04-05 12:03:38,662 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2992) called with curMem=159795, maxMem=1030823608
2015-04-05 12:03:38,662 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 2.9 KB, free 982.9 MB)
2015-04-05 12:03:38,669 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1746) called with curMem=162787, maxMem=1030823608
2015-04-05 12:03:38,670 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1746.0 B, free 982.9 MB)
2015-04-05 12:03:38,671 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:53605 (size: 1746.0 B, free: 983.1 MB)
2015-04-05 12:03:38,671 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
2015-04-05 12:03:38,672 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:838
2015-04-05 12:03:38,673 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 3 (MappedRDD[16] at keyBy at ItemRecSpec.scala:17)
2015-04-05 12:03:38,673 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
2015-04-05 12:03:38,674 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:03:38,674 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
2015-04-05 12:03:38,679 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:03:38,679 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 12:03:42,112 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing broadcast 3
2015-04-05 12:03:42,112 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_3_piece0
2015-04-05 12:03:42,112 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 of size 2665 dropped from memory (free 1030661740)
2015-04-05 12:03:42,113 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on localhost:53605 in memory (size: 2.6 KB, free: 983.1 MB)
2015-04-05 12:03:42,114 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-04-05 12:03:42,114 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManager - Removing block broadcast_3
2015-04-05 12:03:42,114 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 of size 4568 dropped from memory (free 1030666308)
2015-04-05 12:03:42,114 [Spark Context Cleaner] INFO  org.apache.spark.ContextCleaner - Cleaned broadcast 3
2015-04-05 12:03:42,900 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 782 bytes result sent to driver
2015-04-05 12:03:42,907 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 4227 ms on localhost (1/1)
2015-04-05 12:03:42,907 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 3 (foreach at ItemRecSpec.scala:17) finished in 4.233 s
2015-04-05 12:03:42,907 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2015-04-05 12:03:42,907 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: foreach at ItemRecSpec.scala:17, took 47.405752 s
2015-04-05 12:03:42,931 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2015-04-05 12:03:42,931 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2015-04-05 12:03:42,931 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2015-04-05 12:03:42,931 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
2015-04-05 12:03:42,931 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
2015-04-05 12:03:42,931 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2015-04-05 12:03:42,932 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2015-04-05 12:03:42,932 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2015-04-05 12:03:42,932 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2015-04-05 12:03:42,932 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2015-04-05 12:03:42,932 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2015-04-05 12:03:42,932 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2015-04-05 12:03:42,932 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2015-04-05 12:03:42,933 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2015-04-05 12:03:42,933 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2015-04-05 12:03:42,933 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2015-04-05 12:03:42,933 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2015-04-05 12:03:42,934 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2015-04-05 12:03:42,934 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2015-04-05 12:03:42,934 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
2015-04-05 12:03:42,934 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
2015-04-05 12:03:42,934 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
2015-04-05 12:03:42,935 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs,null}
2015-04-05 12:03:42,988 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.0.104:4040
2015-04-05 12:03:42,990 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Stopping DAGScheduler
2015-04-05 12:03:44,049 [sparkDriver-akka.actor.default-dispatcher-15] INFO  org.apache.spark.MapOutputTrackerMasterActor - MapOutputTrackerActor stopped!
2015-04-05 12:03:44,058 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore cleared
2015-04-05 12:03:44,059 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
2015-04-05 12:03:44,060 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2015-04-05 12:03:44,062 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2015-04-05 12:03:44,069 [sparkDriver-akka.actor.default-dispatcher-15] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
2015-04-05 12:03:44,073 [sparkDriver-akka.actor.default-dispatcher-15] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
2015-04-05 12:19:00,525 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-05 12:19:00,838 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-05 12:19:00,839 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-05 12:19:00,839 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-05 12:19:01,198 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-05 12:19:01,273 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Starting remoting
2015-04-05 12:19:01,489 [sparkDriver-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:53706]
2015-04-05 12:19:01,497 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53706.
2015-04-05 12:19:01,527 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-05 12:19:01,562 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-05 12:19:01,585 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150405121901-b222
2015-04-05 12:19:01,590 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-05 12:19:01,913 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-05 12:19:02,048 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-46dd0882-93a1-4d94-8910-783e0a90e5b4
2015-04-05 12:19:02,060 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-05 12:19:02,220 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:19:02,242 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:53707
2015-04-05 12:19:02,242 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 53707.
2015-04-05 12:19:02,382 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:19:02,396 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-05 12:19:02,397 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-05 12:19:02,399 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-05 12:19:02,528 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:53706/user/HeartbeatReceiver
2015-04-05 12:19:02,715 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 53708
2015-04-05 12:19:02,718 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-05 12:19:02,719 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:53708 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 53708)
2015-04-05 12:19:02,722 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-05 12:19:03,229 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-05 12:19:03,232 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-05 12:19:03,418 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-05 12:19:03,420 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-05 12:19:03,423 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:53708 (size: 13.6 KB, free: 983.1 MB)
2015-04-05 12:19:03,424 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-05 12:19:03,428 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-05 12:19:03,710 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-05 12:19:03,751 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemRecSpec.scala:19
2015-04-05 12:19:03,770 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (distinct at ItemRecSpec.scala:19)
2015-04-05 12:19:03,773 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemRecSpec.scala:19) with 1 output partitions (allowLocal=false)
2015-04-05 12:19:03,774 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 1(count at ItemRecSpec.scala:19)
2015-04-05 12:19:03,774 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 0)
2015-04-05 12:19:03,781 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 0)
2015-04-05 12:19:03,792 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:19), which has no missing parents
2015-04-05 12:19:03,835 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4632) called with curMem=152562, maxMem=1030823608
2015-04-05 12:19:03,836 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 982.9 MB)
2015-04-05 12:19:03,846 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2746) called with curMem=157194, maxMem=1030823608
2015-04-05 12:19:03,846 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 982.9 MB)
2015-04-05 12:19:03,847 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:53708 (size: 2.7 KB, free: 983.1 MB)
2015-04-05 12:19:03,848 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 12:19:03,849 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-05 12:19:03,858 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:19)
2015-04-05 12:19:03,859 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-04-05 12:19:03,888 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1346 bytes)
2015-04-05 12:19:03,894 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-05 12:19:03,926 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-05 12:19:03,935 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-05 12:19:03,935 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-05 12:19:03,935 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-05 12:19:03,935 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-05 12:19:03,935 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-05 12:19:04,846 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1871 bytes result sent to driver
2015-04-05 12:19:04,870 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 975 ms on localhost (1/1)
2015-04-05 12:19:04,871 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-05 12:19:04,873 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (distinct at ItemRecSpec.scala:19) finished in 1.002 s
2015-04-05 12:19:04,874 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:19:04,874 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:19:04,875 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 1)
2015-04-05 12:19:04,875 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:19:04,879 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 1: List()
2015-04-05 12:19:04,882 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:19), which is now runnable
2015-04-05 12:19:04,885 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=159940, maxMem=1030823608
2015-04-05 12:19:04,886 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 12:19:04,892 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1545) called with curMem=162476, maxMem=1030823608
2015-04-05 12:19:04,893 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1545.0 B, free 982.9 MB)
2015-04-05 12:19:04,893 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:53708 (size: 1545.0 B, free: 983.1 MB)
2015-04-05 12:19:04,894 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 12:19:04,895 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-04-05 12:19:04,898 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:19)
2015-04-05 12:19:04,898 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-04-05 12:19:04,901 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:19:04,901 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-04-05 12:19:04,920 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:19:04,922 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2015-04-05 12:19:04,993 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 783 bytes result sent to driver
2015-04-05 12:19:05,002 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 96 ms on localhost (1/1)
2015-04-05 12:19:05,002 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (count at ItemRecSpec.scala:19) finished in 0.102 s
2015-04-05 12:19:05,002 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-04-05 12:19:05,008 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemRecSpec.scala:19, took 1.256621 s
2015-04-05 12:19:05,093 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: lookup at ItemRecSpec.scala:31
2015-04-05 12:19:05,094 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 13 (map at Strategy.scala:15)
2015-04-05 12:19:05,095 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (lookup at ItemRecSpec.scala:31) with 1 output partitions (allowLocal=false)
2015-04-05 12:19:05,095 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 3(lookup at ItemRecSpec.scala:31)
2015-04-05 12:19:05,095 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 2)
2015-04-05 12:19:05,098 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 2)
2015-04-05 12:19:05,100 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[13] at map at Strategy.scala:15), which has no missing parents
2015-04-05 12:19:05,105 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2600) called with curMem=164021, maxMem=1030823608
2015-04-05 12:19:05,106 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 12:19:05,112 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1582) called with curMem=166621, maxMem=1030823608
2015-04-05 12:19:05,113 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1582.0 B, free 982.9 MB)
2015-04-05 12:19:05,113 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:53708 (size: 1582.0 B, free: 983.1 MB)
2015-04-05 12:19:05,114 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-04-05 12:19:05,114 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-04-05 12:19:05,130 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 2 (MappedRDD[13] at map at Strategy.scala:15)
2015-04-05 12:19:05,130 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2015-04-05 12:19:05,136 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1398 bytes)
2015-04-05 12:19:05,137 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2015-04-05 12:19:05,182 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 813 bytes result sent to driver
2015-04-05 12:19:05,192 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 54 ms on localhost (1/1)
2015-04-05 12:19:05,193 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-04-05 12:19:05,193 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (map at Strategy.scala:15) finished in 0.062 s
2015-04-05 12:19:05,193 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:19:05,193 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:19:05,193 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 3)
2015-04-05 12:19:05,193 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:19:05,197 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 3: List()
2015-04-05 12:19:05,198 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 3 (MappedRDD[19] at lookup at ItemRecSpec.scala:31), which is now runnable
2015-04-05 12:19:05,201 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3344) called with curMem=168203, maxMem=1030823608
2015-04-05 12:19:05,201 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 12:19:05,210 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1899) called with curMem=171547, maxMem=1030823608
2015-04-05 12:19:05,211 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1899.0 B, free 982.9 MB)
2015-04-05 12:19:05,212 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:53708 (size: 1899.0 B, free: 983.0 MB)
2015-04-05 12:19:05,213 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
2015-04-05 12:19:05,214 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:838
2015-04-05 12:19:05,215 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 3 (MappedRDD[19] at lookup at ItemRecSpec.scala:31)
2015-04-05 12:19:05,215 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
2015-04-05 12:19:05,216 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:19:05,217 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
2015-04-05 12:19:05,224 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:19:05,224 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 12:19:05,247 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 914 bytes result sent to driver
2015-04-05 12:19:05,255 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 32 ms on localhost (1/1)
2015-04-05 12:19:05,255 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 3 (lookup at ItemRecSpec.scala:31) finished in 0.040 s
2015-04-05 12:19:05,255 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2015-04-05 12:19:05,256 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: lookup at ItemRecSpec.scala:31, took 0.162692 s
2015-04-05 12:19:05,272 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: lookup at ItemRecSpec.scala:37
2015-04-05 12:19:05,276 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 145 bytes
2015-04-05 12:19:05,279 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (lookup at ItemRecSpec.scala:37) with 1 output partitions (allowLocal=false)
2015-04-05 12:19:05,279 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 5(lookup at ItemRecSpec.scala:37)
2015-04-05 12:19:05,279 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 4)
2015-04-05 12:19:05,282 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 12:19:05,283 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 5 (MappedRDD[22] at lookup at ItemRecSpec.scala:37), which has no missing parents
2015-04-05 12:19:05,285 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3344) called with curMem=173446, maxMem=1030823608
2015-04-05 12:19:05,285 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 12:19:05,293 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1898) called with curMem=176790, maxMem=1030823608
2015-04-05 12:19:05,294 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1898.0 B, free 982.9 MB)
2015-04-05 12:19:05,295 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:53708 (size: 1898.0 B, free: 983.0 MB)
2015-04-05 12:19:05,296 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
2015-04-05 12:19:05,297 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:838
2015-04-05 12:19:05,298 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 5 (MappedRDD[22] at lookup at ItemRecSpec.scala:37)
2015-04-05 12:19:05,298 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
2015-04-05 12:19:05,300 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:19:05,301 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
2015-04-05 12:19:05,307 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:19:05,307 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
2015-04-05 12:19:05,329 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 914 bytes result sent to driver
2015-04-05 12:19:05,337 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 32 ms on localhost (1/1)
2015-04-05 12:19:05,337 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 5 (lookup at ItemRecSpec.scala:37) finished in 0.038 s
2015-04-05 12:19:05,337 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2015-04-05 12:19:05,338 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: lookup at ItemRecSpec.scala:37, took 0.065602 s
2015-04-05 12:19:05,356 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: lookup at ItemRecSpec.scala:43
2015-04-05 12:19:05,359 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (lookup at ItemRecSpec.scala:43) with 1 output partitions (allowLocal=false)
2015-04-05 12:19:05,359 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 7(lookup at ItemRecSpec.scala:43)
2015-04-05 12:19:05,359 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 6)
2015-04-05 12:19:05,364 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 12:19:05,364 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 7 (MappedRDD[25] at lookup at ItemRecSpec.scala:43), which has no missing parents
2015-04-05 12:19:05,368 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3344) called with curMem=178688, maxMem=1030823608
2015-04-05 12:19:05,368 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 12:19:05,375 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1903) called with curMem=182032, maxMem=1030823608
2015-04-05 12:19:05,376 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1903.0 B, free 982.9 MB)
2015-04-05 12:19:05,377 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:53708 (size: 1903.0 B, free: 983.0 MB)
2015-04-05 12:19:05,378 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
2015-04-05 12:19:05,378 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:838
2015-04-05 12:19:05,380 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 7 (MappedRDD[25] at lookup at ItemRecSpec.scala:43)
2015-04-05 12:19:05,380 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
2015-04-05 12:19:05,381 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 5, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:19:05,382 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 5)
2015-04-05 12:19:05,386 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:19:05,387 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 12:19:05,401 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 5). 914 bytes result sent to driver
2015-04-05 12:19:05,411 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 5) in 22 ms on localhost (1/1)
2015-04-05 12:19:05,411 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2015-04-05 12:19:05,411 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 7 (lookup at ItemRecSpec.scala:43) finished in 0.031 s
2015-04-05 12:19:05,413 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: lookup at ItemRecSpec.scala:43, took 0.056136 s
2015-04-05 12:19:05,431 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2015-04-05 12:19:05,431 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2015-04-05 12:19:05,431 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2015-04-05 12:19:05,431 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
2015-04-05 12:19:05,432 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
2015-04-05 12:19:05,432 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2015-04-05 12:19:05,432 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2015-04-05 12:19:05,432 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2015-04-05 12:19:05,433 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2015-04-05 12:19:05,433 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2015-04-05 12:19:05,433 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2015-04-05 12:19:05,433 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2015-04-05 12:19:05,433 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2015-04-05 12:19:05,434 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2015-04-05 12:19:05,434 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2015-04-05 12:19:05,434 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2015-04-05 12:19:05,434 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2015-04-05 12:19:05,434 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2015-04-05 12:19:05,435 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2015-04-05 12:19:05,435 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
2015-04-05 12:19:05,435 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
2015-04-05 12:19:05,436 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
2015-04-05 12:19:05,436 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs,null}
2015-04-05 12:19:05,490 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.0.104:4040
2015-04-05 12:19:05,492 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Stopping DAGScheduler
2015-04-05 12:19:06,550 [sparkDriver-akka.actor.default-dispatcher-4] INFO  org.apache.spark.MapOutputTrackerMasterActor - MapOutputTrackerActor stopped!
2015-04-05 12:19:06,560 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore cleared
2015-04-05 12:19:06,560 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
2015-04-05 12:19:06,561 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2015-04-05 12:19:06,564 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2015-04-05 12:19:06,569 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
2015-04-05 12:19:06,572 [sparkDriver-akka.actor.default-dispatcher-4] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
2015-04-05 12:27:02,024 [ScalaTest-main] INFO  org.apache.spark.SparkContext - Spark configuration:
spark.app.name=ItemBasedRecommender
spark.cassandra.connection.host=ec2-54-220-128-75.eu-west-1.compute.amazonaws.com
spark.cassandra.connection.timeout_ms=600000
spark.cassandra.input.split.size=2500
spark.deploy.defaultCores=4
spark.deploy.spreadOut=true
spark.executor.memory=3G
spark.logConf=true
spark.master=local
spark.serializer=org.apache.spark.serializer.KryoSerializer
2015-04-05 12:27:02,338 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing view acls to: shahab
2015-04-05 12:27:02,339 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: shahab
2015-04-05 12:27:02,339 [ScalaTest-main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(shahab); users with modify permissions: Set(shahab)
2015-04-05 12:27:02,730 [sparkDriver-akka.actor.default-dispatcher-3] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2015-04-05 12:27:02,803 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Starting remoting
2015-04-05 12:27:03,053 [sparkDriver-akka.actor.default-dispatcher-3] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.104:53729]
2015-04-05 12:27:03,062 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 53729.
2015-04-05 12:27:03,107 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2015-04-05 12:27:03,124 [ScalaTest-main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2015-04-05 12:27:03,148 [ScalaTest-main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-local-20150405122703-ef99
2015-04-05 12:27:03,154 [ScalaTest-main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 983.1 MB
2015-04-05 12:27:03,476 [ScalaTest-main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-04-05 12:27:03,638 [ScalaTest-main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /var/folders/t0/xgcl15w51d551h412_vvpgbh0000gn/T/spark-563a389e-c9e0-4f74-aa73-466c4cc006de
2015-04-05 12:27:03,650 [ScalaTest-main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2015-04-05 12:27:03,894 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:27:03,917 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:53730
2015-04-05 12:27:03,917 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'HTTP file server' on port 53730.
2015-04-05 12:27:04,139 [ScalaTest-main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.14.v20131031
2015-04-05 12:27:04,157 [ScalaTest-main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2015-04-05 12:27:04,158 [ScalaTest-main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2015-04-05 12:27:04,161 [ScalaTest-main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://192.168.0.104:4040
2015-04-05 12:27:04,375 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.util.AkkaUtils - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.0.104:53729/user/HeartbeatReceiver
2015-04-05 12:27:04,691 [ScalaTest-main] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 53731
2015-04-05 12:27:04,695 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2015-04-05 12:27:04,697 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMasterActor - Registering block manager localhost:53731 with 983.1 MB RAM, BlockManagerId(<driver>, localhost, 53731)
2015-04-05 12:27:04,704 [ScalaTest-main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2015-04-05 12:27:05,608 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(138675) called with curMem=0, maxMem=1030823608
2015-04-05 12:27:05,610 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 135.4 KB, free 982.9 MB)
2015-04-05 12:27:05,848 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(13887) called with curMem=138675, maxMem=1030823608
2015-04-05 12:27:05,849 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.6 KB, free 982.9 MB)
2015-04-05 12:27:05,852 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on localhost:53731 (size: 13.6 KB, free: 983.1 MB)
2015-04-05 12:27:05,872 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
2015-04-05 12:27:05,895 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from textFile at DataRepo.scala:42
2015-04-05 12:27:06,197 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2015-04-05 12:27:06,233 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: count at ItemRecSpec.scala:19
2015-04-05 12:27:06,254 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 (distinct at ItemRecSpec.scala:19)
2015-04-05 12:27:06,257 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (count at ItemRecSpec.scala:19) with 1 output partitions (allowLocal=false)
2015-04-05 12:27:06,257 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 1(count at ItemRecSpec.scala:19)
2015-04-05 12:27:06,258 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 0)
2015-04-05 12:27:06,264 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 0)
2015-04-05 12:27:06,275 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:19), which has no missing parents
2015-04-05 12:27:06,319 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(4632) called with curMem=152562, maxMem=1030823608
2015-04-05 12:27:06,320 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 982.9 MB)
2015-04-05 12:27:06,328 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2746) called with curMem=157194, maxMem=1030823608
2015-04-05 12:27:06,329 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 982.9 MB)
2015-04-05 12:27:06,331 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on localhost:53731 (size: 2.7 KB, free: 983.1 MB)
2015-04-05 12:27:06,332 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
2015-04-05 12:27:06,333 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:838
2015-04-05 12:27:06,346 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 0 (MappedRDD[8] at distinct at ItemRecSpec.scala:19)
2015-04-05 12:27:06,349 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
2015-04-05 12:27:06,376 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1346 bytes)
2015-04-05 12:27:06,382 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
2015-04-05 12:27:06,413 [Executor task launch worker-0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: file:/Users/shahab/BackgroundStudies/NetflixDataset/Enriched_Netflix_Dataset/head_URM_100.csv:0+1336116
2015-04-05 12:27:06,422 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2015-04-05 12:27:06,422 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2015-04-05 12:27:06,422 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2015-04-05 12:27:06,423 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2015-04-05 12:27:06,423 [Executor task launch worker-0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2015-04-05 12:27:07,300 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1871 bytes result sent to driver
2015-04-05 12:27:07,325 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 938 ms on localhost (1/1)
2015-04-05 12:27:07,326 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2015-04-05 12:27:07,327 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (distinct at ItemRecSpec.scala:19) finished in 0.968 s
2015-04-05 12:27:07,327 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:27:07,328 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:27:07,329 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 1)
2015-04-05 12:27:07,329 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:27:07,333 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 1: List()
2015-04-05 12:27:07,336 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:19), which is now runnable
2015-04-05 12:27:07,341 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2536) called with curMem=159940, maxMem=1030823608
2015-04-05 12:27:07,341 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 12:27:07,350 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1545) called with curMem=162476, maxMem=1030823608
2015-04-05 12:27:07,351 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1545.0 B, free 982.9 MB)
2015-04-05 12:27:07,353 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on localhost:53731 (size: 1545.0 B, free: 983.1 MB)
2015-04-05 12:27:07,353 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
2015-04-05 12:27:07,355 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:838
2015-04-05 12:27:07,359 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 1 (MappedRDD[10] at distinct at ItemRecSpec.scala:19)
2015-04-05 12:27:07,359 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
2015-04-05 12:27:07,361 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:27:07,362 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
2015-04-05 12:27:07,384 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:27:07,385 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 5 ms
2015-04-05 12:27:07,462 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 783 bytes result sent to driver
2015-04-05 12:27:07,471 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 104 ms on localhost (1/1)
2015-04-05 12:27:07,471 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 1 (count at ItemRecSpec.scala:19) finished in 0.112 s
2015-04-05 12:27:07,471 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2015-04-05 12:27:07,477 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: count at ItemRecSpec.scala:19, took 1.243666 s
2015-04-05 12:27:07,554 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: lookup at ItemRecSpec.scala:31
2015-04-05 12:27:07,555 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 13 (map at Strategy.scala:15)
2015-04-05 12:27:07,556 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (lookup at ItemRecSpec.scala:31) with 1 output partitions (allowLocal=false)
2015-04-05 12:27:07,556 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 3(lookup at ItemRecSpec.scala:31)
2015-04-05 12:27:07,556 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 2)
2015-04-05 12:27:07,559 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(Stage 2)
2015-04-05 12:27:07,560 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 2 (MappedRDD[13] at map at Strategy.scala:15), which has no missing parents
2015-04-05 12:27:07,565 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(2600) called with curMem=164021, maxMem=1030823608
2015-04-05 12:27:07,566 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 2.5 KB, free 982.9 MB)
2015-04-05 12:27:07,574 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1582) called with curMem=166621, maxMem=1030823608
2015-04-05 12:27:07,575 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1582.0 B, free 982.9 MB)
2015-04-05 12:27:07,576 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on localhost:53731 (size: 1582.0 B, free: 983.1 MB)
2015-04-05 12:27:07,576 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
2015-04-05 12:27:07,577 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:838
2015-04-05 12:27:07,589 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 2 (MappedRDD[13] at map at Strategy.scala:15)
2015-04-05 12:27:07,589 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
2015-04-05 12:27:07,596 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS_LOCAL, 1398 bytes)
2015-04-05 12:27:07,596 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
2015-04-05 12:27:07,645 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 813 bytes result sent to driver
2015-04-05 12:27:07,654 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 58 ms on localhost (1/1)
2015-04-05 12:27:07,654 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 2 (map at Strategy.scala:15) finished in 0.064 s
2015-04-05 12:27:07,654 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
2015-04-05 12:27:07,655 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2015-04-05 12:27:07,655 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
2015-04-05 12:27:07,655 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(Stage 3)
2015-04-05 12:27:07,655 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
2015-04-05 12:27:07,658 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents for Stage 3: List()
2015-04-05 12:27:07,658 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 3 (MappedRDD[19] at lookup at ItemRecSpec.scala:31), which is now runnable
2015-04-05 12:27:07,661 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3344) called with curMem=168203, maxMem=1030823608
2015-04-05 12:27:07,661 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 12:27:07,668 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1899) called with curMem=171547, maxMem=1030823608
2015-04-05 12:27:07,669 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1899.0 B, free 982.9 MB)
2015-04-05 12:27:07,670 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on localhost:53731 (size: 1899.0 B, free: 983.0 MB)
2015-04-05 12:27:07,671 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
2015-04-05 12:27:07,672 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:838
2015-04-05 12:27:07,673 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 3 (MappedRDD[19] at lookup at ItemRecSpec.scala:31)
2015-04-05 12:27:07,673 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
2015-04-05 12:27:07,675 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:27:07,675 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
2015-04-05 12:27:07,681 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:27:07,682 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 12:27:07,700 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 914 bytes result sent to driver
2015-04-05 12:27:07,709 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 27 ms on localhost (1/1)
2015-04-05 12:27:07,709 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 3 (lookup at ItemRecSpec.scala:31) finished in 0.035 s
2015-04-05 12:27:07,709 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2015-04-05 12:27:07,710 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: lookup at ItemRecSpec.scala:31, took 0.155083 s
2015-04-05 12:27:07,727 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: lookup at ItemRecSpec.scala:37
2015-04-05 12:27:07,731 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.MapOutputTrackerMaster - Size of output statuses for shuffle 1 is 145 bytes
2015-04-05 12:27:07,734 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (lookup at ItemRecSpec.scala:37) with 1 output partitions (allowLocal=false)
2015-04-05 12:27:07,734 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 5(lookup at ItemRecSpec.scala:37)
2015-04-05 12:27:07,735 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 4)
2015-04-05 12:27:07,737 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 12:27:07,738 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 5 (MappedRDD[22] at lookup at ItemRecSpec.scala:37), which has no missing parents
2015-04-05 12:27:07,740 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3344) called with curMem=173446, maxMem=1030823608
2015-04-05 12:27:07,741 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 12:27:07,747 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1898) called with curMem=176790, maxMem=1030823608
2015-04-05 12:27:07,748 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1898.0 B, free 982.9 MB)
2015-04-05 12:27:07,749 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on localhost:53731 (size: 1898.0 B, free: 983.0 MB)
2015-04-05 12:27:07,750 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
2015-04-05 12:27:07,750 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:838
2015-04-05 12:27:07,751 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 5 (MappedRDD[22] at lookup at ItemRecSpec.scala:37)
2015-04-05 12:27:07,751 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
2015-04-05 12:27:07,753 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:27:07,753 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
2015-04-05 12:27:07,758 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:27:07,758 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 12:27:07,779 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 914 bytes result sent to driver
2015-04-05 12:27:07,788 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 28 ms on localhost (1/1)
2015-04-05 12:27:07,789 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2015-04-05 12:27:07,789 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 5 (lookup at ItemRecSpec.scala:37) finished in 0.036 s
2015-04-05 12:27:07,789 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: lookup at ItemRecSpec.scala:37, took 0.061747 s
2015-04-05 12:27:07,808 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Starting job: lookup at ItemRecSpec.scala:43
2015-04-05 12:27:07,810 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (lookup at ItemRecSpec.scala:43) with 1 output partitions (allowLocal=false)
2015-04-05 12:27:07,810 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 7(lookup at ItemRecSpec.scala:43)
2015-04-05 12:27:07,810 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(Stage 6)
2015-04-05 12:27:07,813 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2015-04-05 12:27:07,814 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 7 (MappedRDD[25] at lookup at ItemRecSpec.scala:43), which has no missing parents
2015-04-05 12:27:07,817 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(3344) called with curMem=178688, maxMem=1030823608
2015-04-05 12:27:07,817 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 982.9 MB)
2015-04-05 12:27:07,823 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(1903) called with curMem=182032, maxMem=1030823608
2015-04-05 12:27:07,824 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1903.0 B, free 982.9 MB)
2015-04-05 12:27:07,825 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on localhost:53731 (size: 1903.0 B, free: 983.0 MB)
2015-04-05 12:27:07,825 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
2015-04-05 12:27:07,826 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:838
2015-04-05 12:27:07,827 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from Stage 7 (MappedRDD[25] at lookup at ItemRecSpec.scala:43)
2015-04-05 12:27:07,827 [sparkDriver-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 1 tasks
2015-04-05 12:27:07,828 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 5, localhost, PROCESS_LOCAL, 1056 bytes)
2015-04-05 12:27:07,829 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 5)
2015-04-05 12:27:07,835 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 1 blocks
2015-04-05 12:27:07,835 [Executor task launch worker-0] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
2015-04-05 12:27:07,861 [Executor task launch worker-0] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 5). 914 bytes result sent to driver
2015-04-05 12:27:07,868 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 5) in 34 ms on localhost (1/1)
2015-04-05 12:27:07,868 [sparkDriver-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 7 (lookup at ItemRecSpec.scala:43) finished in 0.041 s
2015-04-05 12:27:07,869 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2015-04-05 12:27:07,869 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: lookup at ItemRecSpec.scala:43, took 0.060500 s
2015-04-05 12:27:07,887 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2015-04-05 12:27:07,887 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2015-04-05 12:27:07,888 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2015-04-05 12:27:07,888 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump/json,null}
2015-04-05 12:27:07,888 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/threadDump,null}
2015-04-05 12:27:07,888 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2015-04-05 12:27:07,889 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2015-04-05 12:27:07,889 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2015-04-05 12:27:07,889 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2015-04-05 12:27:07,889 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2015-04-05 12:27:07,890 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2015-04-05 12:27:07,890 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2015-04-05 12:27:07,890 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2015-04-05 12:27:07,890 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2015-04-05 12:27:07,890 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2015-04-05 12:27:07,891 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2015-04-05 12:27:07,891 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2015-04-05 12:27:07,891 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2015-04-05 12:27:07,891 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2015-04-05 12:27:07,891 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job/json,null}
2015-04-05 12:27:07,892 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/job,null}
2015-04-05 12:27:07,892 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs/json,null}
2015-04-05 12:27:07,892 [ScalaTest-main-running-ItemRecSpec] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/jobs,null}
2015-04-05 12:27:07,946 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.0.104:4040
2015-04-05 12:27:07,948 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Stopping DAGScheduler
2015-04-05 12:27:09,011 [sparkDriver-akka.actor.default-dispatcher-2] INFO  org.apache.spark.MapOutputTrackerMasterActor - MapOutputTrackerActor stopped!
2015-04-05 12:27:09,023 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.MemoryStore - MemoryStore cleared
2015-04-05 12:27:09,024 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
2015-04-05 12:27:09,026 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2015-04-05 12:27:09,029 [ScalaTest-main-running-ItemRecSpec] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2015-04-05 12:27:09,037 [sparkDriver-akka.actor.default-dispatcher-15] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Shutting down remote daemon.
2015-04-05 12:27:09,041 [sparkDriver-akka.actor.default-dispatcher-15] INFO  akka.remote.RemoteActorRefProvider$RemotingTerminator - Remote daemon shut down; proceeding with flushing remote transports.
